{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15c8fd36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "import gdown\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import cv2\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.feature as feature\n",
    "import xlwings as xw\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import random\n",
    "\n",
    "#libraries for yolo\n",
    "from pytorchyolo.models import load_model\n",
    "from pytorchyolo.utils.transforms import Resize, DEFAULT_TRANSFORMS\n",
    "from pytorchyolo.utils.utils import non_max_suppression\n",
    "from pytorchyolo.utils.loss import compute_loss\n",
    "\n",
    "from matplotlib.ticker import (FormatStrFormatter, AutoMinorLocator, FuncFormatter, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4aa6149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from scripts.facedetectors import MediaPipe, YuNet, YoloFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87537d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BoxCoords(x1=126, y1=169, x2=299, y2=342)] [BoxCoords(x1=120, y1=129, x2=296, y2=349)] [BoxCoords(x1=126, y1=129, x2=288, y2=349)]\n"
     ]
    }
   ],
   "source": [
    "mp = MediaPipe()\n",
    "yn = YuNet()\n",
    "yf = YoloFace()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fd4752",
   "metadata": {},
   "source": [
    "## YOLOFace with FGSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a22160e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patterned after FGSM tutorial (https://pytorch.org/tutorials/beginner/fgsm_tutorial.html)\n",
    "# Define what device we are using\n",
    "print(\"CUDA Available: \", torch.cuda.is_available())\n",
    "device, model = load_model('./weights/yolo_face_sthanhng.cfg', \"./weights/yolo_face_sthanhng.weights\")\n",
    "\n",
    "# Set the model in evaluation mode. In this case this is for the Dropout layers\n",
    "model.eval()\n",
    "\n",
    "epsilons = [0, .05]\n",
    "use_cuda=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff92e19",
   "metadata": {},
   "source": [
    "## Image Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "266919a0-06ad-4f2b-987c-c9fc5825b328",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.imageattributes import extract_gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c215245",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detach_cpu(image):\n",
    "    return image.detach().cpu()\n",
    "\n",
    "# convert 1x3x416x416 to 416x416x3\n",
    "def reshape_image(image):\n",
    "    return np.transpose(np.squeeze(image), (1 ,2, 0))\n",
    "\n",
    "# convert 1x3x416x416 tensor to 416x416x3 numpy image\n",
    "def tensor_to_image(image):\n",
    "    return np.transpose(image.detach().cpu().squeeze().numpy(), (1, 2, 0))\n",
    "\n",
    "def save_tensor_as_image(image, path):\n",
    "    save_img = cv2.cvtColor(np.moveaxis((image.detach().numpy() * 255).squeeze(), 0, -1).astype('uint8'), cv2.COLOR_RGB2BGR)\n",
    "    cv2.imwrite(path, save_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b600713",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytorchyolo'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscripts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mminepsilon\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mminE\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Users\\amosc\\Documents\\Coding\\Thesis\\THS-ST3\\scripts\\minepsilon.py:7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmp\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorchyolo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m detect, models\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgdown\u001b[39;00m \n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pytorchyolo'"
     ]
    }
   ],
   "source": [
    "import scripts.fgsm as fgsm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833d8d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "faces_df = \"\"\n",
    "\n",
    "def load_mask(filename, face_num, target_bbox):\n",
    "    filename = \"restored_mask_\" + os.path.splitext(filename)[0] + \"_\" + str(face_num) + \"_image_final.png\"\n",
    "    mask = cv2.imread(os.path.join(os.getcwd(), RESTORED_MASK_PATH, filename), 0)\n",
    "    \n",
    "    face_row = faces_df.loc[faces_df['filename'] == filename]\n",
    "    padded_dim = (int(face_row[\"x2_pad\"] - face_row[\"x1_pad\"]), int(face_row[\"y2_pad\"] - face_row[\"y1_pad\"]))\n",
    "    target_dim = (int(target_bbox[2] - target_bbox[0]), int(target_bbox[3] - target_bbox[1]))\n",
    "    \n",
    "    #print(dict(zip(*np.unique(mask, return_counts = True)))[255], int(target_dim[0] * target_dim[1] * 0.1))\n",
    "    #print(dict(zip(*np.unique(mask, return_counts = True)))[255] < int(target_dim[0] * target_dim[1] * 0.1))\n",
    "    \n",
    "    if dict(zip(*np.unique(mask, return_counts = True)))[255] < int(target_dim[0] * target_dim[1] * 0.1):\n",
    "        return torch.ones((1, 3, target_dim[1], target_dim[0])), False\n",
    "    \n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (int(mask.shape[0] * 0.5), int(mask.shape[1] * 0.5)))\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "    mask = cv2.cvtColor(mask, cv2.COLOR_GRAY2RGB)\n",
    "    mask = transforms.Compose([DEFAULT_TRANSFORMS])((mask, np.zeros((1, 5))))[0].unsqueeze(0)\n",
    "    \n",
    "    #print(os.path.join(os.getcwd(), RESTORED_MASK_PATH, filename))\n",
    "    #print(\"mask size before:\", mask.shape)\n",
    "    \n",
    "    \n",
    "    #print(\"x1, y1, orig shape\")\n",
    "    #print(int(face_row[\"x1_pad\"]), int(face_row[\"y1_pad\"]), int(face_row[\"x2_pad\"]), int(face_row[\"y2_pad\"]))\n",
    "    #print(original_shape)\n",
    "    \n",
    "    #print(\"target shape:\", original_shape)\n",
    "    #print(\"yoloshape:\", int(face_row[\"x2\"] - face_row[\"x1\"]), int(face_row[\"y2\"] - face_row[\"y1\"]))\n",
    "    \n",
    "    current_dim = max(mask.shape)\n",
    "    diff_x, diff_y = abs(padded_dim[0] - current_dim) / 2, abs(padded_dim[1] - current_dim) / 2\n",
    "    #print(\"first diff:\", diff_x, diff_y)\n",
    "    \n",
    "    if diff_y != 0:\n",
    "        mask = mask[..., int(np.floor(diff_y)):-int(np.ceil(diff_y)), :]\n",
    "    if diff_x != 0:\n",
    "        mask = mask[..., int(np.floor(diff_x)):-int(np.ceil(diff_x))]\n",
    "        \n",
    "    #print(mask.shape == padded_dim, mask.shape, padded_dim, target_dim)\n",
    "    \n",
    "    padding = [\n",
    "        int(abs(face_row[\"x1\"] - face_row[\"x1_pad\"])),\n",
    "        int(abs(face_row[\"y1\"] - face_row[\"y1_pad\"])),\n",
    "        int(abs(face_row[\"x2\"] - face_row[\"x2_pad\"])),\n",
    "        int(abs(face_row[\"y2\"] - face_row[\"y2_pad\"]))\n",
    "    ]\n",
    "    \n",
    "    #print(\"padding:\", padding)\n",
    "    \n",
    "    new_dim = padded_dim[0] - padding[0] - padding[2], padded_dim[1] - padding[1] - padding[3]\n",
    "    diff_x, diff_y = (target_dim[0] - new_dim[0]) / 2, (target_dim[1] - new_dim[1]) / 2\n",
    "    #print(\"second diff:\", diff_x, diff_y)\n",
    "    \n",
    "    padding[0] -= int(np.floor(diff_x))\n",
    "    padding[1] -= int(np.floor(diff_y))\n",
    "    padding[2] -= int(np.ceil(diff_x))\n",
    "    padding[3] -= int(np.ceil(diff_y))\n",
    "    \n",
    "    #print(\"mask size after:\", mask.shape)\n",
    "    #print(\"unpadded bbox:\", (face_row[\"x1\"], face_row[\"y1\"], face_row[\"x2\"], face_row[\"y2\"]))\n",
    "    #print(\"adjusted padding:\", padding)\n",
    "    # mask = mask[..., padding[1]:-padding[3], padding[0]:-padding[2]]\n",
    "    \n",
    "    mask = F.pad(input=mask, pad=(-padding[0], -padding[2], -padding[1], -padding[3]), mode='constant', value=0)\n",
    "    \n",
    "    return mask, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7c3f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(model, device):\n",
    "    \n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    \n",
    "    df = pd.DataFrame() # dataframe storing the dataset\n",
    "    row = {} #the information/columns for a single row in the dataset is stored here\n",
    "    \n",
    "    # Loop over all examples in test set\n",
    "    for path in glob.glob(os.path.join(INPUT_PATH, '*.jpg')):\n",
    "        row['path'] = path\n",
    "        file_basename = os.path.basename(path)\n",
    "        print(file_basename)\n",
    "        \n",
    "        REF_SUBSET = REF_SET[REF_SET['source_file'] == file_basename]\n",
    "        \n",
    "        if len(REF_SUBSET.index) > 0:\n",
    "            model.eval()\n",
    "\n",
    "            model.gradient_mode = False\n",
    "            for yolo_layer in model.yolo_layers:\n",
    "                yolo_layer.gradient_mode = False\n",
    "\n",
    "            # read and transform the image from the path\n",
    "            data = cv2.imread(path)  # read the image\n",
    "            row['source_w'], row['source_h'], _ = data.shape\n",
    "            data = cv2.cvtColor(data, cv2.COLOR_BGR2RGB) #change to rgb\n",
    "            data = transforms.Compose([DEFAULT_TRANSFORMS,Resize(416)])((data, np.zeros((1, 5))))[0].unsqueeze(0) # transform the image\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # Forward pass the data through the model and call non max suppression\n",
    "                nms, nms_output = non_max_suppression(model(data), 0.5, 0.5) #conf_thres and iou_thres = 0.5\n",
    "\n",
    "            face_list = []\n",
    "            print(nms_output)\n",
    "            if type(nms_output[0]) is not int:\n",
    "                face_list = nms_output[0]\n",
    "\n",
    "            data = data.to(device)\n",
    "            # Set requires_grad attribute of tensor. Important for Attack\n",
    "            data.requires_grad = True\n",
    "\n",
    "            model.gradient_mode = True\n",
    "            for yolo_layer in model.yolo_layers:\n",
    "                yolo_layer.gradient_mode = True\n",
    "\n",
    "            output = model(data)\n",
    "\n",
    "            # loop through each of the faces in the image\n",
    "            for face_index, face_row in enumerate(face_list): #nms_output[0] because the model is designed to take in several images at a time from the dataloader but we are only loading the image one at a time\n",
    "                if face_index in set(REF_SUBSET['face_index']):\n",
    "\n",
    "                    row['face_index'] = face_index\n",
    "                    print(\"Face\", face_index)\n",
    "\n",
    "                    row['obj_score'] = face_row[4].item()\n",
    "                    row['class_score'] = face_row[5].item()\n",
    "                    x, y, w, h = face_row[0], face_row[1], face_row[2], face_row[3]\n",
    "\n",
    "#                     factor_x, factor_y, factor_w, factor_h = random.uniform(1, 2), random.uniform(1, 2), random.uniform(1, 2), random.uniform(1, 2)\n",
    "                    normal_x, normal_y, normal_w, normal_h = x / 416, y / 416, w / 416, h / 416\n",
    "\n",
    "#                     new_x = normal_x * factor_x if random.choice([True, False]) else normal_x / factor_x\n",
    "#                     new_y = normal_y * factor_y if random.choice([True, False]) else normal_y / factor_y\n",
    "#                     new_w = normal_w * factor_w if random.choice([True, False]) else normal_w / factor_w\n",
    "#                     new_h = normal_h * factor_h if random.choice([True, False]) else normal_h / factor_h\n",
    "\n",
    "#                     new_x, new_y, new_w, new_h = max(min(1, new_x), 0), max(min(1, new_y), 0), max(min(1, new_w), 0), max(min(1, new_h), 0)\n",
    "\n",
    "                    target = torch.tensor([[0.0, 0, normal_x, normal_y, normal_w, normal_h]])\n",
    "                    target = target.to(device)\n",
    "\n",
    "                    loss, loss_components = compute_loss(output, target, model)\n",
    "\n",
    "                    # cropped image with bounding box\n",
    "                    # getting (x1, y1) upper left, (x2, y2) lower right\n",
    "                    x1 = max(int(np.floor((x - w / 2).detach().cpu().numpy())), 0)\n",
    "                    y1 = max(int(np.floor((y - h / 2).detach().cpu().numpy())), 0)\n",
    "                    x2 = min(int(np.ceil((x + w / 2).detach().cpu().numpy())), 415)\n",
    "                    y2 = min(int(np.ceil((y + h / 2).detach().cpu().numpy())), 415)\n",
    "\n",
    "                    row['x1'], row['y1'], row['x2'], row['y2'] = x1, y1, x2, y2\n",
    "\n",
    "                    cropped_image = detach_cpu(data)[:, :, y1:y2, x1:x2] #get the first dimension, the channels, and crop it\n",
    "                    cropped_image = tensor_to_image(cropped_image) #reshape the image to (w/h, h/w, channel)\n",
    "\n",
    "                    # Zero all existing gradients\n",
    "                    model.zero_grad()\n",
    "                    data.grad = None\n",
    "\n",
    "                    # Calculate gradients of model in backward pass\n",
    "                    loss.backward(retain_graph=True) #TODO: Amos - check if this is correct\n",
    "\n",
    "                    # Collect datagrad\n",
    "                    data_grad = data.grad.data\n",
    "                    #print('Gradient')\n",
    "                    #print(data_grad)\n",
    "                    #print(data_grad.shape)\n",
    "                    #plt.imshow(np.transpose(np.clip(data_grad.squeeze(0).numpy(), 0, 1), (1, 2, 0)))\n",
    "                    #plt.show()\n",
    "\n",
    "                    bbox = (x1, y1, x2, y2)\n",
    "                    mask, used_mask = load_mask(os.path.basename(path), face_index, bbox)\n",
    "                    row['used_mask'] = used_mask\n",
    "\n",
    "\n",
    "                    #TODO: Jay - extract image attributes here\n",
    "                    # extract the image attributes from  the 'cropped_image' variable\n",
    "                    # save the attributes as row['<column name in the dataset>'] = <data> (see examples above for reference)\n",
    "\n",
    "                    row = extract_image_attributes(row, path, face_index, cropped_image * tensor_to_image(mask[0]), \"mask\")\n",
    "                    row = extract_image_attributes(row, path, face_index, cropped_image, \"bbox\")\n",
    "\n",
    "                    whole_mask = np.zeros(data.shape)\n",
    "                    whole_mask[..., y1:y2, x1:x2] = mask\n",
    "#                     inverted_mask = np.zeros(data.shape)\n",
    "#                     inverted_mask[..., y1:y2, x1:x2] = (1 - whole_mask[..., y1:y2, x1:x2]) if used_mask else whole_mask[..., y1:y2, x1:x2]\n",
    "\n",
    "                    #print(\"bbox dim:\", bbox)\n",
    "                    #print(mask.shape, data[:, :, y1:y2, x1:x2].shape)\n",
    "                    # TODO - Amos - determine the value of epsilon by calling fgsm_attack and changing the value of epsilon (see code below)\n",
    "                    # the value of data(image) and data_grad remains constant diba\n",
    "\n",
    "                    #print(\"Calculating min epsilon for YuNet...\")\n",
    "                    yn_min_e_face = minE.min_model_eps(data.clone().detach(), data_grad.clone().detach(), minE.yn_det_fn, whole_mask, bbox)\n",
    "                    #print(\"Calculating min epsilon for MediaPipe...\")\n",
    "#                     mp_min_e_face = minE.min_model_eps(data.clone().detach(), data_grad.clone().detach(), minE.mp_det_fn, whole_mask, bbox)\n",
    "                    #print(\"Calculating min epsilon for YoloFace...\")\n",
    "#                     yf_min_e_face = minE.min_model_eps(data.clone().detach(), data_grad.clone().detach(), minE.yf_det_fn, whole_mask, bbox)\n",
    "                    \n",
    "#                     print(\"yn min face:\", yn_min_e_face, \"mp min face:\", mp_min_e_face, \"yf min face:\", yf_min_e_face)\n",
    "#                     row['e_face_yn'], row['e_face_mp'], row['e_face_yf'] = yn_min_e_face, mp_min_e_face, yf_min_e_face\n",
    "                    \n",
    "                    print(\"yn min face:\", yn_min_e_face)\n",
    "                    row['e_face_yn'] = yn_min_e_face\n",
    "\n",
    "#                     if used_mask:\n",
    "#                         yn_min_e_bg = minE.min_model_eps(data.clone().detach(), data_grad.clone().detach(), minE.yn_det_fn, inverted_mask, bbox, background=True)\n",
    "#                         mp_min_e_bg = minE.min_model_eps(data.clone().detach(), data_grad.clone().detach(), minE.mp_det_fn, inverted_mask, bbox, background=True)\n",
    "#                         yf_min_e_bg = minE.min_model_eps(data.clone().detach(), data_grad.clone().detach(), minE.yf_det_fn, inverted_mask, bbox, background=True)\n",
    "#                     else:\n",
    "#                         yn_min_e_bg = yn_min_e_face\n",
    "#                         mp_min_e_bg = mp_min_e_face\n",
    "#                         yf_min_e_bg = yf_min_e_face\n",
    "\n",
    "#                     print(\"yn min bg:\", yn_min_e_bg, \"mp min bg:\", mp_min_e_bg, \"yf min bg:\", yf_min_e_bg, \"used mask\" if used_mask else \"did not use mask\")\n",
    "#                     row['e_bg_yn'], row['e_bg_mp'], row['e_bg_yf'] = yn_min_e_bg, mp_min_e_bg, yf_min_e_bg\n",
    "\n",
    "                    bbox_mask = np.zeros(data.shape)\n",
    "                    bbox_mask[..., y1:y2, x1:x2] = 1\n",
    "\n",
    "                    if used_mask:\n",
    "                        yn_min_e_bbox = minE.min_model_eps(data.clone().detach(), data_grad.clone().detach(), minE.yn_det_fn, bbox_mask, bbox)\n",
    "#                         mp_min_e_bbox = minE.min_model_eps(data.clone().detach(), data_grad.clone().detach(), minE.mp_det_fn, bbox_mask, bbox)\n",
    "#                         yf_min_e_bbox = minE.min_model_eps(data.clone().detach(), data_grad.clone().detach(), minE.yf_det_fn, bbox_mask, bbox)\n",
    "                    else:\n",
    "                        yn_min_e_bbox = yn_min_e_face\n",
    "#                         mp_min_e_bbox = mp_min_e_face\n",
    "#                         yf_min_e_bbox = yf_min_e_face\n",
    "\n",
    "#                     print(\"yn min bbox:\", yn_min_e_bbox, \"mp min bbox:\", mp_min_e_bbox, \"yf min bbox:\", yf_min_e_bbox)\n",
    "#                     row['e_bbox_yn'], row['e_bbox_mp'], row['e_bbox_yf'] = yn_min_e_bbox, mp_min_e_bbox, yf_min_e_bbox\n",
    "                    \n",
    "                    print(\"yn min bbox:\", yn_min_e_bbox)\n",
    "                    row['e_bbox_yn'] = yn_min_e_bbox \n",
    "\n",
    "                    large_x1 = max(int(np.floor((x - w).detach().cpu().numpy())), 0)\n",
    "                    large_y1 = max(int(np.floor((y - h).detach().cpu().numpy())), 0)\n",
    "                    large_x2 = min(int(np.ceil((x + w).detach().cpu().numpy())), 415)\n",
    "                    large_y2 = min(int(np.ceil((y + h).detach().cpu().numpy())), 415)\n",
    "\n",
    "                    large_bbox_mask = np.zeros(data.shape)\n",
    "                    large_bbox_mask[..., large_y1:large_y2, large_x1:large_x2] = 1\n",
    "\n",
    "                    yn_min_e_lbbox = minE.min_model_eps(data.clone().detach(), data_grad.clone().detach(), minE.yn_det_fn, large_bbox_mask, bbox)\n",
    "#                     mp_min_e_lbbox = minE.min_model_eps(data.clone().detach(), data_grad.clone().detach(), minE.mp_det_fn, large_bbox_mask, bbox)\n",
    "#                     yf_min_e_lbbox = minE.min_model_eps(data.clone().detach(), data_grad.clone().detach(), minE.yf_det_fn, large_bbox_mask, bbox)\n",
    "\n",
    "#                     print(\"yn min lbbox:\", yn_min_e_lbbox, \"mp min lbbox:\", mp_min_e_lbbox, \"yf min lbbox:\", yf_min_e_lbbox)\n",
    "#                     row['e_lbbox_yn'], row['e_lbbox_mp'], row['e_lbbox_yf'] = yn_min_e_lbbox, mp_min_e_lbbox, yf_min_e_lbbox\n",
    "                    \n",
    "                    print(\"yn min lbbox:\", yn_min_e_lbbox)\n",
    "                    row['e_lbbox_yn'] = yn_min_e_lbbox\n",
    "\n",
    "#                     cur_filename_base = os.path.join(PERTURBED_OUTS, os.path.splitext(file_basename)[0] + \"_\" + str(face_index))\n",
    "\n",
    "#                     cur_filename =  cur_filename_base + \"_yf_min_e_face.png\"\n",
    "#                     save_tensor_as_image(minE.fgsm_attack(data.clone().detach(), yf_min_e_face, data_grad.clone().detach(), whole_mask), cur_filename)\n",
    "#                     cur_filename =  cur_filename_base + \"_yf_min_e_bg.png\"\n",
    "#                     save_tensor_as_image(minE.fgsm_attack(data.clone().detach(), yf_min_e_bg, data_grad.clone().detach(), inverted_mask), cur_filename)\n",
    "#                     cur_filename =  cur_filename_base + \"_yf_min_e_bbox.png\"\n",
    "#                     save_tensor_as_image(minE.fgsm_attack(data.clone().detach(), yf_min_e_bbox, data_grad.clone().detach(), bbox_mask), cur_filename)\n",
    "#                     cur_filename =  cur_filename_base + \"_yf_min_e_lbbox.png\"\n",
    "#                     save_tensor_as_image(minE.fgsm_attack(data.clone().detach(), yf_min_e_lbbox, data_grad.clone().detach(), large_bbox_mask), cur_filename)\n",
    "\n",
    "\n",
    "        #             cur_filename =  cur_filename_base + \"yn_min_e_face.png\"\n",
    "        #             save_tensor_as_image(minE.fgsm_attack(data.clone().detach(), yn_min_e_face, data_grad.clone().detach(), mask, *bbox), cur_filename)\n",
    "        #             cur_filename =  cur_filename_base + \"mp_min_e_face.png\"\n",
    "        #             save_tensor_as_image(minE.fgsm_attack(data.clone().detach(), mp_min_e_face, data_grad.clone().detach(), mask, *bbox), cur_filename)\n",
    "        #             cur_filename =  cur_filename_base + \"yf_min_e_face.png\"\n",
    "        #             save_tensor_as_image(minE.fgsm_attack(data.clone().detach(), yf_min_e_face, data_grad.clone().detach(), mask, *bbox), cur_filename)\n",
    "        #             cur_filename =  cur_filename_base + \"yn_min_e_bg.png\"\n",
    "        #             save_tensor_as_image(minE.fgsm_attack(data.clone().detach(), yn_min_e_bg, data_grad.clone().detach(), inverted_mask, *bbox), cur_filename)\n",
    "        #             cur_filename =  cur_filename_base + \"mp_min_e_bg.png\"\n",
    "        #             save_tensor_as_image(minE.fgsm_attack(data.clone().detach(), mp_min_e_bg, data_grad.clone().detach(), inverted_mask, *bbox), cur_filename)\n",
    "        #             cur_filename =  cur_filename_base + \"yf_min_e_bg.png\"\n",
    "        #             save_tensor_as_image(minE.fgsm_attack(data.clone().detach(), yf_min_e_bg, data_grad.clone().detach(), inverted_mask, *bbox), cur_filename)\n",
    "\n",
    "                    # Call FGSM Attack\n",
    "\n",
    "                    #perturbed_data = minE.fgsm_attack(data.clone().detach(), yf_min_e, data_grad.clone().detach(), mask, *bbox)\n",
    "                    #perturbed_data = fgsm_attack(data, max(yn_min_e, mp_min_e), data_grad) #data is the input image, epsilon\n",
    "                    #print(\"can detect faces on unperturbed img?\", minE.mp_det_fn(data.detach()))\n",
    "                    #print(f\"can detect faces on perturbed data with e={max(yn_min_e, mp_min_e) - 0.01}?\", minE.mp_det_fn(fgsm_attack(data, max(yn_min_e, mp_min_e) - 0.01, data_grad).detach()))\n",
    "                    #print(f\"can detect faces on perturbed img? with e={max(yn_min_e, mp_min_e) - 0.01}\", minE.mp_det_fn(perturbed_data.detach()))\n",
    "\n",
    "                    df = df.append(row, ignore_index=True) #append the attributes of one face to the dataframe\n",
    "            \n",
    "    df.to_csv(os.path.join(CSV_PATH, FOLDER_NAME + 'yunet_features_dataset' + str(int(time.time())) + '.csv'), index=False)  #save to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3efbbeb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "folders = []\n",
    "fails = []\n",
    "folders = [\"img_celeba_94\", \"img_celeba_93\"]\n",
    "OUTPUT_FOLDER = os.path.join(os.getcwd(), \"faceseg-outs\")\n",
    "\n",
    "REF_SET = pd.read_csv(os.path.join(os.getcwd(), 'reference_dataset.csv'), index_col=0)\n",
    "REF_SET.reset_index()\n",
    "REF_SET.head()\n",
    "\n",
    "for FOLDER_NAME in folders:\n",
    "    INPUT_PATH = os.path.join(os.getcwd(), 'images', FOLDER_NAME)\n",
    "    FOLDER_PATH = os.path.join(OUTPUT_FOLDER, FOLDER_NAME)\n",
    "    CSV_PATH = os.path.join(FOLDER_PATH, FOLDER_NAME + '_CSV')\n",
    "    RESTORED_MASK_PATH = os.path.join(FOLDER_PATH, FOLDER_NAME + '_restored_mask')\n",
    "    PERTURBED_OUTS = os.path.join(FOLDER_PATH, FOLDER_NAME + \"_perturbed_outs\")\n",
    "    \n",
    "    if not os.path.exists(PERTURBED_OUTS):\n",
    "        os.mkdir(PERTURBED_OUTS)\n",
    "    \n",
    "    CSV_FILE = \"\"\n",
    "    for file in os.listdir(CSV_PATH):\n",
    "        if \"dataset_pixels\" in file and file.endswith(\".csv\"):\n",
    "            CSV_FILE = os.path.join(os.getcwd(), CSV_PATH, file)\n",
    "            \n",
    "    faces_df = pd.read_csv(CSV_FILE)\n",
    "    faces_df.loc[:, [\"x1\", \"y1\", \"x2\", \"y2\", \"x1_pad\", \"y1_pad\", \"x2_pad\", \"y2_pad\"]] = faces_df.loc[:, [\"x1\", \"y1\", \"x2\", \"y2\", \"x1_pad\", \"y1_pad\", \"x2_pad\", \"y2_pad\"]].clip(lower = 0)\n",
    "    \n",
    "    #print(CSV_FILE)\n",
    "    #raise ex\n",
    "    print(\"Working on\", FOLDER_NAME, \"folder\")\n",
    "    try:\n",
    "        pipeline(model, device)\n",
    "    except:\n",
    "        fails += FOLDER_NAME\n",
    "        print(\"An exception occurred\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3896c248",
   "metadata": {},
   "source": [
    "##### ___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
