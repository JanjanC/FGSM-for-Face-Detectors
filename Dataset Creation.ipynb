{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c8fd36",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import cv2\n",
    "import xlwings as xw\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "#libraries for yolo\n",
    "from pytorchyolo.models import load_model\n",
    "from pytorchyolo.utils.transforms import Resize, DEFAULT_TRANSFORMS\n",
    "from pytorchyolo.utils.utils import non_max_suppression\n",
    "from pytorchyolo.utils.loss import compute_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e89d98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.utils import detach_cpu, tensor_to_np_img, save_tensor_img, clone_detach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7376ded",
   "metadata": {},
   "source": [
    "## Face Detector Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4aa6149",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scripts.face_detectors import MediaPipe, YuNet, YoloFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87537d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp = MediaPipe()\n",
    "yn = YuNet()\n",
    "yf = YoloFace()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fd4752",
   "metadata": {},
   "source": [
    "## YOLOFace with FGSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a22160e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patterned after FGSM tutorial (https://pytorch.org/tutorials/beginner/fgsm_tutorial.html)\n",
    "# Define what device we are using\n",
    "print(\"CUDA Available: \", torch.cuda.is_available())\n",
    "main_yf = YoloFace()\n",
    "device, model = main_yf.device, main_yf.yf_face_detector\n",
    "\n",
    "\n",
    "# Set the model in evaluation mode. In this case this is for the Dropout layers\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff92e19",
   "metadata": {},
   "source": [
    "## Image Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266919a0-06ad-4f2b-987c-c9fc5825b328",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scripts import image_attributes\n",
    "from scripts.image_attributes import extract_image_attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfe2116",
   "metadata": {},
   "source": [
    "## Load Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833d8d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mask(filename, face_num, target_bbox):\n",
    "    # Get the mask name and open it\n",
    "    filename = \"restored_mask_\" + os.path.splitext(filename)[0] + \"_\" + str(face_num) + \"_image_final.png\"\n",
    "    mask = cv2.imread(os.path.join(os.getcwd(), RESTORED_MASK_PATH, filename), 0)\n",
    "    \n",
    "    # Find the corresponding row of the mask in the FACES_DF to get the padding\n",
    "    face_row = FACES_DF.loc[FACES_DF['filename'] == filename]\n",
    "    padded_dim = (int(face_row[\"x2_pad\"] - face_row[\"x1_pad\"]), int(face_row[\"y2_pad\"] - face_row[\"y1_pad\"]))\n",
    "    \n",
    "    # Get the target dimensions based on the target bounding boxes\n",
    "    target_dim = (int(target_bbox[2] - target_bbox[0]), int(target_bbox[3] - target_bbox[1]))\n",
    "    \n",
    "    # Get the number of white pixels in the mask\n",
    "    num_white = dict(zip(*np.unique(mask, return_counts = True)))[255]\n",
    "    # Return an clear if the number of white pixels is less than 10% of the target dimensions\n",
    "    if num_white < int(target_dim[0] * target_dim[1] * 0.1):\n",
    "        return torch.ones((1, 3, target_dim[1], target_dim[0])), False\n",
    "    \n",
    "    # Appply morphological transformation to the mask\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (int(mask.shape[0] * 0.5), int(mask.shape[1] * 0.5)))\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "    mask = cv2.cvtColor(mask, cv2.COLOR_GRAY2RGB)\n",
    "    mask = transforms.Compose([DEFAULT_TRANSFORMS])((mask, np.zeros((1, 5))))[0].unsqueeze(0)\n",
    "    \n",
    "    # Masks are always square and should be smaller than the padded dimensions and target dimensions\n",
    "    current_dim = max(mask.shape)\n",
    "    diff_x, diff_y = abs(padded_dim[0] - current_dim) / 2, abs(padded_dim[1] - current_dim) / 2\n",
    "    \n",
    "    # Adjust the mask to be the same as the padded dimensions\n",
    "    if diff_y != 0:\n",
    "        mask = mask[..., int(np.floor(diff_y)):-int(np.ceil(diff_y)), :]\n",
    "    if diff_x != 0:\n",
    "        mask = mask[..., int(np.floor(diff_x)):-int(np.ceil(diff_x))]\n",
    "        \n",
    "    # Calculate the actual size without the padding\n",
    "    padding = [\n",
    "        int(abs(face_row[\"x1\"] - face_row[\"x1_pad\"])),\n",
    "        int(abs(face_row[\"y1\"] - face_row[\"y1_pad\"])),\n",
    "        int(abs(face_row[\"x2\"] - face_row[\"x2_pad\"])),\n",
    "        int(abs(face_row[\"y2\"] - face_row[\"y2_pad\"]))\n",
    "    ]\n",
    "    \n",
    "    # Adjust the padding so that it will match the target bounding box\n",
    "    new_dim = padded_dim[0] - padding[0] - padding[2], padded_dim[1] - padding[1] - padding[3]\n",
    "    diff_x, diff_y = (target_dim[0] - new_dim[0]) / 2, (target_dim[1] - new_dim[1]) / 2\n",
    "    \n",
    "    padding[0] -= int(np.floor(diff_x))\n",
    "    padding[1] -= int(np.floor(diff_y))\n",
    "    padding[2] -= int(np.ceil(diff_x))\n",
    "    padding[3] -= int(np.ceil(diff_y))\n",
    "    \n",
    "    mask = F.pad(input=mask, pad=(-padding[0], -padding[2], -padding[1], -padding[3]), mode='constant', value=0)\n",
    "    \n",
    "    return mask, True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cd60e3",
   "metadata": {},
   "source": [
    "## FGSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b600713",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import scripts.fgsm as fgsm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7c3f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(model, device):\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    \n",
    "    df = pd.DataFrame() # dataframe storing the dataset\n",
    "    row = {} #the information/columns for a single row in the dataset is stored here\n",
    "    \n",
    "    # Loop over all examples in input path\n",
    "    for path in glob.glob(os.path.join(INPUT_PATH, '*.jpg')):\n",
    "        file_basename = os.path.basename(path)\n",
    "        print(file_basename, end=\" \")\n",
    "        \n",
    "        if use_refset:\n",
    "            # Get indices of faces in the image\n",
    "            face_indices = set(REF_SET.loc[REF_SET['source_file'] == file_basename, \"face_index\"])\n",
    "            # If there are no face indices then it means the image is not in the ref set\n",
    "            if not face_indices:\n",
    "                print(\"(skipped)\")\n",
    "                continue\n",
    "        print(\"<- working on\")\n",
    "        \n",
    "        row['path'] = path\n",
    "        \n",
    "        model.eval()\n",
    "        model.gradient_mode = False\n",
    "        \n",
    "        for yolo_layer in model.yolo_layers:\n",
    "            yolo_layer.gradient_mode = False\n",
    "\n",
    "        # Read and transform the image from the path\n",
    "        data = cv2.imread(path)\n",
    "        row['source_w'], row['source_h'], _ = data.shape\n",
    "        data = cv2.cvtColor(data, cv2.COLOR_BGR2RGB)\n",
    "        data = transforms.Compose([DEFAULT_TRANSFORMS,Resize(416)])((data, np.zeros((1, 5))))[0].unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Forward pass the data through the model and call non max suppression\n",
    "            nms, nms_output = non_max_suppression(model(data), 0.5, 0.5) #conf_thres and iou_thres = 0.5\n",
    "\n",
    "        face_list = []\n",
    "        if type(nms_output[0]) is not int:\n",
    "            face_list = nms_output[0]\n",
    "\n",
    "        data = data.to(device)\n",
    "        \n",
    "        # Set requires_grad attribute of tensor. Important for attack\n",
    "        data.requires_grad = True\n",
    "\n",
    "        model.gradient_mode = True\n",
    "        for yolo_layer in model.yolo_layers:\n",
    "            yolo_layer.gradient_mode = True\n",
    "\n",
    "        output = model(data)\n",
    "\n",
    "        # loop through each of the faces in the image\n",
    "        for face_index, face_row in enumerate(face_list): #nms_output[0] because the model is designed to take in several images at a time from the dataloader but we are only loading the image one at a time\n",
    "            \n",
    "            if face_index not in face_indices:\n",
    "                continue\n",
    "\n",
    "            row['face_index'] = face_index\n",
    "            print(\"Face\", face_index)\n",
    "\n",
    "            row['obj_score'] = face_row[4].item()\n",
    "            row['class_score'] = face_row[5].item()\n",
    "            x, y, w, h = face_row[0], face_row[1], face_row[2], face_row[3]\n",
    "\n",
    "            normal_x, normal_y, normal_w, normal_h = x / 415, y / 415, w / 415, h / 415\n",
    "\n",
    "            if fgsm_loss_target == \"bbox\":\n",
    "                target = torch.tensor([[face_row[4].item(), face_row[5].item(), 0, 0, 0, 0]])\n",
    "            elif fgsm_loss_target == \"conf\":\n",
    "                target = torch.tensor([[0.0, 0, normal_x, normal_y, normal_w, normal_h]])\n",
    "\n",
    "            target = target.to(device)\n",
    "            loss, loss_components = compute_loss(output, target, model)\n",
    "\n",
    "            # cropped image with bounding box\n",
    "            # getting (x1, y1) upper left, (x2, y2) lower right\n",
    "            x1 = max(int(np.floor((x - w / 2).detach().cpu().numpy())), 0)\n",
    "            y1 = max(int(np.floor((y - h / 2).detach().cpu().numpy())), 0)\n",
    "            x2 = min(int(np.ceil((x + w / 2).detach().cpu().numpy())), 415)\n",
    "            y2 = min(int(np.ceil((y + h / 2).detach().cpu().numpy())), 415)\n",
    "\n",
    "            row['x1'], row['y1'], row['x2'], row['y2'] = x1, y1, x2, y2\n",
    "\n",
    "            cropped_image = detach_cpu(data)[:, :, y1:y2, x1:x2] #get the first dimension, the channels, and crop it\n",
    "            cropped_image = tensor_to_np_img(cropped_image) #reshape the image to (w/h, h/w, channel)\n",
    "\n",
    "            # Zero all existing gradients\n",
    "            model.zero_grad()\n",
    "            data.grad = None\n",
    "\n",
    "            # Calculate gradients of model in backward pass\n",
    "            loss.backward(retain_graph=True) #TODO: Amos - check if this is correct\n",
    "\n",
    "            # Collect datagrad\n",
    "            data_grad = data.grad.data\n",
    "            if save_noise:\n",
    "                save_tensor_img(data_grad, os.path.join(NOISE_OUTS, fgsm_loss_target + \"_\" + str(face_index) + \"_\" + os.path.basename(path)))\n",
    "\n",
    "            # Get mask\n",
    "            bbox = (x1, y1, x2, y2)\n",
    "            mask, used_mask = load_mask(os.path.basename(path), face_index, bbox)\n",
    "            row['used_mask'] = used_mask\n",
    "\n",
    "            row = extract_image_attributes(row, path, face_index, cropped_image * tensor_to_np_img(mask[0]), \"mask\")\n",
    "            row = extract_image_attributes(row, path, face_index, cropped_image, \"bbox\")\n",
    "\n",
    "            whole_mask = np.zeros(data.shape)\n",
    "            whole_mask[..., y1:y2, x1:x2] = mask\n",
    "            \n",
    "            inverted_mask = np.zeros(data.shape)\n",
    "            inverted_mask[..., y1:y2, x1:x2] = (1 - whole_mask[..., y1:y2, x1:x2]) if used_mask else whole_mask[..., y1:y2, x1:x2]\n",
    "            \n",
    "            bbox_mask = np.zeros(data.shape)\n",
    "            bbox_mask[..., y1:y2, x1:x2] = 1\n",
    "            \n",
    "            large_x1 = max(int(np.floor((x - w).detach().cpu().numpy())), 0)\n",
    "            large_y1 = max(int(np.floor((y - h).detach().cpu().numpy())), 0)\n",
    "            large_x2 = min(int(np.ceil((x + w).detach().cpu().numpy())), 415)\n",
    "            large_y2 = min(int(np.ceil((y + h).detach().cpu().numpy())), 415)\n",
    "\n",
    "            large_bbox_mask = np.zeros(data.shape)\n",
    "            large_bbox_mask[..., large_y1:large_y2, large_x1:large_x2] = 1\n",
    "\n",
    "            print(\"Calculating min epsilon for models...\")\n",
    "            \n",
    "            yn_min_e_face = fgsm.find_min_e(clone_detach(data), clone_detach(data_grad), yn, whole_mask, bbox)\n",
    "            mp_min_e_face = fgsm.find_min_e(clone_detach(data), clone_detach(data_grad), mp, whole_mask, bbox)\n",
    "            yf_min_e_face = fgsm.find_min_e(clone_detach(data), clone_detach(data_grad), yf, whole_mask, bbox)\n",
    "\n",
    "            print(\"yn min face:\", yn_min_e_face, \"mp min face:\", mp_min_e_face, \"yf min face:\", yf_min_e_face)\n",
    "            row['e_face_yn'], row['e_face_mp'], row['e_face_yf'] = yn_min_e_face, mp_min_e_face, yf_min_e_face\n",
    "\n",
    "            if used_mask:\n",
    "                yn_min_e_bg = fgsm.find_min_e(clone_detach(data), clone_detach(data_grad), yn, inverted_mask, bbox, background=True)\n",
    "                mp_min_e_bg = fgsm.find_min_e(clone_detach(data), clone_detach(data_grad), mp, inverted_mask, bbox, background=True)\n",
    "                yf_min_e_bg = fgsm.find_min_e(clone_detach(data), clone_detach(data_grad), yf, inverted_mask, bbox, background=True)\n",
    "            else:\n",
    "                yn_min_e_bg = yn_min_e_face\n",
    "                mp_min_e_bg = mp_min_e_face\n",
    "                yf_min_e_bg = yf_min_e_face\n",
    "\n",
    "            print(\"yn min bg:\", yn_min_e_bg, \"mp min bg:\", mp_min_e_bg, \"yf min bg:\", yf_min_e_bg, \"used mask\" if used_mask else \"did not use mask\")\n",
    "            row['e_bg_yn'], row['e_bg_mp'], row['e_bg_yf'] = yn_min_e_bg, mp_min_e_bg, yf_min_e_bg\n",
    "\n",
    "            if used_mask:\n",
    "                yn_min_e_bbox = fgsm.find_min_e(clone_detach(data), clone_detach(data_grad), yn, bbox_mask, bbox)\n",
    "                mp_min_e_bbox = fgsm.find_min_e(clone_detach(data), clone_detach(data_grad), mp, bbox_mask, bbox)\n",
    "                yf_min_e_bbox = fgsm.find_min_e(clone_detach(data), clone_detach(data_grad), yf, bbox_mask, bbox)\n",
    "            else:\n",
    "                yn_min_e_bbox = yn_min_e_face\n",
    "                mp_min_e_bbox = mp_min_e_face\n",
    "                yf_min_e_bbox = yf_min_e_face\n",
    "\n",
    "            print(\"yn min bbox:\", yn_min_e_bbox, \"mp min bbox:\", mp_min_e_bbox, \"yf min bbox:\", yf_min_e_bbox)\n",
    "            row['e_bbox_yn'], row['e_bbox_mp'], row['e_bbox_yf'] = yn_min_e_bbox, mp_min_e_bbox, yf_min_e_bbox\n",
    "\n",
    "            yn_min_e_lbbox = fgsm.find_min_e(clone_detach(data), clone_detach(data_grad), yn, large_bbox_mask, bbox)\n",
    "            mp_min_e_lbbox = fgsm.find_min_e(clone_detach(data), clone_detach(data_grad), mp, large_bbox_mask, bbox)\n",
    "            yf_min_e_lbbox = fgsm.find_min_e(clone_detach(data), clone_detach(data_grad), yf, large_bbox_mask, bbox)\n",
    "\n",
    "            print(\"yn min lbbox:\", yn_min_e_lbbox, \"mp min lbbox:\", mp_min_e_lbbox, \"yf min lbbox:\", yf_min_e_lbbox)\n",
    "            row['e_lbbox_yn'], row['e_lbbox_mp'], row['e_lbbox_yf'] = yn_min_e_lbbox, mp_min_e_lbbox, yf_min_e_lbbox\n",
    "            df = pd.concat([df, pd.DataFrame([row])], axis=0, ignore_index=True)\n",
    "            \n",
    "    df.to_csv(os.path.join(CSV_PATH, FOLDER_NAME + output_csv_tag + str(int(time.time())) + '.csv'), index=False)  #save to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5af1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "image_attributes.save_color_images = False\n",
    "image_attributes.save_lbp_images = False\n",
    "image_attributes.save_gradient_images = False\n",
    "fgsm_loss_target = \"conf\" # or \"bbox\"\n",
    "output_csv_tag = \"no_random\"\n",
    "save_noise = False\n",
    "use_refset = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3efbbeb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "folders = [\"1--Handshaking\"]\n",
    "fails = []\n",
    "OUTPUT_FOLDER = os.path.join(os.getcwd(), \"outputs\")\n",
    "\n",
    "REF_SET = pd.read_csv(os.path.join(os.getcwd(), \"csv\", \"reference_dataset.csv\"), index_col=0)\n",
    "REF_SET.reset_index()\n",
    "\n",
    "for FOLDER_NAME in folders:\n",
    "    INPUT_PATH = os.path.join(os.getcwd(), 'images', FOLDER_NAME)\n",
    "    FOLDER_PATH = os.path.join(OUTPUT_FOLDER, FOLDER_NAME)\n",
    "    CSV_PATH = os.path.join(FOLDER_PATH, FOLDER_NAME + '_CSV')\n",
    "    RESTORED_MASK_PATH = os.path.join(FOLDER_PATH, FOLDER_NAME + '_restored_mask')\n",
    "    \n",
    "    if save_noise:\n",
    "        NOISE_OUTS = os.path.join(FOLDER_PATH, FOLDER_NAME + \"_noise_outs\")\n",
    "        if not os.path.exists(NOISE_OUTS):\n",
    "            os.mkdir(NOISE_OUTS)\n",
    "    \n",
    "    FACES_DF = None\n",
    "    for file in os.listdir(CSV_PATH):\n",
    "        if \"dataset_pixels\" in file and file.endswith(\".csv\"):\n",
    "            FACES_DF = pd.read_csv(os.path.join(os.getcwd(), CSV_PATH, file))\n",
    "            cols_to_clip = [\"x1\", \"y1\", \"x2\", \"y2\", \"x1_pad\", \"y1_pad\", \"x2_pad\", \"y2_pad\"]\n",
    "            FACES_DF.loc[:, cols_to_clip] = FACES_DF.loc[:, cols_to_clip].clip(lower = 0)\n",
    "            break\n",
    "    else:\n",
    "        fails += FOLDER_NAME\n",
    "        print(\"An exception occurred: dataset_pixels csv not found for\", FOLDER_NAME)\n",
    "        continue\n",
    "    \n",
    "    print(\"Working on\", FOLDER_NAME, \"folder\")\n",
    "    pipeline(model, device)\n",
    "    # this is commented out so we can see any bugs\n",
    "#     try:\n",
    "#         pipeline(model, device)\n",
    "#     except:\n",
    "#         fails += FOLDER_NAME\n",
    "#         print(\"An exception occurred for\", FOLDER_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3896c248",
   "metadata": {},
   "source": [
    "##### ___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
