{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "a852f92699914e198df5106b815978e2",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Preliminaries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "c8252421191a4df885094a0e65e3d42d",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Import the necessary libraries dependencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "1598bddb4f4b472798b05bb0816f503e",
    "deepnote_cell_type": "code",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import time\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "6dee9c13bbbf44c280d5684e4535bc6c",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "from scripts.utils import np_to_tesor_img, tensor_to_np_img, save_tensor_img, open_img_as_tensor, display_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "8f4ad1ac1a6b45699a98ac3b881caf27",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Face Detector Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "e7ac3de10b9a42918812a6e350451dad",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Import the face detection models of `MediaPipe`, `YuNet`, and `YoloFace` and store all three to their own respective variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "63abb28490ec41ee90854ce74eee15a8",
    "deepnote_cell_type": "code",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scripts.face_detectors import MediaPipe, YuNet, YoloFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "953b27c59ff84470913f12e1f603f0e0",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "mp = MediaPipe()\n",
    "yn = YuNet()\n",
    "yf = YoloFace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "9266398ff9e7459b9d1cf04c2ea2da17",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Our Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "3fd34b72b9384ee1a553e8ca8220f291",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Import the `IARM-FGSM` model and store it to a variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "6834509d0b624b4098c0e735457c76cb",
    "deepnote_cell_type": "code",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scripts.iarm import *\n",
    "iarm = IARM()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "8e7445f2714942c69f3ab4cc86a80c91",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Image Feature Extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "6990e6fbd06947e8b80add0599744748",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Import the `image_attributes.py` to get the utilities needed for image feature extraction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "79607e12c04340cea319b7910be5e363",
    "deepnote_cell_type": "code",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scripts import image_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "bfb4663160d44f12ab8c47e9995fa7b2",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "# Settings\n",
    "image_attributes.save_color_images = False\n",
    "image_attributes.save_lbp_images = False\n",
    "image_attributes.save_gradient_images = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "3dda52b7f2334c3a8861cf2d2210845a",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## FGSM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "927acb733d00440b8f5c1cbea488d898",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Create a YOLO-face model instance to be used for generating the noise perturbation in the FGSM attack.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "18b4e765a94045499faa8ea87b14be69",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "# check if CUDA is enabled or available\n",
    "print(\"CUDA Available: \", torch.cuda.is_available())\n",
    "main_yf = YoloFace()\n",
    "device, model = main_yf.device, main_yf.yf_face_detector\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "c1825ad387f8478da00d7ff8d3d6e9f1",
    "deepnote_cell_type": "code",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scripts import fgsm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "14873fbbc4fa4f36ba9085f89d3d81d3",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "#### Sample attack without mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "3586c69ed4a54250a3b703087e0877ff",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "input_file = r\"./input/sample1.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "5b54442f75414a1fba79a41e4ff514fc",
    "deepnote_cell_type": "code",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "\n",
    "input_img = open_img_as_tensor(input_file)\n",
    "feats, grads, bboxes, masks = image_attributes.get_features(input_file)\n",
    "preds = iarm.predict(feats)\n",
    "output_img = fgsm.fgsm_attack(input_img, preds, grads, masks)\n",
    "\n",
    "t2 = time.time()\n",
    "\n",
    "print(\"Our regression model run time:\", t2 - t1)\n",
    "display_img(output_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "3f2e579b74114a3f977ec45f2b73dfa7",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "#### Sample attack with mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "f149fcbb751149098195eac048d56c95",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "input_file = r\"./input/sample2.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "46772c85db89422b9bf76cd9b30f142b",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "from scripts.facesegmentor import FaceSegementor\n",
    "faceseg = FaceSegementor()\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "input_img = open_img_as_tensor(input_file)\n",
    "feats, grads, bboxes, masks = image_attributes.get_features(input_file, face_segmentor=faceseg)\n",
    "preds = iarm.predict(feats)\n",
    "output_img = fgsm.fgsm_attack(input_img, preds, grads, masks)\n",
    "\n",
    "t2 = time.time()\n",
    "\n",
    "print(\"Our regression model run time:\", t2 - t1)\n",
    "display_img(output_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "c5af60163e6e436aaac90d5906697618",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "#### Compare with binary search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "9ceb5b58e5934ce8826923fe20f820cb",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "input_file = r\"./input/sample3.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "02ce460a3f1441a3ad95574c30bebb2c",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "\n",
    "input_img = open_img_as_tensor(input_file)\n",
    "_, grads, bboxes, masks = image_attributes.get_features(input_file)\n",
    "e_mins = []\n",
    "\n",
    "for data_grad, mask, bbox in zip(grads, masks, bboxes):\n",
    "    e_mins.append(fgsm.binary_search(input_img, data_grad, yf, mask, bbox))\n",
    "\n",
    "output_img = fgsm.fgsm_attack(input_img, e_mins, grads, masks)\n",
    "\n",
    "t2 = time.time()\n",
    "\n",
    "print(\"Binary search run time:\", t2 - t1)\n",
    "display_img(output_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "b52c5ca0a742448fb383dab2c62672ec",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "#### Check if the faces can still be detected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "e808e7ac6dd548bf822d03b5d85572b7",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "input_file = r\"./input/sample1.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "d36c11310a934f6a98caf5c55885be30",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "input_img = open_img_as_tensor(input_file)\n",
    "feats, grads, bboxes, masks = image_attributes.get_features(input_file)\n",
    "preds = iarm.predict(feats, multiplier=2.5)\n",
    "output_img = fgsm.fgsm_attack(input_img, preds, grads, masks)\n",
    "\n",
    "print(\"Detected faces:\")\n",
    "yf.detect(output_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "82e02fbd7e5b4003bbedf3b6a112d1c3",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "input_file = r\"./input/sample2.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "9977e7e0296d42fe823c089de65e38dc",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "input_img = open_img_as_tensor(input_file)\n",
    "feats, grads, bboxes, masks = image_attributes.get_features(input_file)\n",
    "preds = iarm.predict(feats, multiplier=2.5)\n",
    "output_img = fgsm.fgsm_attack(input_img, preds, grads, masks)\n",
    "\n",
    "print(\"Detected faces:\")\n",
    "yf.detect(output_img)"
   ]
  }
 ],
 "metadata": {
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "a002ab384366483a8fb7df732eb86e70",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
