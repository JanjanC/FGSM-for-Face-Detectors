{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k\\-Nearest Neighbors \\(k\\-NN\\)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kNN would be well suited for this task because the number of values in the training data is only in the thousands. Additionally, it does not need to compute the values of the results rapidly. With these reasons and its simplicity, it was chosen as one of the models to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "knn_pred = knn.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Çerçevelik       0.87      0.93      0.90       195\n",
      "Ürgüp Sivrisi       0.92      0.85      0.88       180\n",
      "\n",
      "     accuracy                           0.89       375\n",
      "    macro avg       0.89      0.89      0.89       375\n",
      " weighted avg       0.89      0.89      0.89       375\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_val, knn_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8906666666666667\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_val, knn_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression \\(LR\\)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Logistic Regression, we tried out two different ways of implementing a Logistic Regression model which are `SGDClassifier` and `LogisticRegression`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, `SGDClassifier` is used with default parameters and a `random_state` of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgdc = SGDClassifier(random_state = 2)\n",
    "sgdc.fit(X_train, y_train)\n",
    "sgdc_pred = sgdc.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Çerçevelik       0.88      0.90      0.89       195\n",
      "Ürgüp Sivrisi       0.89      0.86      0.87       180\n",
      "\n",
      "     accuracy                           0.88       375\n",
      "    macro avg       0.88      0.88      0.88       375\n",
      " weighted avg       0.88      0.88      0.88       375\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_val, sgdc_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.88\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_val, sgdc_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, `LogisticRegression` is used also with default parameters and a `random_state` of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = LogisticRegression(random_state = 2)\n",
    "sgd.fit(X_train, y_train)\n",
    "sgd_pred = sgd.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Çerçevelik       0.88      0.92      0.90       195\n",
      "Ürgüp Sivrisi       0.91      0.87      0.89       180\n",
      "\n",
      "     accuracy                           0.90       375\n",
      "    macro avg       0.90      0.89      0.90       375\n",
      " weighted avg       0.90      0.90      0.90       375\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_val, sgd_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.896\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_val, sgd_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='font-size:x-large'>Multilayer Perceptrons \\(MLP\\)</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Neural Networks, Multilayer Perceptrons or MLP is the specific model used. It is theoretically more powerful than LR. The data has many features where the range of values of the different seed types overlap. MLP might be able to find a pattern using the combination of different features to classify the seeds well despite of these overlaps. MLP is a feedforward neural network algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(random_state = 2)\n",
    "mlp.fit(X_train, y_train)\n",
    "mlp_pred = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9013333333333333\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, mlp_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection and Hyperparameter tuning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, all models will undergo feature selection and hyperparameter tuning to find the best features and hyperparameters that each model will use to produce the highest prediction accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k\\-Nearest Neighbors \\(k\\-NN\\)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter tuning using GridSearchCV**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning using `GridSearchCV` performs cross validation on all possible combinations of the hyperparameters. In this case, we have 2 variables that we can tune for SKLearn's implementation of kNN: `n_neighbors` and `weights`. The weights of 1 to 20 were chosen as an arbitrary relatively small range. Had these values yielded poor results, the range of values would be adjusted. But, because of their good accuracy scores they were kept.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = [\n",
    "    {\n",
    "        \"n_neighbors\": list(range(1, 21)),\n",
    "        \"weights\": ['uniform', 'distance']\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=KNeighborsClassifier(), n_jobs=-1,\n",
       "             param_grid=[{'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,\n",
       "                                          13, 14, 15, 16, 17, 18, 19, 20],\n",
       "                          'weights': ['uniform', 'distance']}])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "knn = KNeighborsClassifier()\n",
    "clf = GridSearchCV(knn, hyperparameters, n_jobs=-1, cv=3)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:\n",
      " {'n_neighbors': 19, 'weights': 'uniform'}\n",
      "Best accuracy:  89.60000000000001 %\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Çerçevelik       0.87      0.94      0.90       195\n",
      "Ürgüp Sivrisi       0.93      0.85      0.89       180\n",
      "\n",
      "     accuracy                           0.90       375\n",
      "    macro avg       0.90      0.89      0.90       375\n",
      " weighted avg       0.90      0.90      0.90       375\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Best parameters found:\\n', clf.best_params_)\n",
    "y_pred = clf.predict(X_val)\n",
    "print(\"Best accuracy: \", (accuracy_score(y_val, y_pred) * 100), \"%\")\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in the results above, the best parameters to use for a kNN model is a `k` value of 19 and a `weights` value of `uniform`.  These settings gave the highest score using cross validation on the train data. It's accuracy on the validation set is 89.6%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter tuning using GridSearchCV and Feature Selection using Univariate Selection (SelectKbest + f_classif)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Univariate feature selection can be done using SKLearn's `SelectKbest` function. Here we pass it the `f_classif` function so that it will calculate the ANOVA F-value of each feature to determine its significance in determining the label. This loop searches for the best combination of parameters for each of the best `k` features where `k` is a number from 1 to 12. It keeps the parameters that yield the best accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best grid:  {'n_neighbors': 19, 'weights': 'uniform', 'selected features': array(['Area', 'Perimeter', 'Major_Axis_Length', 'Minor_Axis_Length',\n",
      "       'Convex_Area', 'Eccentricity', 'Solidity', 'Extent', 'Roundness',\n",
      "       'Aspect_Ration', 'Compactness'], dtype=object)}\n",
      "Best accuracy:  89.86666666666666 %\n"
     ]
    }
   ],
   "source": [
    "# Parameter selection\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "best_score = 0\n",
    "best_grid = None\n",
    "\n",
    "for k in range(1, pumpkin_seeds_df.shape[1]):\n",
    "    kBest = SelectKBest(score_func = f_classif, k = k)\n",
    "    best_features = kBest.fit(X_train, y_train)\n",
    "    only_keep = best_features.get_feature_names_out()\n",
    "\n",
    "    knn = KNeighborsClassifier()\n",
    "    clf = GridSearchCV(knn, hyperparameters, n_jobs=-1, cv=3)\n",
    "    clf.fit(X_train[only_keep], y_train)\n",
    "\n",
    "    pred = clf.predict(X_val[only_keep])\n",
    "    accuracy = accuracy_score(y_val, pred)\n",
    "\n",
    "    if accuracy > best_score:\n",
    "        best_score = accuracy\n",
    "        best_grid = clf.best_params_\n",
    "        best_grid[\"selected features\"] = only_keep\n",
    "\n",
    "print(\"Best grid: \", best_grid)\n",
    "print(\"Best accuracy: \", (best_score * 100), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show that removing the feature `Equiv_Diameter` improves the score by .2%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Earlier Hyperparameter Tuning Method Testing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this earlier hyperparameter tuning method testing, a `ParameterGrid` is utilized with the separated raw and normalized data as options for hyperparameter tuning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    " hyperparameters = [\n",
    "     {\n",
    "         \"normal_data\": [0, 1],\n",
    "         \"n_neighbors\": list(range(1, 21)),\n",
    "         \"weights\": ['uniform', 'distance']\n",
    "     }\n",
    " ]\n",
    " from sklearn.model_selection import ParameterGrid\n",
    " best_score = 0\n",
    "\n",
    " for g in ParameterGrid(hyperparameters):\n",
    "     X_cur_train, y_cur_train = (X_train, y_train)\n",
    "     clf = KNeighborsClassifier()\n",
    "     clf.set_params(n_neighbors = g[\"n_neighbors\"], weights = g[\"weights\"])\n",
    "\n",
    "     clf.fit(X_cur_train, y_cur_train)\n",
    "\n",
    "     pred = clf.predict(X_val)\n",
    "     accuracy = accuracy_score(y_val, pred)\n",
    "\n",
    "     print(g)\n",
    "     print(\"Accuracy:\", accuracy)\n",
    "\n",
    "     if accuracy > best_score:\n",
    "         best_score = accuracy\n",
    "         best_grid = g\n",
    "\n",
    " print(\"Best accuracy: \", best_score, \"%\")\n",
    " print(\"Best grid: \", best_grid)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Earlier Hyperparameter Tuning and Feature Selection Method Testing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This testing includes hyperparemeter tuning and feature selection. `SelectKBest`, `f_classif`, and `ParameterGrid` were used to perform both methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# With feature selection (takes a long time to run)\n",
    " hyperparameters = [\n",
    "     {\n",
    "         \"k-features\": list(range(1, pumpkin_seeds_df.shape[1])),\n",
    "         \"n_neighbors\": list(range(1, 21)),\n",
    "         \"weights\": ['uniform', 'distance']\n",
    "     }\n",
    " ]\n",
    "\n",
    " from sklearn.feature_selection import SelectKBest\n",
    " from sklearn.feature_selection import f_classif\n",
    "\n",
    " best_score = 0\n",
    "\n",
    " for g in ParameterGrid(hyperparameters):\n",
    "     X_cur_train, y_cur_train = (X_train, y_train)\n",
    "\n",
    "     test = SelectKBest(score_func = f_classif, k = g[\"k-features\"])\n",
    "     best_features = test.fit(X_cur_train, y_cur_train)\n",
    "     only_keep = best_features.get_feature_names_out()\n",
    "     X_cur_train = X_cur_train[only_keep]\n",
    "\n",
    "\n",
    "     clf = KNeighborsClassifier()\n",
    "     clf.set_params(n_neighbors = g[\"n_neighbors\"], weights = g[\"weights\"])\n",
    "\n",
    "     clf.fit(X_cur_train, y_cur_train)\n",
    "\n",
    "     pred = clf.predict(X_val[only_keep])\n",
    "     accuracy = accuracy_score(y_val, pred)\n",
    "\n",
    "     if accuracy > best_score:\n",
    "         best_score = accuracy\n",
    "         best_grid = g\n",
    "         best_grid[\"selected features\"] = best_features.get_feature_names_out()\n",
    "\n",
    " print(\"Best accuracy: \", best_score, \"%\")\n",
    " print(\"Best grid: \", best_grid)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot of kNN with increasing K value and uniform distance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code plots the accuracy of a kNN classifier model as the value of `k` increases from 1 to 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = []\n",
    "\n",
    "for i in range(1,21):\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(X_train, y_train)\n",
    "    knn_pred = knn.predict(X_val)\n",
    "    acc = accuracy_score(y_val, knn_pred)\n",
    "    accuracy.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(1,21), accuracy, label = \"Trained on normally distributed data\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xlabel('Number of neighbors')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best value of k: 17\n",
      "Best accuracy of k: 90.13%\n"
     ]
    }
   ],
   "source": [
    "print(\"Best value of k:\", accuracy.index(max(accuracy)) + 1)\n",
    "print(\"Best accuracy of k: %.2f%%\" % (max(accuracy) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph shows that the the accuracy and the number of neighbors have a positive correlation. Though the results of this visualization seem to indicate that the value of `k` produces better results when it is 17, keep in mind that the earlier parameter search used cross validation. Cross validation makes models less susceptible to overfitting, thus only values from the earlier tuning will be considered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression \\(LR\\)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter tuning using GridSearchCV**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first set of hyperparameters have `l2` or `none` as the values for penalty denoting that the model  does not perform any feature selection. The list of solvers chosen here are the ones that are capable of working with a penalty of `l2` and `none`. The values for C are an arbitrary range of values that were tweaked during testing to produce good scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = [\n",
    "    {\n",
    "        \"penalty\": [\"l2\"],\n",
    "        \"solver\": [\"newton-cg\", \"lbfgs\", \"sag\"],\n",
    "        \"C\": [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "    },\n",
    "    {\n",
    "        \"penalty\": [\"none\"],\n",
    "        \"solver\": [\"newton-cg\", \"lbfgs\", \"sag\"],\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=LogisticRegression(max_iter=10000, random_state=2),\n",
       "             n_jobs=-1,\n",
       "             param_grid=[{'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
       "                          'penalty': ['l2'],\n",
       "                          'solver': ['newton-cg', 'lbfgs', 'sag']},\n",
       "                         {'penalty': ['none'],\n",
       "                          'solver': ['newton-cg', 'lbfgs', 'sag']}])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "lgr = LogisticRegression(max_iter = 10000, random_state = 2)\n",
    "clf = GridSearchCV(lgr, hyperparameters, n_jobs=-1, cv=3)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:\n",
      " {'C': 10, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "Best accuracy:  89.33333333333333 %\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Çerçevelik       0.88      0.92      0.90       195\n",
      "Ürgüp Sivrisi       0.91      0.87      0.89       180\n",
      "\n",
      "     accuracy                           0.89       375\n",
      "    macro avg       0.89      0.89      0.89       375\n",
      " weighted avg       0.89      0.89      0.89       375\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Best parameters found:\\n', clf.best_params_)\n",
    "y_pred = clf.predict(X_val)\n",
    "print(\"Best accuracy: \", (accuracy_score(y_val, y_pred) * 100), \"%\")\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter search found that the best values for `C`, `penalty`, and `solver` are 10, `l2`, and `newton-cg` respectively. The accuracy of the tuned logistic regression classifier is 89.33% which is lower than the tuned kNN model and the tuned kNN model with feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter tuning using GridSearchCV and Feature Selection using l1 (LASSO) penalty (+ liblinear solver)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this set of hyperparameters, both penalties `l2` and `l1` are considered to compare if a model that goes through feature selection produces a higher accuracy than a model that does not. Notably, the `l1` penalty or LASSO regression is a form of feature selection because it tends to give sparse data by zeroing out the weight of less important features. `liblinear` is chosen as the only solver because it is the only one that is capable of working with `l1` penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With feature selection\n",
    "hyperparameters = [\n",
    "    {\n",
    "        \"penalty\": [\"l1\", \"l2\"],\n",
    "        \"C\": [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=LogisticRegression(max_iter=10000, random_state=2,\n",
       "                                          solver='liblinear'),\n",
       "             n_jobs=-1,\n",
       "             param_grid=[{'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
       "                          'penalty': ['l1', 'l2']}])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "lgr = LogisticRegression(solver = \"liblinear\", max_iter = 10000, random_state = 2)\n",
    "clf = GridSearchCV(lgr, hyperparameters, n_jobs=-1, cv=3)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.24067727, -0.47855168,  0.43530614, -0.26806807,  1.17349112,\n",
       "        -0.94694144,  1.92508032,  0.69731068,  0.02915466, -0.6167836 ,\n",
       "         0.47332471, -0.55711101]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_estimator_.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:\n",
      " {'C': 10, 'penalty': 'l2'}\n",
      "Best accuracy:  0.8933333333333333 %\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Çerçevelik       0.88      0.92      0.90       195\n",
      "Ürgüp Sivrisi       0.91      0.87      0.89       180\n",
      "\n",
      "     accuracy                           0.89       375\n",
      "    macro avg       0.89      0.89      0.89       375\n",
      " weighted avg       0.89      0.89      0.89       375\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Best parameters found:\\n', clf.best_params_)\n",
    "y_pred = clf.predict(X_val)\n",
    "print(\"Best accuracy: \", accuracy_score(y_val, y_pred), \"%\")\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the parameter search found that `l2` penalty is better than `l1` penalty so we can say that performing feature selection through this method does not improve the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter tuning using GridSearchCV and Feature Selection using Univariate Selection (SelectKbest + f_classif)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from LASSO regression, we can use the same method used earlier on kNN to perform feature selection. Here we will test the best `k` features starting from 5 to 12. Starting the testing with `k = 5` helps reduce the amount of run time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best grid:  {'C': 10, 'penalty': 'l2', 'solver': 'newton-cg', 'selected features': array(['Area', 'Perimeter', 'Major_Axis_Length', 'Minor_Axis_Length',\n",
      "       'Eccentricity', 'Solidity', 'Extent', 'Roundness', 'Aspect_Ration',\n",
      "       'Compactness'], dtype=object)}\n",
      "Best accuracy:  89.60000000000001 %\n"
     ]
    }
   ],
   "source": [
    "# Parameter selection using SelectKBest\n",
    "hyperparameters = [\n",
    "    {\n",
    "        \"penalty\": [\"l2\"],\n",
    "        \"solver\": [\"newton-cg\", \"lbfgs\", \"sag\"],\n",
    "        \"C\": [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "    },\n",
    "    {\n",
    "        \"penalty\": [\"none\"],\n",
    "        \"solver\": [\"newton-cg\", \"lbfgs\", \"sag\"],\n",
    "    }\n",
    "]\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "best_score = 0\n",
    "best_grid = None\n",
    "\n",
    "for k in range(5, pumpkin_seeds_df.shape[1]):\n",
    "    kBest = SelectKBest(score_func = f_classif, k = k)\n",
    "    best_features = kBest.fit(X_train, y_train)\n",
    "    only_keep = best_features.get_feature_names_out()\n",
    "\n",
    "    lgr = LogisticRegression(max_iter = 10000, random_state = 2)\n",
    "    clf = GridSearchCV(lgr, hyperparameters, n_jobs=-1, cv=3)\n",
    "    clf.fit(X_train[only_keep], y_train)\n",
    "\n",
    "    pred = clf.predict(X_val[only_keep])\n",
    "    accuracy = accuracy_score(y_val, pred)\n",
    "\n",
    "    if accuracy > best_score:\n",
    "        best_score = accuracy\n",
    "        best_grid = clf.best_params_\n",
    "        best_grid[\"selected features\"] = only_keep\n",
    "\n",
    "print(\"Best grid: \", best_grid)\n",
    "print(\"Best accuracy: \", (best_score * 100), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Univariate feature selection improves the accuracy of the model by .3%. It removed 2 features: `Equiv_Diameter` and `Convex_Area`. This score matches the score of the kNN model with hyperparameter tuning but is still less than the kNN model with feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='font-size:x-large'>Multilayer Perceptrons \\(MLP\\)</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter tuning using GridSearchCV**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values for hidden size were chosen by getting the floor of two\\-thirds of the number of features and adding or subtracting 1 \\($\\lfloor n * \\frac{2}{3}\\rfloor \\pm1$ where n is the number of features\\). The numbers for `alpha`, `batch_size`, and `learning_rate_init` are arbitrary numbers that were adjusted over the course of testing to improve accuracy. The solver `adam` and the activation function `relu` were chosen because they are somewhat fast and are just generally good defaults for neural network models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = [\n",
    "    {\n",
    "        'alpha': [0.001, 0.01, 0.1],\n",
    "        'hidden_layer_sizes': [(6,), (7,)],\n",
    "        \"batch_size\": [275, 300, 325],\n",
    "        \"learning_rate_init\": [0.01, 0.1, 1]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=MLPClassifier(max_iter=10000, random_state=2),\n",
       "             n_jobs=-1,\n",
       "             param_grid=[{'alpha': [0.001, 0.01, 0.1],\n",
       "                          'batch_size': [275, 300, 325],\n",
       "                          'hidden_layer_sizes': [(6,), (7,)],\n",
       "                          'learning_rate_init': [0.01, 0.1, 1]}])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "mlp = MLPClassifier(solver = 'adam', max_iter = 10000, activation = \"relu\", random_state = 2)\n",
    "clf = GridSearchCV(mlp, hyperparameters, n_jobs=-1, cv=3)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:\n",
      " {'alpha': 0.01, 'batch_size': 300, 'hidden_layer_sizes': (7,), 'learning_rate_init': 0.1}\n",
      "Best accuracy:  92.53333333333333 %\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Çerçevelik       0.92      0.94      0.93       195\n",
      "Ürgüp Sivrisi       0.94      0.91      0.92       180\n",
      "\n",
      "     accuracy                           0.93       375\n",
      "    macro avg       0.93      0.92      0.93       375\n",
      " weighted avg       0.93      0.93      0.93       375\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Best parameters found:\\n', clf.best_params_)\n",
    "y_pred = clf.predict(X_val)\n",
    "print(\"Best accuracy: \", (accuracy_score(y_val, y_pred) * 100), \"%\")\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter search found that the best values for `alpha`, `batch_size`, `hidden_layer_sizes`, and `learning_rate_init` are 0.01, 300, (7,), and 0.1 respectively. The accuracy of the tuned MLP classifier is 92.53% which is the highest that we have seen thus far among the other models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter tuning using GridSearchCV and Feature Selection using Univariate Selection (SelectKbest + f_classif)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will apply the same univariate feature selection method used in the other two model types. Like the one in LR, this model will start with `k = 5` to reduce the length of time that this code needs to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best grid:  {'alpha': 0.1, 'batch_size': 275, 'hidden_layer_sizes': (7,), 'learning_rate_init': 0.1, 'selected features': array(['Area', 'Perimeter', 'Major_Axis_Length', 'Minor_Axis_Length',\n",
      "       'Eccentricity', 'Solidity', 'Extent', 'Roundness', 'Aspect_Ration',\n",
      "       'Compactness'], dtype=object)}\n",
      "Best accuracy:  92.26666666666667 %\n"
     ]
    }
   ],
   "source": [
    "# Parameter selection\n",
    "\n",
    "hyperparameters = {\n",
    "    'alpha': [0.001, 0.01, 0.1],\n",
    "    \"batch_size\": [275, 300, 325],\n",
    "    \"learning_rate_init\": [0.01, 0.1, 1]\n",
    "}\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "best_score = 0\n",
    "best_grid = None\n",
    "\n",
    "for k in range(5, pumpkin_seeds_df.shape[1]):\n",
    "    kBest = SelectKBest(score_func = f_classif, k = k)\n",
    "    best_features = kBest.fit(X_train, y_train)\n",
    "    only_keep = best_features.get_feature_names_out()\n",
    "    hidden_layers = int(k * 2 / 3)\n",
    "    hyperparameters['hidden_layer_sizes'] = [(hidden_layers - 1,), (hidden_layers,), (hidden_layers + 1,)]\n",
    "\n",
    "    mlp = MLPClassifier(solver = 'adam', max_iter = 10000, activation = \"relu\", random_state = 2)\n",
    "    clf = GridSearchCV(mlp, hyperparameters, n_jobs=-1, cv=3)\n",
    "    clf.fit(X_train[only_keep], y_train)\n",
    "\n",
    "    pred = clf.predict(X_val[only_keep])\n",
    "    accuracy = accuracy_score(y_val, pred)\n",
    "\n",
    "    if accuracy > best_score:\n",
    "        best_score = accuracy\n",
    "        best_grid = clf.best_params_\n",
    "        best_grid[\"selected features\"] = only_keep\n",
    "\n",
    "print(\"Best grid: \", best_grid)\n",
    "print(\"Best accuracy: \", (best_score * 100), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strangely, performing feature selection on the MLP model actually decreased its score by .27%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Earlier Hyperparameter Tuning Testing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "hyperparameters = [\n",
    "    {\n",
    "        'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)],\n",
    "        'activation': ['tanh', 'relu'],\n",
    "        'solver': ['sgd', 'adam'],\n",
    "        'alpha': [0.0001, 0.05],\n",
    "        'learning_rate': ['constant','adaptive'],\n",
    "        \"random_state\": [2],\n",
    "    }\n",
    "]\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "mlp = MLPClassifier(max_iter=1000)\n",
    "clf = GridSearchCV(mlp, hyperparameters, n_jobs=-1, cv=3)\n",
    "clf.fit(X_train, y_train)\n",
    "# Best parameter set\n",
    "print('Best parameters found:\\n', clf.best_params_)\n",
    "\n",
    "# All results\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n",
    "y_pred = clf.predict(X_val)\n",
    "print(classification_report(y_val, y_pred))\n",
    "print(compute_accuracy(y_pred, y_val))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Earlier Hyperparameter Tuning and Feature Selection Method Testing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# With feature selection\n",
    "hyperparameters = [\n",
    "    {\n",
    "        'activation': ['tanh', 'relu'],\n",
    "        'alpha': [10, 50, 100],\n",
    "        'hidden_layer_sizes': [(50,2), (50,), (100,)],\n",
    "        \"batch_size\": [50, 100, 200, 50],\n",
    "        \"learning_rate_init\": [0.0001, 0.001, 0.01],\n",
    "        \"random_state\": [2],\n",
    "    }\n",
    "]\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "best_score = 0\n",
    "mlp = MLPClassifier(solver = 'adam', max_iter = 1000)\n",
    "\n",
    "for g in ParameterGrid(hyperparameters):\n",
    "    test = SelectKBest(score_func = f_classif, k = 7)\n",
    "    best_features = test.fit(X_train, y_train)\n",
    "    only_keep = best_features.get_feature_names_out()\n",
    "\n",
    "    mlp.set_params(**{i: g[i] for i in g if i != \"k-features\"})\n",
    "\n",
    "    mlp.fit(X_train[only_keep], y_train)\n",
    "\n",
    "    pred = mlp.predict(X_val[only_keep])\n",
    "    accuracy = accuracy_score(y_val, pred)\n",
    "\n",
    "    if accuracy > best_score:\n",
    "        best_score = accuracy\n",
    "        best_grid = g\n",
    "        best_grid[\"selected features\"] = best_features.get_feature_names_out()\n",
    "\n",
    "print(\"Best accuracy: \", best_score, \"%\")\n",
    "print(\"Best grid: \", best_grid)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| ML Models | Default Parameters | Hyperparameter Tuning | Feature Selection & Hyperparameter Tuning |\n",
    "| :------------------------ | :----------------- | :-------------------- | :---------------------------------------- |\n",
    "| kNN | 89.07% | 89.60% | 89.87% |\n",
    "| LR \\(SGDClassifier\\) | 88.00% | \\- | \\- |\n",
    "| LR \\(LogisticRegression\\) | 89.60% | 89.33% | 89.60% |\n",
    "| MLP | 90.13% | 92.53% | 92.27% |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the table above, it can be concluded that the MLP model with hyperparameter tuning has the best performance when trained on the training set and evaluated on the validation set. Because of this, it will be the final model that will be used. With the attained best accuracy of each machine learning model through experimentation, comparing to the paper, we have surpassed their accuracy in **k\\-NN \\(87.64% to 89.87%\\)**, **Logistic Regression \\(87.92% to 89.60%\\)**, and **Multilayer Perceptron \\(88.52% to 92.53%\\)**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(alpha=0.01, batch_size=300, hidden_layer_sizes=(7,),\n",
       "              learning_rate_init=0.1, max_iter=10000, random_state=2)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params = {\n",
    "    'alpha': 0.01,\n",
    "    'batch_size': 300,\n",
    "    'hidden_layer_sizes': (7,),\n",
    "    'learning_rate_init': 0.1\n",
    "}\n",
    "final_clf = MLPClassifier(solver = 'adam', max_iter = 10000, activation = \"relu\", random_state = 2)\n",
    "final_clf.set_params(**best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine the testing and validation data into one set to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val = pd.concat([X_train, X_val])\n",
    "y_train_val = np.concatenate((y_train, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy:  0.8986666666666666 %\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Çerçevelik       0.90      0.91      0.90       195\n",
      "Ürgüp Sivrisi       0.90      0.89      0.89       180\n",
      "\n",
      "     accuracy                           0.90       375\n",
      "    macro avg       0.90      0.90      0.90       375\n",
      " weighted avg       0.90      0.90      0.90       375\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_clf.fit(X_train_val, y_train_val)\n",
    "\n",
    "y_pred = final_clf.predict(X_test)\n",
    "print(\"Best accuracy: \", accuracy_score(y_test, y_pred), \"%\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
