{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a29bc5e-acba-40e7-b582-f4733d3eaf13",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Separate Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572e2b3d-dda8-48f6-b511-164eb0fe195f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "#TODO: change FOLDER_NAME and NUM_FOLDERS\n",
    "FOLDER_NAME = '11--Meeting'\n",
    "SOURCE_PATH = os.path.join(os.getcwd(), 'images', FOLDER_NAME)\n",
    "NUM_FOLDERS = 3\n",
    "\n",
    "for index, path in enumerate(glob.glob(os.path.join(SOURCE_PATH, '*.jpg'))):\n",
    "    #compute folder\n",
    "    folder_index = index % NUM_FOLDERS\n",
    "    \n",
    "    #create folder\n",
    "    DESTINATION_PATH = os.path.join(os.getcwd(), 'images', FOLDER_NAME + '_' + str(folder_index))\n",
    "    if not os.path.exists(DESTINATION_PATH):\n",
    "        os.mkdir(DESTINATION_PATH)\n",
    "    \n",
    "    file = os.path.basename(path) #extract file name\n",
    "    source = os.path.join(SOURCE_PATH, file) # source + file name\n",
    "    destination =  os.path.join(DESTINATION_PATH, file) # destination + file name\n",
    "    \n",
    "    os.rename(source, destination) #move images to the destination folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c8fd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "import gdown\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "import shutil\n",
    "from PIL import Image\n",
    "\n",
    "#libraries for yolo\n",
    "from pytorchyolo.models import load_model\n",
    "from pytorchyolo.utils.transforms import Resize, DEFAULT_TRANSFORMS\n",
    "from pytorchyolo.utils.utils import non_max_suppression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd6df4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_weights():\n",
    "    model_file=[\n",
    "        'yolo_face_sthanhng.weights',\n",
    "        'yolo_face_sthanhng.cfg'\n",
    "    ]\n",
    "    \n",
    "    gdrive_url=[\n",
    "        'https://drive.google.com/uc?id=1utquM5TAnfIa1Aq0X9fCvrllHiTWazdD',\n",
    "        'https://drive.google.com/uc?id=1CPUZlYL5ik4d9y6oCyzi0930KgzawI6V'\n",
    "    ]\n",
    "    \n",
    "    cwd=os.getcwd() \n",
    "    if 'weights' in os.listdir(cwd):\n",
    "        for i in range(len(model_file)):\n",
    "            if model_file[i] in os.listdir(os.path.join(cwd, 'weights')):\n",
    "                print(model_file[i] + ':: status : file already exists')\n",
    "            else:\n",
    "                gdown.download(gdrive_url[i],os.path.join(cwd, 'weights', model_file[i]), quiet=False)\n",
    "    else:\n",
    "        os.makedirs(os.path.join(cwd,'weights'))\n",
    "        for i in range(len(model_file)):\n",
    "            gdown.download(gdrive_url[i], os.path.join(cwd, 'weights', model_file[i]), quiet=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847df66c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# download the necessary weights for YOLO-Face\n",
    "download_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fd4752",
   "metadata": {},
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "sys.path.append('src_release')## YOLOFace with FGSM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4964d9-d8a9-421d-8111-12d51ab2218d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# YOLO Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a22160e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Patterned after FGSM tutorial (https://pytorch.org/tutorials/beginner/fgsm_tutorial.html)\n",
    "# Define what device we are using\n",
    "print(\"CUDA Available: \", torch.cuda.is_available())\n",
    "device, model = load_model('./weights/yolo_face_sthanhng.cfg', \"./weights/yolo_face_sthanhng.weights\")\n",
    "\n",
    "# Set the model in evaluation mode. In this case this is for the Dropout layers\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c215245",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detach_cpu(image):\n",
    "    return image.detach().cpu()\n",
    "\n",
    "# convert 1x3x416x416 to 416x416x3\n",
    "def reshape_image(image):\n",
    "    return np.transpose(np.squeeze(image), (1 ,2, 0))\n",
    "\n",
    "# convert 1x3x416x416 tensor to 416x416x3 numpy image\n",
    "def tensor_to_image(image):\n",
    "    return np.transpose(image.detach().cpu().squeeze().numpy(), (1, 2, 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7c3f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: update the paths as needed\n",
    "FOLDER_NAME = '3--Riot_3'\n",
    "FOLDER_PATH = os.path.join(os.getcwd(), 'images', FOLDER_NAME)\n",
    "BBOX_PATH = os.path.join(os.getcwd(), FOLDER_NAME + '_box')\n",
    "PAD_PATH = os.path.join(os.getcwd(), FOLDER_NAME + '_pad')\n",
    "ORIG_PAD_PATH = os.path.join(os.getcwd(), FOLDER_NAME + '_original_pad')\n",
    "PROCESS_PATH = os.path.join(os.getcwd(), FOLDER_NAME + '_process')\n",
    "CSV_PATH = os.path.join(os.getcwd(), FOLDER_NAME + '_csv')\n",
    "\n",
    "if not os.path.exists(BBOX_PATH):\n",
    "    os.mkdir(BBOX_PATH)\n",
    "\n",
    "if not os.path.exists(PAD_PATH):\n",
    "    os.mkdir(PAD_PATH)\n",
    "    \n",
    "if not os.path.exists(ORIG_PAD_PATH):\n",
    "    os.mkdir(ORIG_PAD_PATH)\n",
    "    \n",
    "if not os.path.exists(PROCESS_PATH):\n",
    "    os.mkdir(PROCESS_PATH)\n",
    "\n",
    "if not os.path.exists(CSV_PATH):\n",
    "    os.mkdir(CSV_PATH)\n",
    "    \n",
    "def pipeline(model, device):\n",
    "    \n",
    "    df = pd.DataFrame() # dataframe storing the dataset\n",
    "    row = {} #the information/columns for a single row in the dataset is stored here\n",
    "    \n",
    "    # Loop over all examples in test set\n",
    "    for path in glob.glob(os.path.join(FOLDER_PATH, '*.jpg')):\n",
    "        \n",
    "        # GET THE FILENAME\n",
    "        filename = os.path.basename(path)\n",
    "        filename = filename.split(\".\")[0]\n",
    "        print(filename)\n",
    "        \n",
    "        # INITIALIZE A FACE COUNTER\n",
    "        face_count = 0\n",
    "        \n",
    "        row['path'] = path\n",
    "        row['source_file'] = path.split(\"\\\\\")[-1]\n",
    "        # print(path)\n",
    "        # print(row['source_file'])\n",
    "        \n",
    "        # read and transform the image from the path\n",
    "        data = cv2.imread(path)  # read the image\n",
    "        data = cv2.cvtColor(data, cv2.COLOR_BGR2RGB) #change to rgb\n",
    "        data = transforms.Compose([DEFAULT_TRANSFORMS,Resize(416)])((data, np.zeros((1, 5))))[0].unsqueeze(0) # transform the image\n",
    "        \n",
    "        # Forward pass the data through the model\n",
    "        output = model(data)\n",
    "        \n",
    "        # call non max suppression\n",
    "        nms, nms_output = non_max_suppression(output, 0.5, 0.5) #conf_thres and iou_thres = 0.5\n",
    "        face_list = []\n",
    "        if type(nms_output[0]) is not int:\n",
    "            face_list = nms_output[0]\n",
    "        \n",
    "        # loop through each of the faces in the image\n",
    "        for face_index, face_row in enumerate(face_list): #nms_output[0] because the model is designed to take in several images at a time from the dataloader but we are only loading the image one at a time\n",
    "            # FACE_FILENAME\n",
    "            face_filename = filename + \"_\" + str(face_count)\n",
    "            row['face_index'] = face_index\n",
    "#             print('Face ', face_index)\n",
    "            \n",
    "            # get the coordinate of the face bounding box\n",
    "            #(x1, y1) upper left, (x2, y2) lower right\n",
    "            x, y, w, h = face_row[0], face_row[1], face_row[2], face_row[3]\n",
    "            \n",
    "            # cropped image with bounding box\n",
    "            # getting (x1, y1) upper left, (x2, y2) lower right\n",
    "            x1 = int(np.floor((x - w / 2).detach().cpu().numpy()))\n",
    "            y1 = int(np.floor((y - h / 2).detach().cpu().numpy()))\n",
    "            x2 = int(np.ceil((x + w / 2).detach().cpu().numpy()))\n",
    "            y2 = int(np.ceil((y + h / 2).detach().cpu().numpy()))\n",
    "            \n",
    "            row['x1'], row['y1'], row['x2'], row['y2'] = x1, y1, x2, y2\n",
    "            \n",
    "            # print('Cropped')\n",
    "            # print(x1, y1, x2, y2)\n",
    "            \n",
    "            bbox_image = detach_cpu(data)[:, :, y1:y2, x1:x2] #get the first dimension, the channels, and crop it\n",
    "            bbox_image = tensor_to_image(bbox_image) #reshape the image to (w/h, h/w, channel)\n",
    "            bbox_image = np.transpose(transforms.Compose([DEFAULT_TRANSFORMS,Resize(128)])((bbox_image, np.zeros((1, 5))))[0], (1, 2, 0)).numpy() # resize image to 128x128\n",
    "            bbox_image = (bbox_image * 255).astype(np.uint8)\n",
    "#             plt.imshow(bbox_image)\n",
    "#             plt.show()\n",
    "            \n",
    "#             bbox_image = cv2.cvtColor(bbox_image, cv2.COLOR_RGB2BGR)\n",
    "#             cv2.imwrite(os.path.join(BBOX_PATH, row['source_file'] + str(face_index) + '.jpg'), bbox_image)\n",
    "            \n",
    "            # getting (x1, y1) upper left, (x2, y2) lower right\n",
    "             # cropped image with bounding box + some padding\n",
    "            x1_pad = max(int(np.floor((x - w).detach().cpu().numpy())), 0) # prevent negative values\n",
    "            y1_pad = max(int(np.floor((y - h).detach().cpu().numpy())), 0)\n",
    "            x2_pad = min(int(np.ceil((x + w).detach().cpu().numpy())), 415) # prevent from getting out of range\n",
    "            y2_pad = min(int(np.ceil((y + h).detach().cpu().numpy())), 415)\n",
    "            \n",
    "            row['x1_pad'], row['y1_pad'], row['x2_pad'], row['y2_pad'] = x1_pad, y1_pad, x2_pad, y2_pad\n",
    "            \n",
    "            pad_image = detach_cpu(data)[:, :, y1_pad:y2_pad, x1_pad:x2_pad] #get the first dimension, the channels, and crop it\n",
    "            pad_image = tensor_to_image(pad_image) #reshape the image to (w/h, h/w, channel)\n",
    "            \n",
    "            # Original Pad Size\n",
    "            # GET THE LONGEST MAX AND THEN PAD\n",
    "            greater_size = max((x2_pad - x1_pad),(y2_pad - y1_pad))\n",
    "            orig_pad_image = np.transpose(transforms.Compose([DEFAULT_TRANSFORMS,Resize(greater_size)])((pad_image, np.zeros((1, 5))))[0], (1, 2, 0)).numpy() # resize image to GREATER SIZE\n",
    "            orig_pad_image = (orig_pad_image * 255).astype(np.uint8)\n",
    "            orig_pad_image = cv2.cvtColor(orig_pad_image, cv2.COLOR_RGB2BGR)         \n",
    "            cv2.imwrite(os.path.join(ORIG_PAD_PATH, \"mask_\" + face_filename + \"_image_final.png\"), orig_pad_image)\n",
    "            \n",
    "            pad_image = np.transpose(transforms.Compose([DEFAULT_TRANSFORMS,Resize(128)])((pad_image, np.zeros((1, 5))))[0], (1, 2, 0)).numpy() # resize image to 128x128\n",
    "            pad_image = (pad_image * 255).astype(np.uint8)\n",
    "#             plt.imshow(pad_image)\n",
    "#             plt.show()\n",
    "            \n",
    "            # SAVE THE ORIGINAL NAME OF THE IMAGE FIRST.... AND THEN FOR EACH FACE APPEND THE FACE COUNTER            \n",
    "            face_count += 1\n",
    "            \n",
    "            # ADD TO ROW FACE_FILENAME\n",
    "            row['mask_filename'] = \"mask_\" + face_filename + \"_image_final.png\"\n",
    "            \n",
    "            # SAVE AS 24-BIT PNG WITH THE FORMAT OF IMAGEFILENAME_NO STUFF AND SUFFIX\n",
    "            im = Image.fromarray(pad_image)\n",
    "            im.save(PROCESS_PATH + \"\\\\\" +  face_filename + \"_image_final.png\")\n",
    "            \n",
    "            # [DUPLICATE WITH THE BLACK.PNG]            \n",
    "            # Create the black _cc_occ_labels and _sp_labels (16 bit pngs)\n",
    "            cc_occ_png = PROCESS_PATH + \"\\\\\" +  face_filename + \"_cc_occ_labels.png\"\n",
    "            sp_png = PROCESS_PATH + \"\\\\\" +  face_filename + \"_sp_labels.png\"\n",
    "            mask_png = PROCESS_PATH + '\\\\' + face_filename + \"_mask.png\"\n",
    "            \n",
    "            # black.png is reference image being duplicated\n",
    "            shutil.copyfile(\"black.png\", (cc_occ_png))\n",
    "            shutil.copyfile(\"black.png\", (sp_png))    \n",
    "            \n",
    "            pad_image = cv2.cvtColor(pad_image, cv2.COLOR_RGB2BGR)\n",
    "            cv2.imwrite(os.path.join(PAD_PATH, row['source_file'] + str(face_index) + '.jpg'), pad_image)\n",
    "            \n",
    "            #add tow to dataset\n",
    "            df = df.append(row, ignore_index=True) #append the attributes of one face to the dataframe                              \n",
    "    df.to_csv(os.path.join(CSV_PATH, 'dataset' + '.csv'), index=False)  #save to csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50f4d7a-fb0d-4910-8f24-d43dd0120b3e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# FaceSeg Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfc1b58-cdbb-4a6f-88cb-7bdf20aeee4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "sys.path.append('src_release')\n",
    "\n",
    "#libraries for face segmentation\n",
    "from data_loader import get_dataloader\n",
    "from models.encoder_decoder_faceoccnet import FaceOccNet \n",
    "from torch_utils import torch_load_weights,evaluation,viz_notebook,plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b36df10-db2b-4241-89b7-3856bf10af25",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model_path = (\"./ptlabel_best_model.pth\")\n",
    "fs_model = FaceOccNet(input_channels=3, n_classes=3,is_regularized=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882b6e0f-c3c6-4cbe-9bfe-4f5783c8d1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "fs_model.to(fs_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a45f9e5-7819-4dd2-8bad-1f380e33f153",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "print('[Start] Model')\n",
    "print(fs_model)\n",
    "print('[End] Model')    \n",
    "print('[Start] Keras viz')\n",
    "summary(fs_model,(3,128,128))\n",
    "print('[End] Keras viz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebf209b-af3f-45b1-a02b-91f3888a0100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You have to set def load in serialization.py to have its map_location parameter = 'cpu'\n",
    "if os.path.exists(load_model_path) and os.path.isfile(load_model_path):\n",
    "    _, _ = torch_load_weights(fs_model, None, load_model_path, model_only=True)\n",
    "    print(f'Loaded model from {load_model_path}')\n",
    "else:\n",
    "    print(f'The model does not exist in {load_model_path} or is not a file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afa0032-b678-4839-9481-06001c159fba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm as fs_tqdm\n",
    "from skimage import measure\n",
    "import matplotlib.pyplot as plt\n",
    "from data_tools import decode_mask2img,encode_img2mask\n",
    "\n",
    "MASK_PATH = os.path.join(os.getcwd(), FOLDER_NAME + '_mask')\n",
    "\n",
    "if not os.path.exists(MASK_PATH):\n",
    "    os.mkdir(MASK_PATH)\n",
    "\n",
    "'''\n",
    "Visualization function for tensorboard and notebook\n",
    "'''\n",
    "def tf_viz_img(mask_tmp,i,pred=True):\n",
    "    if pred:\n",
    "        mask_tmp = torch.argmax(mask_tmp[i], dim=0).numpy().copy()\n",
    "    else:\n",
    "        mask_tmp = mask_tmp[i].numpy().copy()\n",
    "    mask_tmp = decode_mask2img(mask_tmp)\n",
    "    mask_tmp = np.transpose(mask_tmp, (2,0,1))\n",
    "    mask_tmp = mask_tmp / 255.0\n",
    "    return mask_tmp\n",
    "\n",
    "'''\n",
    "Main Visualization function\n",
    "'''\n",
    "def viz_notebook_brew(fs_model,eval_dataloader,fs_device,ibv_stop=-1):\n",
    "    import matplotlib.pyplot as plt\n",
    "    unorm = torchvision.transforms.Compose([ torchvision.transforms.Normalize((-1, -1, -1), (2, 2, 2))])\n",
    "    #batch_val = iter(eval_dataloader).next()\n",
    "    fs_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for ibv, batch_val in fs_tqdm(enumerate(eval_dataloader),\n",
    "                               desc='viz'):\n",
    "            fs_img, mask_gt, mask_sp, fn = batch_val\n",
    "            pred_mask, _ = fs_model(fs_img.to(fs_device))\n",
    "            pred_mask = pred_mask.cpu()\n",
    "            mask_gt = mask_gt.cpu().data\n",
    "            face_count = 0\n",
    "            for b in range(pred_mask.shape[0]):\n",
    "                pred_tmp = tf_viz_img(pred_mask,b,pred=True)\n",
    "                # mask_gt_tmp = tf_viz_img(mask_gt,b,pred=False)\n",
    "                pred_tmp = np.transpose(pred_tmp, (1,2,0))\n",
    "                # mask_gt_tmp = np.transpose(mask_gt_tmp, (1,2,0))\n",
    "                ##plotting\n",
    "#                 fig = plt.figure()\n",
    "#                 plt.subplot(1,3,1)\n",
    "#                 plt.title(f'Image {fs_img[b].shape[2]}')\n",
    "#                 plt.imshow(np.transpose(unorm(fs_img[b]), (1,2,0)))\n",
    "#                 plt.axis('off')\n",
    "                print(\"IMAGE FILENAME IS: \" + fn[face_count])\n",
    "                                \n",
    "                # TURN THE BLUE AND GREEN PRED_TMP TO WHITE\n",
    "                # Convert non-black pixels to white\n",
    "                non_black_pixels_mask = np.any(pred_tmp != [0, 0, 0], axis=-1)  \n",
    "                pred_tmp[non_black_pixels_mask] = [1, 1, 1]     \n",
    "                \n",
    "#                 plt.subplot(1,3,2)\n",
    "#                 plt.title(f'Prediction {pred_tmp.shape[0]}')\n",
    "#                 plt.imshow(pred_tmp)\n",
    "#                 plt.axis('off')                                                                                \n",
    "                plt.imsave(MASK_PATH + '\\\\' + 'mask_' + fn[face_count], pred_tmp)\n",
    "                \n",
    "                face_count+=1\n",
    "                \n",
    "#                 plt.show()\n",
    "#                 plt.close(fig)\n",
    "            if ibv_stop == ibv:\n",
    "                break        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83e05db-c29a-4fe5-b58c-25037ef817a6",
   "metadata": {},
   "source": [
    "# Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f661e02",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# [FACE EXTRACTION PART]\n",
    "pipeline(model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0160e920-7df2-4ce9-84ee-18e62120f88e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# [FACE SEGMENTATION PART]\n",
    "eval_dataloader = get_dataloader((PROCESS_PATH,),\n",
    "                      batch_size=64,\n",
    "                      mode='eval', \n",
    "                      num_workers = 4,\n",
    "                      n_classes=3,\n",
    "                      dataset_name='PartLabel')                        \n",
    "\n",
    "viz_notebook_brew(fs_model,eval_dataloader,fs_device,ibv_stop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fceb1c08-1acf-4216-8358-6a39602ab408",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "RESTORED_MASK_PATH = os.path.join(os.getcwd(), FOLDER_NAME + '_restored_mask')\n",
    "APPLIED_MASK_PATH = os.path.join(os.getcwd(), FOLDER_NAME + '_applied_mask')\n",
    "if not os.path.exists(RESTORED_MASK_PATH):\n",
    "    os.mkdir(RESTORED_MASK_PATH)\n",
    "    \n",
    "if not os.path.exists(APPLIED_MASK_PATH):\n",
    "    os.mkdir(APPLIED_MASK_PATH)    \n",
    "\n",
    "# LOAD DATASET\n",
    "faces_df = pd.read_csv(CSV_PATH + \"\\\\dataset.csv\")\n",
    "pixels_df = pd.DataFrame()\n",
    "no_whites_df = pd.DataFrame()\n",
    "\n",
    "# PER MASK FILES IN LOCATION\n",
    "masks = glob.glob(MASK_PATH + \"/*.png\")\n",
    "for mask in masks:\n",
    "    with open(mask, 'rb') as file:\n",
    "        # GET THE FILENAME       \n",
    "        filename = os.path.basename(MASK_PATH + mask)         \n",
    "        print(filename)\n",
    "                          \n",
    "        # MATCH FILENAME WITH DATASET\n",
    "        face_row = faces_df.loc[faces_df['mask_filename'] == filename]        \n",
    "        \n",
    "        # SET THE RESTORED FILENAME \n",
    "        restored_filename = \"restored_\" + filename                \n",
    "        face_row['filename'] = restored_filename  \n",
    "        \n",
    "        # GET THE ORIGINAL SIZE\n",
    "        x_size = face_row['x2_pad'] - face_row['x1_pad']\n",
    "        y_size = face_row['y2_pad'] - face_row['y1_pad']        \n",
    "        greater_size = max(x_size.item(), y_size.item())        \n",
    "        restored_size = (greater_size, greater_size)                \n",
    "        \n",
    "        # OPEN AND PROCESS IMAGE\n",
    "        img = Image.open(file).convert(\"RGB\")        \n",
    "        img = img.resize(restored_size)\n",
    "                    \n",
    "        img_arr = np.array(img)\n",
    "        \n",
    "        # Convert non-black pixels to white\n",
    "        non_black_pixels_mask = np.any(img_arr != [0, 0, 0], axis=-1)          \n",
    "        img_arr[non_black_pixels_mask] = [255, 255, 255]\n",
    "        \n",
    "        # CHECK NUMBER OF PIXELS AND UNIQUE VALUES WITH THIS\n",
    "        # unique, counts = np.unique(img_arr, return_counts=True)\n",
    "        # print(np.asarray((unique, counts)).T)\n",
    "        # CHECK UNIQUE VALUES WITH THIS\n",
    "        # with np.printoptions(threshold=np.inf):\n",
    "        #     print(img_arr)\n",
    "                                        \n",
    "        # COUNT PIXELS FASTER ALTERNATIVE\n",
    "        unique, counts = np.unique(img_arr, return_counts=True)\n",
    "        if unique.size == 2:\n",
    "            total_pixels = np.asarray((unique, counts)).T[1][1] / 3\n",
    "            face_row['pixels'] = total_pixels\n",
    "        else:\n",
    "            total_pixels = 0\n",
    "            face_row['pixels'] = total_pixels\n",
    "            no_whites_df = no_whites_df.append(face_row, ignore_index=True)            \n",
    "            # Convert everything to white\n",
    "            black_pixels_mask = np.any(img_arr == [0, 0, 0], axis=-1)\n",
    "            img_arr[black_pixels_mask] = [255, 255, 255]            \n",
    "        \n",
    "        pixels_df = pixels_df.append(face_row, ignore_index=True)\n",
    "        \n",
    "        # SAVE THE FILE\n",
    "        sum_img = Image.fromarray(img_arr)\n",
    "        sum_img = sum_img.save(RESTORED_MASK_PATH + \"\\\\\" + restored_filename)\n",
    "        \n",
    "        # MASKING HERE   \n",
    "        # OPEN THE FILE AND MATCH THE FILENAME \n",
    "        img_orig = Image.open(ORIG_PAD_PATH + \"\\\\\" + filename)\n",
    "        img_orig = np.array(img_orig)        \n",
    "        # Select the location of all black pixels\n",
    "        black_pixels_mask = np.any(img_arr == [0, 0, 0], axis=-1)\n",
    "        img_orig[black_pixels_mask] = [0, 0, 0]     \n",
    "        \n",
    "        sum_img_orig = Image.fromarray(img_orig)\n",
    "        sum_img_orig = sum_img_orig.save(APPLIED_MASK_PATH + \"\\\\\" + filename)\n",
    "        \n",
    "pixels_df.to_csv(os.path.join(CSV_PATH, 'dataset_pixels' + str(int(time.time())) + '.csv'), index=False)  #save to csv        \n",
    "no_whites_df.to_csv(os.path.join(CSV_PATH, 'dataset_no_whites' + str(int(time.time())) + '.csv'), index=False)  #save to csv      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef0ae4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f4f540",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
