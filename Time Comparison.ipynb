{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c8fd36",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import cv2\n",
    "import torchvision.transforms as transforms\n",
    "import joblib\n",
    "\n",
    "#libraries for yolo\n",
    "from pytorchyolo.models import load_model\n",
    "from pytorchyolo.utils.transforms import Resize, DEFAULT_TRANSFORMS\n",
    "from pytorchyolo.utils.utils import non_max_suppression\n",
    "from pytorchyolo.utils.loss import compute_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e89d98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.utils import detach_cpu, tensor_to_np_img, save_tensor_img, clone_detach, open_img_as_tensor, display_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7376ded",
   "metadata": {},
   "source": [
    "## Face Detector Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4aa6149",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scripts.face_detectors import MediaPipe, YuNet, YoloFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87537d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp = MediaPipe()\n",
    "yn = YuNet()\n",
    "yf = YoloFace()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3477648",
   "metadata": {},
   "source": [
    "## Our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3739291d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scripts.custom_mlp import *\n",
    "our_model = joblib.load(\"model_dumps/penalty_1.1578947368421053_HSV_bbox_e_face_yf.pkl\").best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fd4752",
   "metadata": {},
   "source": [
    "## YOLOFace with FGSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a22160e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patterned after FGSM tutorial (https://pytorch.org/tutorials/beginner/fgsm_tutorial.html)\n",
    "# Define what device we are using\n",
    "print(\"CUDA Available: \", torch.cuda.is_available())\n",
    "main_yf = YoloFace()\n",
    "device, model = main_yf.device, main_yf.yf_face_detector\n",
    "\n",
    "\n",
    "# Set the model in evaluation mode. In this case this is for the Dropout layers\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff92e19",
   "metadata": {},
   "source": [
    "## Image Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266919a0-06ad-4f2b-987c-c9fc5825b328",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scripts import image_attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfe2116",
   "metadata": {},
   "source": [
    "## Load Masks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cd60e3",
   "metadata": {},
   "source": [
    "## FGSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b600713",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import scripts.fgsm as fgsm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5af1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "image_attributes.save_color_images = False\n",
    "image_attributes.save_lbp_images = False\n",
    "image_attributes.save_gradient_images = False\n",
    "fgsm_loss_target = \"conf\" # or \"bbox\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba02a996",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = r\"./images/1--Handshaking/1_Handshaking_Handshaking_1_51.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf8a98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "masks[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68d1803",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "data = open_img_as_tensor(input_image)\n",
    "feats, grads, bboxes, masks = image_attributes.get_features(input_image)\n",
    "feats = preprocess(feats.loc[:, our_model.feature_names_in_])\n",
    "preds = our_model.predict(feats)\n",
    "for e, data_grad, bbox, mask in zip(preds, grads, bboxes, masks):\n",
    "    data = fgsm.fgsm_attack(data, e, data_grad, mask)\n",
    "t2 = time.time()\n",
    "print(\"Our regression model run time:\", t2 - t1)\n",
    "display_img(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19191864",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scripts.facesegmentor import FaceSegementor\n",
    "faceseg = FaceSegementor()\n",
    "\n",
    "t1 = time.time()\n",
    "data = open_img_as_tensor(input_image)\n",
    "feats, grads, bboxes, masks = image_attributes.get_features(input_image, face_segmentor=faceseg)\n",
    "feats = preprocess(feats.loc[:, our_model.feature_names_in_])\n",
    "preds = our_model.predict(feats)\n",
    "for e, data_grad, bbox, mask in zip(preds, grads, bboxes, masks):\n",
    "    data = fgsm.fgsm_attack(data, e, data_grad, mask)\n",
    "t2 = time.time()\n",
    "print(\"Our regression model run time:\", t2 - t1)\n",
    "display_img(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2661cd3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "data = open_img_as_tensor(input_image)\n",
    "feats, grads, bboxes = get_features(input_image)\n",
    "for data_grad, bbox in zip(grads, bboxes):\n",
    "    bbox_mask = np.zeros(data.shape)\n",
    "    bbox_mask[..., bbox[1]:bbox[3], bbox[0]:bbox[2]] = 1\n",
    "    e = fgsm.binary_search(clone_detach(data), clone_detach(data_grad), yf, bbox_mask, bbox)\n",
    "    data = fgsm.fgsm_attack(data, e, data_grad, bbox_mask)\n",
    "t2 = time.time()\n",
    "print(\"Binary Search run time:\", t2 - t1)\n",
    "display_img(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0219377c",
   "metadata": {},
   "outputs": [],
   "source": [
    "yf.detect?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3896c248",
   "metadata": {},
   "source": [
    "##### ___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
