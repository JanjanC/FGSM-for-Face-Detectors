{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c8fd36",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import cv2\n",
    "import torchvision.transforms as transforms\n",
    "import joblib\n",
    "\n",
    "#libraries for yolo\n",
    "from pytorchyolo.models import load_model\n",
    "from pytorchyolo.utils.transforms import Resize, DEFAULT_TRANSFORMS\n",
    "from pytorchyolo.utils.utils import non_max_suppression\n",
    "from pytorchyolo.utils.loss import compute_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e89d98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.utils import detach_cpu, tensor_to_np_img, save_tensor_img, clone_detach, open_img_as_tensor, display_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7376ded",
   "metadata": {},
   "source": [
    "## Face Detector Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4aa6149",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scripts.face_detectors import MediaPipe, YuNet, YoloFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87537d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp = MediaPipe()\n",
    "yn = YuNet()\n",
    "yf = YoloFace()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3477648",
   "metadata": {},
   "source": [
    "## Our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3739291d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scripts.custom_mlp import *\n",
    "our_model = joblib.load(\"model_dumps/best_one.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fd4752",
   "metadata": {},
   "source": [
    "## YOLOFace with FGSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a22160e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patterned after FGSM tutorial (https://pytorch.org/tutorials/beginner/fgsm_tutorial.html)\n",
    "# Define what device we are using\n",
    "print(\"CUDA Available: \", torch.cuda.is_available())\n",
    "main_yf = YoloFace()\n",
    "device, model = main_yf.device, main_yf.yf_face_detector\n",
    "\n",
    "\n",
    "# Set the model in evaluation mode. In this case this is for the Dropout layers\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff92e19",
   "metadata": {},
   "source": [
    "## Image Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266919a0-06ad-4f2b-987c-c9fc5825b328",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scripts import image_attributes\n",
    "from scripts.image_attributes import extract_image_attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfe2116",
   "metadata": {},
   "source": [
    "## Load Masks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cd60e3",
   "metadata": {},
   "source": [
    "## FGSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b600713",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import scripts.fgsm as fgsm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7c3f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(path):\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    \n",
    "    df = pd.DataFrame() # dataframe storing the dataset\n",
    "    row = {} #the information/columns for a single row in the dataset is stored here\n",
    "    grads = []\n",
    "    bboxes = []\n",
    "    \n",
    "    file_basename = os.path.basename(path)\n",
    "    print(file_basename, end=\" \")\n",
    "    print(\"<- working on\")\n",
    "\n",
    "    row['path'] = path\n",
    "\n",
    "    model.eval()\n",
    "    model.gradient_mode = False\n",
    "\n",
    "    for yolo_layer in model.yolo_layers:\n",
    "        yolo_layer.gradient_mode = False\n",
    "\n",
    "    # Read and transform the image from the path\n",
    "    data = cv2.imread(path)\n",
    "    row['source_w'], row['source_h'], _ = data.shape\n",
    "    data = cv2.cvtColor(data, cv2.COLOR_BGR2RGB)\n",
    "    data = transforms.Compose([DEFAULT_TRANSFORMS,Resize(416)])((data, np.zeros((1, 5))))[0].unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Forward pass the data through the model and call non max suppression\n",
    "        nms, nms_output = non_max_suppression(model(data), 0.5, 0.5) #conf_thres and iou_thres = 0.5\n",
    "\n",
    "    face_list = []\n",
    "    if type(nms_output[0]) is not int:\n",
    "        face_list = nms_output[0]\n",
    "\n",
    "    data = data.to(device)\n",
    "\n",
    "    # Set requires_grad attribute of tensor. Important for attack\n",
    "    data.requires_grad = True\n",
    "\n",
    "    model.gradient_mode = True\n",
    "    for yolo_layer in model.yolo_layers:\n",
    "        yolo_layer.gradient_mode = True\n",
    "\n",
    "    output = model(data)\n",
    "\n",
    "    # loop through each of the faces in the image\n",
    "    for face_index, face_row in enumerate(face_list): #nms_output[0] because the model is designed to take in several images at a time from the dataloader but we are only loading the image one at a time\n",
    "\n",
    "        row['face_index'] = face_index\n",
    "        print(\"Face\", face_index)\n",
    "\n",
    "        row['obj_score'] = face_row[4].item()\n",
    "        row['class_score'] = face_row[5].item()\n",
    "        x, y, w, h = face_row[0], face_row[1], face_row[2], face_row[3]\n",
    "\n",
    "        normal_x, normal_y, normal_w, normal_h = x / 415, y / 415, w / 415, h / 415\n",
    "\n",
    "        if fgsm_loss_target == \"bbox\":\n",
    "            target = torch.tensor([[face_row[4].item(), face_row[5].item(), 0, 0, 0, 0]])\n",
    "        elif fgsm_loss_target == \"conf\":\n",
    "            target = torch.tensor([[0.0, 0, normal_x, normal_y, normal_w, normal_h]])\n",
    "\n",
    "        target = target.to(device)\n",
    "        loss, loss_components = compute_loss(output, target, model)\n",
    "\n",
    "        # cropped image with bounding box\n",
    "        # getting (x1, y1) upper left, (x2, y2) lower right\n",
    "        x1 = max(int(np.floor((x - w / 2).detach().cpu().numpy())), 0)\n",
    "        y1 = max(int(np.floor((y - h / 2).detach().cpu().numpy())), 0)\n",
    "        x2 = min(int(np.ceil((x + w / 2).detach().cpu().numpy())), 415)\n",
    "        y2 = min(int(np.ceil((y + h / 2).detach().cpu().numpy())), 415)\n",
    "\n",
    "        row['x1'], row['y1'], row['x2'], row['y2'] = x1, y1, x2, y2\n",
    "        row['x'], row['y'], row['w'], row['h'] = x, y, w, h\n",
    "        \n",
    "        cropped_image = detach_cpu(data)[:, :, y1:y2, x1:x2] #get the first dimension, the channels, and crop it\n",
    "        cropped_image = tensor_to_np_img(cropped_image) #reshape the image to (w/h, h/w, channel)\n",
    "\n",
    "        # Zero all existing gradients\n",
    "        model.zero_grad()\n",
    "        data.grad = None\n",
    "\n",
    "        # Calculate gradients of model in backward pass\n",
    "        loss.backward(retain_graph=True) #TODO: Amos - check if this is correct\n",
    "        \n",
    "        # Collect datagrad\n",
    "        data_grad = data.grad.data\n",
    "        grads.append(clone_detach(data_grad))\n",
    "        \n",
    "        bbox = (x1, y1, x2, y2)\n",
    "        bboxes.append(bbox)\n",
    "        \n",
    "        row = extract_image_attributes(row, path, face_index, cropped_image, \"bbox\")\n",
    "        \n",
    "        df = pd.concat([df, pd.DataFrame([row])], axis=0, ignore_index=True)\n",
    "    \n",
    "    return df, grads, bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5af1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "image_attributes.save_color_images = False\n",
    "image_attributes.save_lbp_images = False\n",
    "image_attributes.save_gradient_images = False\n",
    "fgsm_loss_target = \"conf\" # or \"bbox\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba02a996",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = r\"./images/1--Handshaking/1_Handshaking_Handshaking_1_51.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68d1803",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "data = open_img_as_tensor(input_image)\n",
    "feats, grads, bboxes = get_features(input_image)\n",
    "feats = preprocess(feats.loc[:, our_model.feature_names_in_])\n",
    "preds = our_model.predict(feats)\n",
    "for e, data_grad, bbox in zip(preds, grads, bboxes):\n",
    "    bbox_mask = np.zeros(data.shape)\n",
    "    bbox_mask[..., bbox[1]:bbox[3], bbox[0]:bbox[2]] = 1\n",
    "    data = fgsm.fgsm_attack(data, e, data_grad, bbox_mask)\n",
    "t2 = time.time()\n",
    "print(\"Our regression model run time:\", t2 - t1)\n",
    "display_img(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2661cd3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "data = open_img_as_tensor(input_image)\n",
    "for data_grad, bbox in zip(grads, bboxes):\n",
    "    bbox_mask = np.zeros(data.shape)\n",
    "    bbox_mask[..., bbox[1]:bbox[3], bbox[0]:bbox[2]] = 1\n",
    "    e = fgsm.binary_search(clone_detach(data), clone_detach(data_grad), yf, bbox_mask, bbox)\n",
    "    data = fgsm.fgsm_attack(data, e, data_grad, bbox_mask)\n",
    "t2 = time.time()\n",
    "print(\"Binary Search run time:\", t2 - t1)\n",
    "display_img(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3896c248",
   "metadata": {},
   "source": [
    "##### ___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
