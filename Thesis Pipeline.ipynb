{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c8fd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "import gdown\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import cv2\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.feature as feature\n",
    "import xlwings as xw\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import random\n",
    "\n",
    "#libraries for yolo\n",
    "from pytorchyolo.models import load_model\n",
    "from pytorchyolo.utils.transforms import Resize, DEFAULT_TRANSFORMS\n",
    "from pytorchyolo.utils.utils import non_max_suppression\n",
    "from pytorchyolo.utils.loss import compute_loss\n",
    "\n",
    "from matplotlib.ticker import (FormatStrFormatter, AutoMinorLocator, FuncFormatter, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd6df4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_weights():\n",
    "    model_file=[\n",
    "        'yolo_face_sthanhng.weights',\n",
    "        'yolo_face_sthanhng.cfg'\n",
    "    ]\n",
    "    \n",
    "    gdrive_url=[\n",
    "        'https://drive.google.com/uc?id=1utquM5TAnfIa1Aq0X9fCvrllHiTWazdD',\n",
    "        'https://drive.google.com/uc?id=1CPUZlYL5ik4d9y6oCyzi0930KgzawI6V'\n",
    "    ]\n",
    "    \n",
    "    cwd=os.getcwd() \n",
    "    if 'weights' in os.listdir(cwd):\n",
    "        for i in range(len(model_file)):\n",
    "            if model_file[i] in os.listdir(os.path.join(cwd, 'weights')):\n",
    "                print(model_file[i] + ':: status : file already exists')\n",
    "            else:\n",
    "                gdown.download(gdrive_url[i],os.path.join(cwd, 'weights', model_file[i]), quiet=False)\n",
    "    else:\n",
    "        os.makedirs(os.path.join(cwd,'weights'))\n",
    "        for i in range(len(model_file)):\n",
    "            gdown.download(gdrive_url[i], os.path.join(cwd, 'weights', model_file[i]), quiet=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847df66c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# download the necessary weights for YOLO-Face\n",
    "download_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fd4752",
   "metadata": {},
   "source": [
    "## YOLOFace with FGSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a22160e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patterned after FGSM tutorial (https://pytorch.org/tutorials/beginner/fgsm_tutorial.html)\n",
    "# Define what device we are using\n",
    "print(\"CUDA Available: \", torch.cuda.is_available())\n",
    "device, model = load_model('./weights/yolo_face_sthanhng.cfg', \"./weights/yolo_face_sthanhng.weights\")\n",
    "\n",
    "# Set the model in evaluation mode. In this case this is for the Dropout layers\n",
    "model.eval()\n",
    "\n",
    "epsilons = [0, .05]\n",
    "use_cuda=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020b4f73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# FGSM attack code\n",
    "def fgsm_attack(image, epsilon, data_grad, x1, y1, x2, y2):\n",
    "    # Collect the element-wise sign of the data gradient\n",
    "    image = image\n",
    "    sign_data_grad = data_grad.sign()\n",
    "    # Create the perturbed image by adjusting each pixel of the input image\n",
    "    perturbed_image = image\n",
    "    perturbed_image[:, :, y1:y2, x1:x2] = perturbed_image[:, :, y1:y2, x1:x2] + epsilon * sign_data_grad[:, :, y1:y2, x1:x2] # apply it only to the face region\n",
    "    # Adding clipping to maintain [0,1] range\n",
    "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "    # Return the perturbed image\n",
    "    return perturbed_image\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff92e19",
   "metadata": {},
   "source": [
    "## Image Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266919a0-06ad-4f2b-987c-c9fc5825b328",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalBinaryPatterns:\n",
    "  def __init__(self, numPoints, radius):\n",
    "    self.numPoints = numPoints\n",
    "    self.radius = radius\n",
    "\n",
    "  def describe(self, image, eps = 1e-7):\n",
    "    lbp = feature.local_binary_pattern(image, self.numPoints, self.radius, method=\"uniform\")\n",
    "    (hist, _) = np.histogram(lbp.ravel(), bins=np.arange(0, self.numPoints+3), range=(0, self.numPoints + 2))\n",
    "\n",
    "    # Normalize the histogram\n",
    "    hist = hist.astype('float')\n",
    "    hist /= (hist.sum() + eps)\n",
    "\n",
    "    return hist, lbp\n",
    "\n",
    "# From https://medium.com/mlearning-ai/how-to-plot-color-channels-histogram-of-an-image-in-python-using-opencv-40022032e127\n",
    "# Extracts image's color channel\n",
    "def extract_color_channel(path, face_index, image):\n",
    "    # BGR Image Color Conversion\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    hls = cv2.cvtColor(image, cv2.COLOR_BGR2HLS)\n",
    "    lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
    "    ycrcb = cv2.cvtColor(image, cv2.COLOR_BGR2YCrCb)\n",
    "    \n",
    "    COLOR_PATH = os.path.join(FOLDER_PATH, FOLDER_NAME + '_Colors')\n",
    "    if not os.path.exists(COLOR_PATH):\n",
    "        os.mkdir(COLOR_PATH)\n",
    "    \n",
    "    cv2.imwrite(os.path.join(COLOR_PATH, os.path.splitext(os.path.basename(path))[0]) + '_RGB_' + str(face_index) + '.png', rgb)\n",
    "    cv2.imwrite(os.path.join(COLOR_PATH, os.path.splitext(os.path.basename(path))[0]) + '_HSV_' + str(face_index) + '.png', hsv)\n",
    "    cv2.imwrite(os.path.join(COLOR_PATH, os.path.splitext(os.path.basename(path))[0]) + '_HSL_' + str(face_index) + '.png', hls)\n",
    "    cv2.imwrite(os.path.join(COLOR_PATH, os.path.splitext(os.path.basename(path))[0]) + '_LAB_' + str(face_index) + '.png', lab)\n",
    "    cv2.imwrite(os.path.join(COLOR_PATH, os.path.splitext(os.path.basename(path))[0]) + '_YCRCB_' + str(face_index) + '.png', ycrcb)\n",
    "\n",
    "#     # RGB Image Histogram\n",
    "#     red_hist = cv2.calcHist([rgb], [0], None, [256], [0, 256])\n",
    "#     green_hist = cv2.calcHist([rgb], [1], None, [256], [0, 256])\n",
    "#     blue_hist = cv2.calcHist([rgb], [2], None, [256], [0, 256])\n",
    "\n",
    "#     # HSV Image Histogram\n",
    "#     hue_hist_HSV = cv2.calcHist([hsv], [0], None, [256], [0, 256])\n",
    "#     saturation_hist_HSV = cv2.calcHist([hsv], [1], None, [256], [0, 256])\n",
    "#     value_hist = cv2.calcHist([hsv], [2], None, [256], [0, 256])\n",
    "\n",
    "#     # HLS Image Histogram\n",
    "#     hue_hist_HLS = cv2.calcHist([hls], [0], None, [256], [0, 256])\n",
    "#     lightness_hist_HLS = cv2.calcHist([hls], [1], None, [256], [0, 256])\n",
    "#     saturation_hist_HLS = cv2.calcHist([hls], [2], None, [256], [0, 256])\n",
    "\n",
    "#     # LAB Image Histogram\n",
    "#     lightness_hist_LAB = cv2.calcHist([lab], [0], None, [256], [0, 256])\n",
    "#     a_hist_LAB = cv2.calcHist([lab], [1], None, [256], [0, 256])\n",
    "#     b_hist_LAB = cv2.calcHist([lab], [2], None, [256], [0, 256])\n",
    "\n",
    "#     # YCrCb Image Histogram\n",
    "#     y_hist = cv2.calcHist([ycrcb], [0], None, [256], [0, 256])\n",
    "#     cr_hist = cv2.calcHist([ycrcb], [1], None, [256], [0, 256])\n",
    "#     cb_hist = cv2.calcHist([ycrcb], [2], None, [256], [0, 256])\n",
    "\n",
    "#     # RGB Image Plot\n",
    "#     plt.subplot(4, 1, 1)\n",
    "#     plt.imshow(rgb)\n",
    "#     plt.title('RGB Image')\n",
    "#     plt.xticks([])\n",
    "#     plt.yticks([])\n",
    "\n",
    "#     plt.subplot(4, 1, 2)\n",
    "#     plt.plot(red_hist, color='r')\n",
    "#     plt.xlim([0, 256])\n",
    "#     plt.ylim([0, 500])\n",
    "#     plt.title('Red Histogram')\n",
    "\n",
    "#     plt.subplot(4, 1, 3)\n",
    "#     plt.plot(green_hist, color='g')\n",
    "#     plt.xlim([0, 256])\n",
    "#     plt.ylim([0, 500])\n",
    "#     plt.title('Green Histogram')\n",
    "\n",
    "#     plt.subplot(4, 1, 4)\n",
    "#     plt.plot(blue_hist, color='b')\n",
    "#     plt.xlim([0, 256])\n",
    "#     plt.ylim([0, 500])\n",
    "#     plt.title('Blue Histogram')\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     #plt.show()\n",
    "\n",
    "    r, g, b = cv2.split(rgb)\n",
    "    \n",
    "    r *= 255\n",
    "    g *= 255\n",
    "    b *= 255\n",
    "    \n",
    "    RGB_PATH = os.path.join(FOLDER_PATH, FOLDER_NAME + '_RGB')\n",
    "    if not os.path.exists(RGB_PATH):\n",
    "        os.mkdir(RGB_PATH)\n",
    "    \n",
    "    cv2.imwrite(os.path.join(RGB_PATH, os.path.splitext(os.path.basename(path))[0]) + '_R_RGB_' + str(face_index) + '.png', r)\n",
    "    cv2.imwrite(os.path.join(RGB_PATH, os.path.splitext(os.path.basename(path))[0]) + '_G_RGB_' + str(face_index) + '.png', g)\n",
    "    cv2.imwrite(os.path.join(RGB_PATH, os.path.splitext(os.path.basename(path))[0]) + '_B_RGB_' + str(face_index) + '.png', b)\n",
    "    \n",
    "    r_hist = cv2.calcHist(r, [0], None, [26], [0, 256])\n",
    "    r_hist = r_hist.ravel()\n",
    "    r_hist = r_hist.astype('float')\n",
    "    r_hist /= r_hist.sum()\n",
    "    \n",
    "    g_hist = cv2.calcHist([g], [0], None, [26], [0, 256])\n",
    "    g_hist = g_hist.ravel()\n",
    "    g_hist = g_hist.astype('float')\n",
    "    g_hist /= g_hist.sum()\n",
    "    \n",
    "    b_hist = cv2.calcHist([b], [0], None, [26], [0, 256])\n",
    "    b_hist = b_hist.ravel()\n",
    "    b_hist = b_hist.astype('float')\n",
    "    b_hist /= b_hist.sum()\n",
    "\n",
    "#     # HSV Image Plot\n",
    "#     plt.subplot(4, 1, 1)\n",
    "#     #plt.imshow(hsv)\n",
    "#     plt.title('HSV Image')\n",
    "#     plt.xticks([])\n",
    "#     plt.yticks([])\n",
    "\n",
    "#     plt.subplot(4, 1, 2)\n",
    "#     plt.plot(hue_hist_HSV, color='c')\n",
    "#     plt.xlim([0, 256])\n",
    "#     plt.ylim([0, 2500])\n",
    "#     plt.title('Hue Histogram')\n",
    "\n",
    "#     plt.subplot(4, 1, 3)\n",
    "#     plt.plot(saturation_hist_HSV, color='m')\n",
    "#     plt.xlim([0, 256])\n",
    "#     plt.ylim([0, 1000])\n",
    "#     plt.title('Saturation Histogram')\n",
    "\n",
    "#     plt.subplot(4, 1, 4)\n",
    "#     plt.plot(value_hist, color='y')\n",
    "#     plt.xlim([0, 256])\n",
    "#     plt.ylim([0, 1000])\n",
    "#     plt.title('Value Histogram')\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "    \n",
    "    h, s, v = cv2.split(hsv)\n",
    "    \n",
    "    s *= 255\n",
    "    v *= 255\n",
    "    \n",
    "    HSV_PATH = os.path.join(FOLDER_PATH, FOLDER_NAME + '_HSV')\n",
    "    if not os.path.exists(HSV_PATH):\n",
    "        os.mkdir(HSV_PATH)\n",
    "    \n",
    "    cv2.imwrite(os.path.join(HSV_PATH, os.path.splitext(os.path.basename(path))[0]) + '_H_HSV_' + str(face_index) + '.png', h)\n",
    "    cv2.imwrite(os.path.join(HSV_PATH, os.path.splitext(os.path.basename(path))[0]) + '_S_HSV_' + str(face_index) + '.png', s)\n",
    "    cv2.imwrite(os.path.join(HSV_PATH, os.path.splitext(os.path.basename(path))[0]) + '_V_HSV_' + str(face_index) + '.png', v)\n",
    "    \n",
    "    h_hist_HSV = cv2.calcHist([h], [0], None, [36], [0, 361])\n",
    "    h_hist_HSV = h_hist_HSV.ravel()\n",
    "    h_hist_HSV = h_hist_HSV.astype('float')\n",
    "    h_hist_HSV /= h_hist_HSV.sum()\n",
    "    \n",
    "    s_hist_HSV = cv2.calcHist([s], [0], None, [26], [0, 256])\n",
    "    s_hist_HSV = s_hist_HSV.ravel()\n",
    "    s_hist_HSV = s_hist_HSV.astype('float')\n",
    "    s_hist_HSV /= s_hist_HSV.sum()\n",
    "    \n",
    "    v_hist_HSV = cv2.calcHist([v], [0], None, [26], [0, 256])\n",
    "    v_hist_HSV = v_hist_HSV.ravel()\n",
    "    v_hist_HSV = v_hist_HSV.astype('float')\n",
    "    v_hist_HSV /= v_hist_HSV.sum()\n",
    "    \n",
    "#     # HLS Image Plot\n",
    "#     plt.subplot(4, 1, 1)\n",
    "#     plt.imshow(hls)\n",
    "#     plt.title('HLS Image')\n",
    "#     plt.xticks([])\n",
    "#     plt.yticks([])\n",
    "\n",
    "#     plt.subplot(4, 1, 2)\n",
    "#     plt.plot(hue_hist_HLS, color='r')\n",
    "#     plt.xlim([0, 256])\n",
    "#     plt.ylim([0, 2500])\n",
    "#     plt.title('Hue Histogram')\n",
    "\n",
    "#     plt.subplot(4, 1, 3)\n",
    "#     plt.plot(lightness_hist_HLS, color='g')\n",
    "#     plt.xlim([0, 256])\n",
    "#     plt.ylim([0, 1000])\n",
    "#     plt.title('Lightness Histogram')\n",
    "\n",
    "#     plt.subplot(4, 1, 4)\n",
    "#     plt.plot(saturation_hist_HLS, color='b')\n",
    "#     plt.xlim([0, 256])\n",
    "#     plt.ylim([0, 1000])\n",
    "#     plt.title('Saturation Histogram')\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "    h, l, s = cv2.split(hls)\n",
    "    \n",
    "    l *= 255\n",
    "    s *= 255\n",
    "    \n",
    "    HSL_PATH = os.path.join(FOLDER_PATH, FOLDER_NAME + '_HSL')\n",
    "    if not os.path.exists(HSL_PATH):\n",
    "        os.mkdir(HSL_PATH)\n",
    "    \n",
    "    cv2.imwrite(os.path.join(HSL_PATH, os.path.splitext(os.path.basename(path))[0]) + '_H_HSL_' + str(face_index) + '.png', h)\n",
    "    cv2.imwrite(os.path.join(HSL_PATH, os.path.splitext(os.path.basename(path))[0]) + '_S_HSL_' + str(face_index) + '.png', s)\n",
    "    cv2.imwrite(os.path.join(HSL_PATH, os.path.splitext(os.path.basename(path))[0]) + '_L_HSL_' + str(face_index) + '.png', l)\n",
    "    \n",
    "    h_hist_HSL = cv2.calcHist([h], [0], None, [36], [0, 361])\n",
    "    h_hist_HSL = h_hist_HSL.ravel()\n",
    "    h_hist_HSL = h_hist_HSL.astype('float')\n",
    "    h_hist_HSL /= h_hist_HSL.sum()\n",
    "    \n",
    "    l_hist_HSL = cv2.calcHist([l], [0], None, [26], [0, 256])\n",
    "    l_hist_HSL = l_hist_HSL.ravel()\n",
    "    l_hist_HSL = l_hist_HSL.astype('float')\n",
    "    l_hist_HSL /= l_hist_HSL.sum()\n",
    "    \n",
    "    s_hist_HSL = cv2.calcHist([s], [0], None, [26], [0, 256])\n",
    "    s_hist_HSL = s_hist_HSL.ravel()\n",
    "    s_hist_HSL = s_hist_HSL.astype('float')\n",
    "    s_hist_HSL /= s_hist_HSL.sum()\n",
    "    \n",
    "#     # LAB Image Plot\n",
    "#     plt.subplot(4, 1, 1)\n",
    "#     plt.imshow(lab)\n",
    "#     plt.title('LAB Image')\n",
    "#     plt.xticks([])\n",
    "#     plt.yticks([])\n",
    "\n",
    "#     plt.subplot(4, 1, 2)\n",
    "#     plt.plot(lightness_hist_LAB, color='c')\n",
    "#     plt.xlim([0, 256])\n",
    "#     plt.ylim([0, 1000])\n",
    "#     plt.title('Lightness Histogram')\n",
    "\n",
    "#     plt.subplot(4, 1, 3)\n",
    "#     plt.plot(a_hist_LAB, color='m')\n",
    "#     plt.xlim([0, 256])\n",
    "#     plt.ylim([0, 20000])\n",
    "#     plt.title('A Histogram')\n",
    "\n",
    "#     plt.subplot(4, 1, 4)\n",
    "#     plt.plot(b_hist_LAB, color='y')\n",
    "#     plt.xlim([0, 256])\n",
    "#     plt.ylim([0, 20000])\n",
    "#     plt.title('B Histogram')\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "    \n",
    "    l, a, b = cv2.split(lab)\n",
    "    \n",
    "    LAB_PATH = os.path.join(FOLDER_PATH, FOLDER_NAME + '_LAB')\n",
    "    if not os.path.exists(LAB_PATH):\n",
    "        os.mkdir(LAB_PATH)\n",
    "    \n",
    "    cv2.imwrite(os.path.join(LAB_PATH, os.path.splitext(os.path.basename(path))[0]) + '_L_LAB_' + str(face_index) + '.png', l)\n",
    "    cv2.imwrite(os.path.join(LAB_PATH, os.path.splitext(os.path.basename(path))[0]) + '_A_LAB_' + str(face_index) + '.png', a)\n",
    "    cv2.imwrite(os.path.join(LAB_PATH, os.path.splitext(os.path.basename(path))[0]) + '_B_LAB_' + str(face_index) + '.png', b)\n",
    "    \n",
    "    l_hist_LAB = cv2.calcHist([l], [0], None, [26], [0, 256])\n",
    "    l_hist_LAB = l_hist_LAB.ravel()\n",
    "    l_hist_LAB = l_hist_LAB.astype('float')\n",
    "    l_hist_LAB /= l_hist_LAB.sum()\n",
    "    \n",
    "    a_hist_LAB = cv2.calcHist([a], [0], None, [26], [0, 256])\n",
    "    a_hist_LAB = a_hist_LAB.ravel()\n",
    "    a_hist_LAB = a_hist_LAB.astype('float')\n",
    "    a_hist_LAB /= a_hist_LAB.sum()\n",
    "    \n",
    "    b_hist_LAB = cv2.calcHist([b], [0], None, [26], [0, 256])\n",
    "    b_hist_LAB = b_hist_LAB.ravel()\n",
    "    b_hist_LAB = b_hist_LAB.astype('float')\n",
    "    b_hist_LAB /= b_hist_LAB.sum()\n",
    "    \n",
    "#     # YCrCb Image Plot\n",
    "#     plt.subplot(4, 1, 1)\n",
    "#     plt.imshow(ycrcb)\n",
    "#     plt.title('YCrCb Image')\n",
    "#     plt.xticks([])\n",
    "#     plt.yticks([])\n",
    "\n",
    "#     plt.subplot(4, 1, 2)\n",
    "#     plt.plot(y_hist, color='r')\n",
    "#     plt.xlim([0, 256])\n",
    "#     plt.ylim([0, 1000])\n",
    "#     plt.title('Y Histogram')\n",
    "\n",
    "#     plt.subplot(4, 1, 3)\n",
    "#     plt.plot(cr_hist, color='g')\n",
    "#     plt.xlim([0, 256])\n",
    "#     plt.ylim([0, 20000])\n",
    "#     plt.title('Cr Histogram')\n",
    "\n",
    "#     plt.subplot(4, 1, 4)\n",
    "#     plt.plot(cb_hist, color='b')\n",
    "#     plt.xlim([0, 256])\n",
    "#     plt.ylim([0, 20000])\n",
    "#     plt.title('Cb Histogram')\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "    \n",
    "    y, cr, cb = cv2.split(ycrcb)\n",
    "    \n",
    "    y *= 255\n",
    "    cr *= 255\n",
    "    cb *= 255\n",
    "    \n",
    "    YCRCB_PATH = os.path.join(FOLDER_PATH, FOLDER_NAME + '_YCRCB')\n",
    "    if not os.path.exists(YCRCB_PATH):\n",
    "        os.mkdir(YCRCB_PATH)\n",
    "    \n",
    "    cv2.imwrite(os.path.join(YCRCB_PATH, os.path.splitext(os.path.basename(path))[0]) + '_Y_YCRCB_' + str(face_index) + '.png', y)\n",
    "    cv2.imwrite(os.path.join(YCRCB_PATH, os.path.splitext(os.path.basename(path))[0]) + '_CR_YCRCB_' + str(face_index) + '.png', cr)\n",
    "    cv2.imwrite(os.path.join(YCRCB_PATH, os.path.splitext(os.path.basename(path))[0]) + '_CB_YCRCB_' + str(face_index) + '.png', cb)\n",
    "    \n",
    "    \n",
    "    y_hist = cv2.calcHist([y], [0], None, [26], [0, 256])\n",
    "    y_hist = y_hist.ravel()\n",
    "    y_hist = y_hist.astype('float')\n",
    "    y_hist /= y_hist.sum()\n",
    "    \n",
    "    cr_hist = cv2.calcHist([cr], [0], None, [26], [0, 256])\n",
    "    cr_hist = cr_hist.ravel()\n",
    "    cr_hist = cr_hist.astype('float')\n",
    "    cr_hist /= cr_hist.sum()\n",
    "    \n",
    "    cb_hist = cv2.calcHist([cb], [0], None, [26], [0, 256])\n",
    "    cb_hist = cb_hist.ravel()\n",
    "    cb_hist = cb_hist.astype('float')\n",
    "    cb_hist /= cb_hist.sum()\n",
    "    \n",
    "    face_index = str(face_index)\n",
    "    # rows = itertools.zip_longest([path], [face_index], r_hist, g_hist, b_hist, h_hist_HSV, s_hist_HSV, v_hist_HSV, h_hist_HSL, s_hist_HSL, l_hist_HSL, l_hist_LAB, a_hist_LAB, b_hist_LAB, y_hist, cr_hist, cb_hist)\n",
    "    \n",
    "    # with open(\"color.csv\", \"a\", newline = \"\") as f:\n",
    "    #     if os.stat(\"color.csv\").st_size == 0:\n",
    "    #         csv.writer(f).writerow([\"Path\", \"Face Index\", \"Red\", \"Green\", \"Blue\", \"Hue_HSV\", \"Saturation_HSV\", \"Value_HSV\", \"Hue_HSL\", \"Saturation_HSL\", \"Lightness_HSL\", \"Lightness_LAB\", \"A_LAB\", \"B_LAB\", \"Y\", \"Cr\", \"Cb\"])\n",
    "    #     csv.writer(f).writerows(rows)\n",
    "    \n",
    "    return r_hist, g_hist, b_hist, h_hist_HSV, s_hist_HSV, v_hist_HSV, h_hist_HSL, s_hist_HSL, l_hist_HSL, l_hist_LAB, a_hist_LAB, b_hist_LAB, y_hist, cr_hist, cb_hist\n",
    "\n",
    "# From https://medium.com/mlearning-ai/color-shape-and-texture-feature-extraction-using-opencv-cb1feb2dbd73\n",
    "# Extracts Local Binary Pattern (Texture) of an image\n",
    "def extract_lbp(path, face_index, image):\n",
    "    # reads the input image as a grayscale image\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    desc = LocalBinaryPatterns(24, 8)\n",
    "    lbp_hist, lbp_img = desc.describe(gray)\n",
    "    \n",
    "    LBP_PATH = os.path.join(FOLDER_PATH, FOLDER_NAME + '_LBP')\n",
    "    if not os.path.exists(LBP_PATH):\n",
    "        os.mkdir(LBP_PATH)\n",
    "    \n",
    "    cv2.imwrite(os.path.join(LBP_PATH, os.path.splitext(os.path.basename(path))[0]) + '_LBP_' + str(face_index) + '.png', lbp_img)\n",
    "    \n",
    "    # plt.imshow(lbp_img, cmap = plt.get_cmap('gray'))\n",
    "    # plt.show()\n",
    "    # lbp_hist = cv2.calcHist([lbp_img], [0], None, [256], [0, 256])\n",
    "    \n",
    "    # face_index = str(face_index)\n",
    "    # rows = itertools.zip_longest([path], [face_index], lbp_hist)\n",
    "    \n",
    "    # with open(\"lbp.csv\", \"a\", newline = \"\") as f:\n",
    "    #     if os.stat(\"lbp.csv\").st_size == 0:\n",
    "    #         csv.writer(f).writerow([\"Path\", \"Face Index\", \"LBP\"])\n",
    "    #     csv.writer(f).writerows(rows)\n",
    "    \n",
    "    return lbp_hist\n",
    "    \n",
    "# From https://docs.opencv.org/4.x/d2/d2c/tutorial_sobel_derivatives.html and https://gist.github.com/rahit/c078cabc0a48f2570028bff397a9e154\n",
    "def extract_gradients(path, face_index, image):\n",
    "    # Uses the Sobel Filter to extract the gradients of an image\n",
    "    # reads the input image, then converts BGR color space to RGB\n",
    "    # img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # compute the 1st order Sobel derivative in X-direction\n",
    "    sobelx = cv2.Sobel(image, cv2.CV_64F, 1, 0, ksize=5)\n",
    "\n",
    "    # compute the 1st order Sobel derivative in Y-direction\n",
    "    sobely = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize=5)\n",
    "    \n",
    "    # combine sobelx and sobely to form sobel\n",
    "    sobel = sobelx + sobely\n",
    "    \n",
    "    SOBEL_PATH = os.path.join(FOLDER_PATH, FOLDER_NAME + '_SOBEL')\n",
    "    if not os.path.exists(SOBEL_PATH):\n",
    "        os.mkdir(SOBEL_PATH)\n",
    "    \n",
    "    cv2.imwrite(os.path.join(SOBEL_PATH, os.path.splitext(os.path.basename(path))[0]) + '_SOBELX_' + str(face_index) + '.png', sobelx)\n",
    "    cv2.imwrite(os.path.join(SOBEL_PATH, os.path.splitext(os.path.basename(path))[0]) + '_SOBELY_' + str(face_index) + '.png', sobely)\n",
    "    cv2.imwrite(os.path.join(SOBEL_PATH, os.path.splitext(os.path.basename(path))[0]) + '_SOBEL_' + str(face_index) + '.png', sobel)\n",
    "\n",
    "    # # display sobelx, sobely, and sobel\n",
    "    # plt.imshow(sobelx, cmap = \"gray\")\n",
    "    # plt.show()\n",
    "    # plt.imshow(sobely, cmap = \"gray\")\n",
    "    # plt.show()\n",
    "    # plt.imshow(sobel, cmap = \"gray\")\n",
    "    # plt.show()\n",
    "    \n",
    "    sobelx_hist = cv2.calcHist([np.float32(sobelx)], [0], None, [26], [0, 256])\n",
    "    sobelx_hist = sobelx_hist.ravel()\n",
    "    sobelx_hist = sobelx_hist.astype('float')\n",
    "    sobelx_hist /= sobelx_hist.sum()\n",
    "    \n",
    "    sobely_hist = cv2.calcHist([np.float32(sobely)], [0], None, [26], [0, 256])\n",
    "    sobely_hist = sobely_hist.ravel()\n",
    "    sobely_hist = sobely_hist.astype('float')\n",
    "    sobely_hist /= sobely_hist.sum()\n",
    "    \n",
    "    sobel_hist = cv2.calcHist([np.float32(sobel)], [0], None, [26], [0, 256])\n",
    "    sobel_hist = sobel_hist.ravel()\n",
    "    sobel_hist = sobel_hist.astype('float')\n",
    "    sobel_hist /= sobel_hist.sum()\n",
    "    \n",
    "    # face_index = str(face_index)\n",
    "    # rows = itertools.zip_longest([path], [face_index], sobelx_hist, sobely_hist, sobel_hist)\n",
    "    \n",
    "    # with open(\"gradient.csv\", \"a\", newline = \"\") as f:\n",
    "    #     if os.stat(\"gradient.csv\").st_size == 0:\n",
    "    #         csv.writer(f).writerow([\"Path\", \"Face Index\", \"Sobel X\", \"Sobel Y\", \"Sobel\"])\n",
    "    #     csv.writer(f).writerows(rows)\n",
    "    \n",
    "    return sobelx_hist, sobely_hist, sobel_hist\n",
    "\n",
    "def extract_image_attributes(row, path, face_index, image):\n",
    "    r_hist, g_hist, b_hist, h_hist_HSV, s_hist_HSV, v_hist_HSV, h_hist_HSL, s_hist_HSL, l_hist_HSL, l_hist_LAB, a_hist_LAB, b_hist_LAB, y_hist, cr_hist, cb_hist = extract_color_channel(path, face_index, image)\n",
    "    lbp_hist = extract_lbp(path, face_index, image)\n",
    "    sobelx_hist, sobely_hist, sobel_hist = extract_gradients(path, face_index, image)\n",
    "    \n",
    "    RGB_size = SV_HSV_size = SL_HSL_size = LAB_size = YCRCB_size = 26\n",
    "    \n",
    "    for i in range(0, RGB_size):\n",
    "        row['R_BIN_' + str(i)] = r_hist[i]\n",
    "        \n",
    "    for i in range(0, RGB_size):\n",
    "        row['G_BIN_' + str(i)] = g_hist[i]\n",
    "        \n",
    "    for i in range(0, RGB_size):\n",
    "        row['B_BIN_' + str(i)] = b_hist[i]\n",
    "        \n",
    "    for i in range(0, h_hist_HSV.size):\n",
    "        row['H_HSV_BIN' + str(i)] = h_hist_HSV[i]\n",
    "        \n",
    "    for i in range(0, SV_HSV_size):\n",
    "        row['S_HSV_BIN' + str(i)] = s_hist_HSV[i]\n",
    "        \n",
    "    for i in range(0, SV_HSV_size):\n",
    "        row['V_HSV_BIN' + str(i)] = v_hist_HSV[i]\n",
    "        \n",
    "    for i in range(0, h_hist_HSL.size):\n",
    "        row['H_HSL_BIN' + str(i)] = h_hist_HSL[i]\n",
    "        \n",
    "    for i in range(0, SL_HSL_size):\n",
    "        row['S_HSL_BIN' + str(i)] = s_hist_HSL[i]\n",
    "        \n",
    "    for i in range(0, SL_HSL_size):\n",
    "        row['L_HSL_BIN' + str(i)] = l_hist_HSL[i]\n",
    "        \n",
    "    for i in range(0, LAB_size):\n",
    "        row['L_LAB_BIN' + str(i)] = l_hist_LAB[i]\n",
    "    \n",
    "    for i in range(0, LAB_size):\n",
    "        row['A_LAB_BIN' + str(i)] = a_hist_LAB[i]\n",
    "        \n",
    "    for i in range(0, LAB_size):\n",
    "        row['B_LAB_BIN' + str(i)] = b_hist_LAB[i]\n",
    "        \n",
    "    for i in range(0, YCRCB_size):\n",
    "        row['Y_BIN' + str(i)] = y_hist[i]\n",
    "        \n",
    "    for i in range(0, YCRCB_size):\n",
    "        row['CR_BIN' + str(i)] = cr_hist[i]\n",
    "        \n",
    "    for i in range(0, YCRCB_size):\n",
    "        row['CB_BIN' + str(i)] = cb_hist[i]\n",
    "        \n",
    "    for i in range(0, lbp_hist.size):\n",
    "        row[\"LBP_BIN\" + str(i)] = lbp_hist[i]\n",
    "        \n",
    "    for i in range(0, sobelx_hist.size):\n",
    "        row[\"SOBELX_BIN\" + str(i)] = sobelx_hist[i]\n",
    "        \n",
    "    for i in range(0, sobely_hist.size):\n",
    "        row[\"SOBELY_BIN\" + str(i)] = sobely_hist[i]\n",
    "        \n",
    "    for i in range(0, sobel_hist.size):\n",
    "        row[\"SOBEL_BIN\" + str(i)] = sobel_hist[i]\n",
    "    \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c215245",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detach_cpu(image):\n",
    "    return image.detach().cpu()\n",
    "\n",
    "# convert 1x3x416x416 to 416x416x3\n",
    "def reshape_image(image):\n",
    "    return np.transpose(np.squeeze(image), (1 ,2, 0))\n",
    "\n",
    "# convert 1x3x416x416 tensor to 416x416x3 numpy image\n",
    "def tensor_to_image(image):\n",
    "    return np.transpose(image.detach().cpu().squeeze().numpy(), (1, 2, 0))\n",
    "\n",
    "def save_tensor_as_image(image, path):\n",
    "    save_img = cv2.cvtColor(np.moveaxis((image.detach().numpy() * 255).squeeze(), 0, -1).astype('uint8'), cv2.COLOR_RGB2BGR)\n",
    "    cv2.imwrite(path, save_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b600713",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import minepsilon as minE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833d8d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mask(filename, face_num, target_bbox):\n",
    "    faces_df = pd.read_csv(CSV_FILE)\n",
    "    filename = \"restored_mask_\" + os.path.splitext(filename)[0] + \"_\" + str(face_num) + \"_image_final.png\"\n",
    "    mask = cv2.imread(os.path.join(os.getcwd(), RESTORED_MASK_PATH, filename), 0)\n",
    "    \n",
    "    face_row = faces_df.loc[faces_df['filename'] == filename]\n",
    "    padded_dim = (int(face_row[\"x2_pad\"] - face_row[\"x1_pad\"]), int(face_row[\"y2_pad\"] - face_row[\"y1_pad\"]))\n",
    "    target_dim = (int(target_bbox[2] - target_bbox[0]), int(target_bbox[3] - target_bbox[1]))\n",
    "    \n",
    "    if dict(zip(*np.unique(mask, return_counts = True)))[255] < int(target_dim[0] * target_dim[1] * 0.5):\n",
    "        return torch.ones((1, 3, target_dim[1], target_dim[0])), False\n",
    "    \n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (int(mask.shape[0] * 0.5), int(mask.shape[1] * 0.5)))\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "    mask = cv2.cvtColor(mask, cv2.COLOR_GRAY2RGB)\n",
    "    mask = transforms.Compose([DEFAULT_TRANSFORMS])((mask, np.zeros((1, 5))))[0].unsqueeze(0)\n",
    "    \n",
    "    #print(os.path.join(os.getcwd(), RESTORED_MASK_PATH, filename))\n",
    "    #print(\"mask size before:\", mask.shape)\n",
    "    \n",
    "    \n",
    "    #print(\"x1, y1, orig shape\")\n",
    "    #print(int(face_row[\"x1_pad\"]), int(face_row[\"y1_pad\"]), int(face_row[\"x2_pad\"]), int(face_row[\"y2_pad\"]))\n",
    "    #print(original_shape)\n",
    "    \n",
    "    #print(\"target shape:\", original_shape)\n",
    "    #print(\"yoloshape:\", int(face_row[\"x2\"] - face_row[\"x1\"]), int(face_row[\"y2\"] - face_row[\"y1\"]))\n",
    "    \n",
    "    current_dim = max(mask.shape)\n",
    "    diff_x, diff_y = abs(padded_dim[0] - current_dim) / 2, abs(padded_dim[1] - current_dim) / 2\n",
    "    #print(\"first diff:\", diff_x, diff_y)\n",
    "    \n",
    "    if diff_y != 0:\n",
    "        mask = mask[..., int(np.floor(diff_y)):-int(np.ceil(diff_y)), :]\n",
    "    if diff_x != 0:\n",
    "        mask = mask[..., int(np.floor(diff_x)):-int(np.ceil(diff_x))]\n",
    "        \n",
    "    #print(mask.shape == padded_dim, mask.shape, padded_dim, target_dim)\n",
    "    \n",
    "    padding = [\n",
    "        int(abs(face_row[\"x1\"] - face_row[\"x1_pad\"])),\n",
    "        int(abs(face_row[\"y1\"] - face_row[\"y1_pad\"])),\n",
    "        int(abs(face_row[\"x2\"] - face_row[\"x2_pad\"])),\n",
    "        int(abs(face_row[\"y2\"] - face_row[\"y2_pad\"]))\n",
    "    ]\n",
    "    \n",
    "    #print(\"padding:\", padding)\n",
    "    \n",
    "    new_dim = padded_dim[0] - padding[0] - padding[2], padded_dim[1] - padding[1] - padding[3]\n",
    "    diff_x, diff_y = (target_dim[0] - new_dim[0]) / 2, (target_dim[1] - new_dim[1]) / 2\n",
    "    #print(\"second diff:\", diff_x, diff_y)\n",
    "    \n",
    "    padding[0] -= int(np.floor(diff_x))\n",
    "    padding[1] -= int(np.floor(diff_y))\n",
    "    padding[2] -= int(np.ceil(diff_x))\n",
    "    padding[3] -= int(np.ceil(diff_y))\n",
    "    \n",
    "    #print(\"mask size after:\", mask.shape)\n",
    "    #print(\"unpadded bbox:\", (face_row[\"x1\"], face_row[\"y1\"], face_row[\"x2\"], face_row[\"y2\"]))\n",
    "    #print(\"adjusted padding:\", padding)\n",
    "    mask = mask[..., padding[1]:-padding[3], padding[0]:-padding[2]]\n",
    "    \n",
    "    return mask, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7c3f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(model, device):\n",
    "    \n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    \n",
    "    df = pd.DataFrame() # dataframe storing the dataset\n",
    "    row = {} #the information/columns for a single row in the dataset is stored here\n",
    "    \n",
    "    # Loop over all examples in test set\n",
    "    for path in glob.glob(os.path.join(INPUT_PATH, '*.jpg')):\n",
    "        row['path'] = path\n",
    "        file_basename = os.path.basename(path)\n",
    "        print(file_basename)\n",
    "        \n",
    "        model.eval()\n",
    "            \n",
    "        model.gradient_mode = False\n",
    "        for yolo_layer in model.yolo_layers:\n",
    "            yolo_layer.gradient_mode = False\n",
    "        \n",
    "        # read and transform the image from the path\n",
    "        data = cv2.imread(path)  # read the image\n",
    "        data = cv2.cvtColor(data, cv2.COLOR_BGR2RGB) #change to rgb\n",
    "        data = transforms.Compose([DEFAULT_TRANSFORMS,Resize(416)])((data, np.zeros((1, 5))))[0].unsqueeze(0) # transform the image\n",
    "        \n",
    "        # Forward pass the data through the model and call non max suppression\n",
    "        nms, nms_output = non_max_suppression(model(data), 0.5, 0.5) #conf_thres and iou_thres = 0.5\n",
    "        \n",
    "        face_list = []\n",
    "        if type(nms_output[0]) is not int:\n",
    "            face_list = nms_output[0]\n",
    "        \n",
    "        data = data.to(device)\n",
    "        # Set requires_grad attribute of tensor. Important for Attack\n",
    "        data.requires_grad = True\n",
    "        \n",
    "        model.gradient_mode = True\n",
    "        for yolo_layer in model.yolo_layers:\n",
    "            yolo_layer.gradient_mode = True\n",
    "        \n",
    "        output = model(data)\n",
    "        \n",
    "        # loop through each of the faces in the image\n",
    "        for face_index, face_row in enumerate(face_list): #nms_output[0] because the model is designed to take in several images at a time from the dataloader but we are only loading the image one at a time\n",
    "            row['face_index'] = face_index\n",
    "            print(\"Face\", face_index)\n",
    "            \n",
    "            x, y, w, h = face_row[0], face_row[1], face_row[2], face_row[3]\n",
    "            \n",
    "            factor_x, factor_y, factor_w, factor_h = random.uniform(1, 2), random.uniform(1, 2), random.uniform(1, 2), random.uniform(1, 2)\n",
    "            normal_x, normal_y, normal_w, normal_h = x / 416, y / 416, w / 416, h / 416\n",
    "            \n",
    "            new_x = normal_x * factor_x if random.choice([True, False]) else normal_x / factor_x\n",
    "            new_y = normal_y * factor_y if random.choice([True, False]) else normal_y / factor_y\n",
    "            new_w = normal_w * factor_w if random.choice([True, False]) else normal_w / factor_w\n",
    "            new_h = normal_h * factor_h if random.choice([True, False]) else normal_h / factor_h\n",
    "            \n",
    "            new_x, new_y, new_w, new_h = max(min(1, new_x), 0), max(min(1, new_y), 0), max(min(1, new_w), 0), max(min(1, new_h), 0)\n",
    "            \n",
    "            target = torch.tensor([[0.0, 0, new_x, new_y, new_w, new_h]])\n",
    "            target = target.to(device)\n",
    "            \n",
    "            loss, loss_components = compute_loss(output, target, model)\n",
    "            \n",
    "            # cropped image with bounding box\n",
    "            # getting (x1, y1) upper left, (x2, y2) lower right\n",
    "            x1 = max(int(np.floor((x - w / 2).detach().cpu().numpy())), 0)\n",
    "            y1 = max(int(np.floor((y - h / 2).detach().cpu().numpy())), 0)\n",
    "            x2 = min(int(np.ceil((x + w / 2).detach().cpu().numpy())), 415)\n",
    "            y2 = min(int(np.ceil((y + h / 2).detach().cpu().numpy())), 415)\n",
    "            \n",
    "            row['x1'], row['y1'], row['x2'], row['y2'] = x1, y1, x2, y2\n",
    "            \n",
    "            cropped_image = detach_cpu(data)[:, :, y1:y2, x1:x2] #get the first dimension, the channels, and crop it\n",
    "            cropped_image = tensor_to_image(cropped_image) #reshape the image to (w/h, h/w, channel)\n",
    "            \n",
    "            # Zero all existing gradients\n",
    "            model.zero_grad()\n",
    "            data.grad = None\n",
    "\n",
    "            # Calculate gradients of model in backward pass\n",
    "            loss.backward(retain_graph=True) #TODO: Amos - check if this is correct\n",
    "            \n",
    "            # Collect datagrad\n",
    "            data_grad = data.grad.data\n",
    "            #print('Gradient')\n",
    "            #print(data_grad)\n",
    "            #print(data_grad.shape)\n",
    "            #plt.imshow(np.transpose(np.clip(data_grad.squeeze(0).numpy(), 0, 1), (1, 2, 0)))\n",
    "            #plt.show()\n",
    "            \n",
    "            bbox = (x1, y1, x2, y2)\n",
    "            mask, used_mask = load_mask(os.path.basename(path), face_index, bbox)\n",
    "            row['used_mask'] = used_mask\n",
    "            inverted_mask = 1 - mask\n",
    "            \n",
    "            #TODO: Jay - extract image attributes here\n",
    "            # extract the image attributes from  the 'cropped_image' variable\n",
    "            # save the attributes as row['<column name in the dataset>'] = <data> (see examples above for reference)\n",
    "            \n",
    "            row = extract_image_attributes(row, path, face_index, cropped_image * tensor_to_image(mask[0]))\n",
    "            \n",
    "            #print(\"bbox dim:\", bbox)\n",
    "            #print(mask.shape, data[:, :, y1:y2, x1:x2].shape)\n",
    "            # TODO - Amos - determine the value of epsilon by calling fgsm_attack and changing the value of epsilon (see code below)\n",
    "            # the value of data(image) and data_grad remains constant diba\n",
    "            \n",
    "            #print(\"Calculating min epsilon for YuNet...\")\n",
    "            yn_min_e_face = minE.min_model_eps(data.clone().detach(), data_grad.clone().detach(), minE.yn_det_fn, mask, bbox)\n",
    "            #print(\"Calculating min epsilon for MediaPipe...\")\n",
    "            mp_min_e_face = minE.min_model_eps(data.clone().detach(), data_grad.clone().detach(), minE.mp_det_fn, mask, bbox)\n",
    "            #print(\"Calculating min epsilon for YoloFace...\")\n",
    "            yf_min_e_face = minE.min_model_eps(data.clone().detach(), data_grad.clone().detach(), minE.yf_det_fn, mask, bbox)\n",
    "            print(\"yunet min face:\", yn_min_e_face, \"mediapipe min face:\", mp_min_e_face, \"yoloface min face:\", yf_min_e_face)\n",
    "            \n",
    "            row['e_face_yn'], row['e_face_mp'], row['e_face_yf'] = yn_min_e_face, mp_min_e_face, yf_min_e_face\n",
    "            \n",
    "            if used_mask:\n",
    "                yn_min_e_bg = minE.min_model_eps(data.clone().detach(), data_grad.clone().detach(), minE.yn_det_fn, inverted_mask, bbox)\n",
    "                mp_min_e_bg = minE.min_model_eps(data.clone().detach(), data_grad.clone().detach(), minE.mp_det_fn, inverted_mask, bbox)\n",
    "                yf_min_e_bg = minE.min_model_eps(data.clone().detach(), data_grad.clone().detach(), minE.yf_det_fn, inverted_mask, bbox)\n",
    "            else:\n",
    "                yn_min_e_bg = yn_min_e_face\n",
    "                mp_min_e_bg = mp_min_e_face\n",
    "                yf_min_e_bg = yf_min_e_face\n",
    "                \n",
    "            print(\"yunet min bg:\", yn_min_e_bg, \"mediapipe min bg:\", mp_min_e_bg, \"yoloface min bg:\", yf_min_e_bg)\n",
    "            \n",
    "            row['e_bg_yn'], row['e_bg_mp'], row['e_bg_yf'] = yn_min_e_bg, mp_min_e_bg, yf_min_e_bg\n",
    "            \n",
    "            cur_filename = os.path.join(PERTURBED_OUTS, str(face_index) + \"yn_min_e_face_\" + file_basename)\n",
    "            save_tensor_as_image(minE.fgsm_attack(data.clone().detach(), yn_min_e_face, data_grad.clone().detach(), mask, *bbox), cur_filename)\n",
    "            cur_filename = os.path.join(PERTURBED_OUTS, str(face_index) + \"mp_min_e_face_\" + file_basename)\n",
    "            save_tensor_as_image(minE.fgsm_attack(data.clone().detach(), mp_min_e_face, data_grad.clone().detach(), mask, *bbox), cur_filename)\n",
    "            cur_filename = os.path.join(PERTURBED_OUTS, str(face_index) + \"yf_min_e_face_\" + file_basename)\n",
    "            save_tensor_as_image(minE.fgsm_attack(data.clone().detach(), yf_min_e_face, data_grad.clone().detach(), mask, *bbox), cur_filename)\n",
    "            cur_filename = os.path.join(PERTURBED_OUTS, str(face_index) + \"yn_min_e_bg_\" + file_basename)\n",
    "            save_tensor_as_image(minE.fgsm_attack(data.clone().detach(), yn_min_e_bg, data_grad.clone().detach(), mask, *bbox), cur_filename)\n",
    "            cur_filename = os.path.join(PERTURBED_OUTS, str(face_index) + \"mp_min_e_bg_\" + file_basename)\n",
    "            save_tensor_as_image(minE.fgsm_attack(data.clone().detach(), mp_min_e_bg, data_grad.clone().detach(), mask, *bbox), cur_filename)\n",
    "            cur_filename = os.path.join(PERTURBED_OUTS, str(face_index) + \"yf_min_e_bg_\" + file_basename)\n",
    "            save_tensor_as_image(minE.fgsm_attack(data.clone().detach(), yf_min_e_bg, data_grad.clone().detach(), mask, *bbox), cur_filename)\n",
    "            \n",
    "            # Call FGSM Attack\n",
    "            \n",
    "            #perturbed_data = minE.fgsm_attack(data.clone().detach(), yf_min_e, data_grad.clone().detach(), mask, *bbox)\n",
    "            #perturbed_data = fgsm_attack(data, max(yn_min_e, mp_min_e), data_grad) #data is the input image, epsilon\n",
    "            #print(\"can detect faces on unperturbed img?\", minE.mp_det_fn(data.detach()))\n",
    "            #print(f\"can detect faces on perturbed data with e={max(yn_min_e, mp_min_e) - 0.01}?\", minE.mp_det_fn(fgsm_attack(data, max(yn_min_e, mp_min_e) - 0.01, data_grad).detach()))\n",
    "            #print(f\"can detect faces on perturbed img? with e={max(yn_min_e, mp_min_e) - 0.01}\", minE.mp_det_fn(perturbed_data.detach()))\n",
    "            \n",
    "            df = df.append(row, ignore_index=True) #append the attributes of one face to the dataframe\n",
    "            \n",
    "#             plt.imshow(tensor_to_image(perturbed_data))\n",
    "            plt.show()\n",
    "            \n",
    "    df.to_csv(os.path.join(CSV_PATH, FOLDER_NAME + '_features_dataset' + str(int(time.time())) + '.csv'), index=False)  #save to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3efbbeb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "folders = [\"test\"]\n",
    "OUTPUT_FOLDER = os.path.join(os.getcwd(), \"WIDER\")\n",
    "FOLDER_NAME = \"\"\n",
    "\n",
    "for FOLDER_NAME in folders:\n",
    "    INPUT_PATH = os.path.join(os.getcwd(), 'images', FOLDER_NAME)\n",
    "    FOLDER_PATH = os.path.join(OUTPUT_FOLDER, FOLDER_NAME)\n",
    "    CSV_PATH = os.path.join(FOLDER_PATH, FOLDER_NAME + '_CSV')\n",
    "    RESTORED_MASK_PATH = os.path.join(FOLDER_PATH, FOLDER_NAME + '_restored_mask')\n",
    "    PERTURBED_OUTS = os.path.join(FOLDER_PATH, FOLDER_NAME + \"_perturbed_outs\")\n",
    "    \n",
    "    if not os.path.exists(PERTURBED_OUTS):\n",
    "        os.mkdir(PERTURBED_OUTS)\n",
    "    \n",
    "    CSV_FILE = \"\"\n",
    "    for file in os.listdir(CSV_PATH):\n",
    "        if \"dataset_pixels\" in file and file.endswith(\".csv\"):\n",
    "            CSV_FILE = os.path.join(os.getcwd(), CSV_PATH, file)\n",
    "    #print(CSV_FILE)\n",
    "    #raise ex\n",
    "    print(\"Working on\", FOLDER_NAME, \"folder\")\n",
    "    pipeline(model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8e404e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchview import draw_graph\n",
    "# path = 'E:\\\\Documents\\\\GitHub\\\\THS-ST1\\\\images\\\\Test\\\\girl.jpg'\n",
    "\n",
    "device, model = load_model('./weights/yolo_face_sthanhng.cfg', \"./weights/yolo_face_sthanhng.weights\")\n",
    "\n",
    "# model.eval()\n",
    "# model.gradient_mode = True\n",
    "# for yolo_layer in model.yolo_layers:\n",
    "#     yolo_layer.gradient_mode = True\n",
    "\n",
    "# # model_graph = draw_graph(model, input_size=(1, 3, 416, 416), device='meta', save_graph=True, mode='train', filename='train_graph')\n",
    "# # model_graph.visual_graph\n",
    "# data = cv2.imread(path)  # read the image\n",
    "# # print(data)\n",
    "# data = cv2.cvtColor(data, cv2.COLOR_BGR2RGB) #change to rgb\n",
    "# data = transforms.Compose([DEFAULT_TRANSFORMS,Resize(416)])((data, np.zeros((1, 5))))[0].unsqueeze(0) # transform the image\n",
    "# output1 = model(data)\n",
    "# output2 = model(data)\n",
    "# target = torch.tensor([[0.0, 0.0, 0.5, 0.5, 0.5, 0.5]])\n",
    "# target = target.to(device)\n",
    "# print(target.shape)\n",
    "# output = model(data)\n",
    "# for x in output:\n",
    "#     print(x.shape)\n",
    "\n",
    "# model.gradient_mode = False\n",
    "# for yolo_layer in model.yolo_layers:\n",
    "#     yolo_layer.gradient_mode = False\n",
    "\n",
    "# output = model(data)\n",
    "# print(output.shape)\n",
    "# # compute_loss(output, target, model)\n",
    "# # print(output.shape)\n",
    "\n",
    "# # for i, yolo_layer in enumerate(model.yolo_layers):\n",
    "# #     yolo_layer.gradient_mode = True\n",
    "# #     print(yolo_layer.gradient_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3896c248",
   "metadata": {},
   "source": [
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
