{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "601fd57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "import gdown\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import cv2\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.feature as feature\n",
    "import xlwings as xw\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import random\n",
    "\n",
    "#libraries for yolo\n",
    "from pytorchyolo.models import load_model\n",
    "from pytorchyolo.utils.transforms import Resize, DEFAULT_TRANSFORMS\n",
    "from pytorchyolo.utils.utils import non_max_suppression\n",
    "from pytorchyolo.utils.loss import compute_loss\n",
    "\n",
    "from matplotlib.ticker import (FormatStrFormatter, AutoMinorLocator, FuncFormatter, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b306b61c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Darknet(\n",
       "  (module_list): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (conv_0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm_0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_0): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (conv_1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (batch_norm_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_1): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (conv_2): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm_2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_2): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (conv_3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm_3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_3): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (shortcut_4): Sequential()\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (conv_5): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (batch_norm_5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_5): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (conv_6): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm_6): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_6): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (conv_7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm_7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_7): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (8): Sequential(\n",
       "      (shortcut_8): Sequential()\n",
       "    )\n",
       "    (9): Sequential(\n",
       "      (conv_9): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm_9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_9): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (10): Sequential(\n",
       "      (conv_10): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm_10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_10): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (11): Sequential(\n",
       "      (shortcut_11): Sequential()\n",
       "    )\n",
       "    (12): Sequential(\n",
       "      (conv_12): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (batch_norm_12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_12): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (13): Sequential(\n",
       "      (conv_13): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm_13): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_13): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (14): Sequential(\n",
       "      (conv_14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm_14): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_14): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (15): Sequential(\n",
       "      (shortcut_15): Sequential()\n",
       "    )\n",
       "    (16): Sequential(\n",
       "      (conv_16): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm_16): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_16): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (17): Sequential(\n",
       "      (conv_17): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm_17): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_17): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (18): Sequential(\n",
       "      (shortcut_18): Sequential()\n",
       "    )\n",
       "    (19): Sequential(\n",
       "      (conv_19): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm_19): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_19): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (20): Sequential(\n",
       "      (conv_20): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm_20): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_20): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (21): Sequential(\n",
       "      (shortcut_21): Sequential()\n",
       "    )\n",
       "    (22): Sequential(\n",
       "      (conv_22): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm_22): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_22): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (23): Sequential(\n",
       "      (conv_23): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm_23): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_23): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (24): Sequential(\n",
       "      (shortcut_24): Sequential()\n",
       "    )\n",
       "    (25): Sequential(\n",
       "      (conv_25): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm_25): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_25): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (26): Sequential(\n",
       "      (conv_26): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm_26): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_26): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (27): Sequential(\n",
       "      (shortcut_27): Sequential()\n",
       "    )\n",
       "    (28): Sequential(\n",
       "      (conv_28): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm_28): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_28): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (29): Sequential(\n",
       "      (conv_29): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm_29): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_29): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (30): Sequential(\n",
       "      (shortcut_30): Sequential()\n",
       "    )\n",
       "    (31): Sequential(\n",
       "      (conv_31): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm_31): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_31): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (32): Sequential(\n",
       "      (conv_32): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm_32): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_32): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (33): Sequential(\n",
       "      (shortcut_33): Sequential()\n",
       "    )\n",
       "    (34): Sequential(\n",
       "      (conv_34): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm_34): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_34): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (35): Sequential(\n",
       "      (conv_35): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm_35): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_35): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (36): Sequential(\n",
       "      (shortcut_36): Sequential()\n",
       "    )\n",
       "    (37): Sequential(\n",
       "      (conv_37): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (batch_norm_37): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_37): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (38): Sequential(\n",
       "      (conv_38): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm_38): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_38): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (39): Sequential(\n",
       "      (conv_39): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm_39): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_39): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (40): Sequential(\n",
       "      (shortcut_40): Sequential()\n",
       "    )\n",
       "    (41): Sequential(\n",
       "      (conv_41): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm_41): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_41): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (42): Sequential(\n",
       "      (conv_42): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm_42): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_42): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (43): Sequential(\n",
       "      (shortcut_43): Sequential()\n",
       "    )\n",
       "    (44): Sequential(\n",
       "      (conv_44): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm_44): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_44): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (45): Sequential(\n",
       "      (conv_45): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm_45): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_45): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (46): Sequential(\n",
       "      (shortcut_46): Sequential()\n",
       "    )\n",
       "    (47): Sequential(\n",
       "      (conv_47): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm_47): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_47): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (48): Sequential(\n",
       "      (conv_48): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm_48): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_48): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (49): Sequential(\n",
       "      (shortcut_49): Sequential()\n",
       "    )\n",
       "    (50): Sequential(\n",
       "      (conv_50): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm_50): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_50): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (51): Sequential(\n",
       "      (conv_51): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm_51): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_51): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (52): Sequential(\n",
       "      (shortcut_52): Sequential()\n",
       "    )\n",
       "    (53): Sequential(\n",
       "      (conv_53): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm_53): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_53): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (54): Sequential(\n",
       "      (conv_54): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm_54): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_54): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (55): Sequential(\n",
       "      (shortcut_55): Sequential()\n",
       "    )\n",
       "    (56): Sequential(\n",
       "      (conv_56): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm_56): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_56): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (57): Sequential(\n",
       "      (conv_57): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm_57): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_57): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (58): Sequential(\n",
       "      (shortcut_58): Sequential()\n",
       "    )\n",
       "    (59): Sequential(\n",
       "      (conv_59): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm_59): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_59): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (60): Sequential(\n",
       "      (conv_60): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm_60): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_60): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (61): Sequential(\n",
       "      (shortcut_61): Sequential()\n",
       "    )\n",
       "    (62): Sequential(\n",
       "      (conv_62): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (batch_norm_62): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_62): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (63): Sequential(\n",
       "      (conv_63): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm_63): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_63): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (64): Sequential(\n",
       "      (conv_64): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm_64): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_64): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (65): Sequential(\n",
       "      (shortcut_65): Sequential()\n",
       "    )\n",
       "    (66): Sequential(\n",
       "      (conv_66): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm_66): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_66): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (67): Sequential(\n",
       "      (conv_67): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm_67): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_67): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (68): Sequential(\n",
       "      (shortcut_68): Sequential()\n",
       "    )\n",
       "    (69): Sequential(\n",
       "      (conv_69): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm_69): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_69): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (70): Sequential(\n",
       "      (conv_70): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm_70): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_70): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (71): Sequential(\n",
       "      (shortcut_71): Sequential()\n",
       "    )\n",
       "    (72): Sequential(\n",
       "      (conv_72): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm_72): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_72): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (73): Sequential(\n",
       "      (conv_73): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm_73): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_73): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (74): Sequential(\n",
       "      (shortcut_74): Sequential()\n",
       "    )\n",
       "    (75): Sequential(\n",
       "      (conv_75): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm_75): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_75): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (76): Sequential(\n",
       "      (conv_76): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm_76): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_76): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (77): Sequential(\n",
       "      (conv_77): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm_77): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_77): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (78): Sequential(\n",
       "      (conv_78): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm_78): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_78): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (79): Sequential(\n",
       "      (conv_79): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm_79): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_79): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (80): Sequential(\n",
       "      (conv_80): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm_80): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_80): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (81): Sequential(\n",
       "      (conv_81): Conv2d(1024, 18, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (82): Sequential(\n",
       "      (yolo_82): YOLOLayer(\n",
       "        (mse_loss): MSELoss()\n",
       "        (bce_loss): BCELoss()\n",
       "      )\n",
       "    )\n",
       "    (83): Sequential(\n",
       "      (route_83): Sequential()\n",
       "    )\n",
       "    (84): Sequential(\n",
       "      (conv_84): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm_84): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_84): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (85): Sequential(\n",
       "      (upsample_85): Upsample()\n",
       "    )\n",
       "    (86): Sequential(\n",
       "      (route_86): Sequential()\n",
       "    )\n",
       "    (87): Sequential(\n",
       "      (conv_87): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm_87): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_87): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (88): Sequential(\n",
       "      (conv_88): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm_88): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_88): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (89): Sequential(\n",
       "      (conv_89): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm_89): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_89): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (90): Sequential(\n",
       "      (conv_90): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm_90): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_90): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (91): Sequential(\n",
       "      (conv_91): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm_91): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_91): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (92): Sequential(\n",
       "      (conv_92): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm_92): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_92): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (93): Sequential(\n",
       "      (conv_93): Conv2d(512, 18, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (94): Sequential(\n",
       "      (yolo_94): YOLOLayer(\n",
       "        (mse_loss): MSELoss()\n",
       "        (bce_loss): BCELoss()\n",
       "      )\n",
       "    )\n",
       "    (95): Sequential(\n",
       "      (route_95): Sequential()\n",
       "    )\n",
       "    (96): Sequential(\n",
       "      (conv_96): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm_96): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_96): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (97): Sequential(\n",
       "      (upsample_97): Upsample()\n",
       "    )\n",
       "    (98): Sequential(\n",
       "      (route_98): Sequential()\n",
       "    )\n",
       "    (99): Sequential(\n",
       "      (conv_99): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm_99): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_99): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (100): Sequential(\n",
       "      (conv_100): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm_100): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_100): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (101): Sequential(\n",
       "      (conv_101): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm_101): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_101): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (102): Sequential(\n",
       "      (conv_102): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm_102): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_102): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (103): Sequential(\n",
       "      (conv_103): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm_103): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_103): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (104): Sequential(\n",
       "      (conv_104): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm_104): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (leaky_104): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (105): Sequential(\n",
       "      (conv_105): Conv2d(256, 18, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (106): Sequential(\n",
       "      (yolo_106): YOLOLayer(\n",
       "        (mse_loss): MSELoss()\n",
       "        (bce_loss): BCELoss()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device, model = load_model('./weights/yolo_face_sthanhng.cfg', \"./weights/yolo_face_sthanhng.weights\")\n",
    "\n",
    "# Set the model in evaluation mode. In this case this is for the Dropout layers\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65f89937",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalBinaryPatterns:\n",
    "  def __init__(self, numPoints, radius):\n",
    "    self.numPoints = numPoints\n",
    "    self.radius = radius\n",
    "\n",
    "  def describe(self, image, eps = 1e-7):\n",
    "    lbp = feature.local_binary_pattern(image, self.numPoints, self.radius, method=\"uniform\")\n",
    "    (hist, _) = np.histogram(lbp.ravel(), bins=np.arange(0, self.numPoints+3), range=(0, self.numPoints + 2))\n",
    "\n",
    "    # Normalize the histogram\n",
    "    hist = hist.astype('float')\n",
    "    hist /= (hist.sum() + eps)\n",
    "\n",
    "    return hist, lbp\n",
    "\n",
    "# From https://medium.com/mlearning-ai/how-to-plot-color-channels-histogram-of-an-image-in-python-using-opencv-40022032e127\n",
    "# Extracts image's color channel\n",
    "def extract_color_channel(path, face_index, image, version):\n",
    "    # BGR Image Color Conversion\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    hls = cv2.cvtColor(image, cv2.COLOR_BGR2HLS)\n",
    "    lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
    "    ycrcb = cv2.cvtColor(image, cv2.COLOR_BGR2YCrCb)\n",
    "    \n",
    "    r, g, b = cv2.split(rgb)\n",
    "    \n",
    "    r *= 255\n",
    "    g *= 255\n",
    "    b *= 255\n",
    "    \n",
    "    r_hist = cv2.calcHist(r, [0], None, [26], [0, 256])\n",
    "    r_hist = r_hist.ravel()\n",
    "    r_hist = r_hist.astype('float')\n",
    "    r_hist /= r_hist.sum()\n",
    "    \n",
    "    g_hist = cv2.calcHist([g], [0], None, [26], [0, 256])\n",
    "    g_hist = g_hist.ravel()\n",
    "    g_hist = g_hist.astype('float')\n",
    "    g_hist /= g_hist.sum()\n",
    "    \n",
    "    b_hist = cv2.calcHist([b], [0], None, [26], [0, 256])\n",
    "    b_hist = b_hist.ravel()\n",
    "    b_hist = b_hist.astype('float')\n",
    "    b_hist /= b_hist.sum()\n",
    "    \n",
    "    h, s, v = cv2.split(hsv)\n",
    "    \n",
    "    s *= 255\n",
    "    v *= 255\n",
    "    \n",
    "    h_hist_HSV = cv2.calcHist([h], [0], None, [36], [0, 361])\n",
    "    h_hist_HSV = h_hist_HSV.ravel()\n",
    "    h_hist_HSV = h_hist_HSV.astype('float')\n",
    "    h_hist_HSV /= h_hist_HSV.sum()\n",
    "    \n",
    "    s_hist_HSV = cv2.calcHist([s], [0], None, [26], [0, 256])\n",
    "    s_hist_HSV = s_hist_HSV.ravel()\n",
    "    s_hist_HSV = s_hist_HSV.astype('float')\n",
    "    s_hist_HSV /= s_hist_HSV.sum()\n",
    "    \n",
    "    v_hist_HSV = cv2.calcHist([v], [0], None, [26], [0, 256])\n",
    "    v_hist_HSV = v_hist_HSV.ravel()\n",
    "    v_hist_HSV = v_hist_HSV.astype('float')\n",
    "    v_hist_HSV /= v_hist_HSV.sum()\n",
    "    \n",
    "    h, l, s = cv2.split(hls)\n",
    "    \n",
    "    l *= 255\n",
    "    s *= 255\n",
    "    \n",
    "    h_hist_HSL = cv2.calcHist([h], [0], None, [36], [0, 361])\n",
    "    h_hist_HSL = h_hist_HSL.ravel()\n",
    "    h_hist_HSL = h_hist_HSL.astype('float')\n",
    "    h_hist_HSL /= h_hist_HSL.sum()\n",
    "    \n",
    "    l_hist_HSL = cv2.calcHist([l], [0], None, [26], [0, 256])\n",
    "    l_hist_HSL = l_hist_HSL.ravel()\n",
    "    l_hist_HSL = l_hist_HSL.astype('float')\n",
    "    l_hist_HSL /= l_hist_HSL.sum()\n",
    "    \n",
    "    s_hist_HSL = cv2.calcHist([s], [0], None, [26], [0, 256])\n",
    "    s_hist_HSL = s_hist_HSL.ravel()\n",
    "    s_hist_HSL = s_hist_HSL.astype('float')\n",
    "    s_hist_HSL /= s_hist_HSL.sum()\n",
    "    \n",
    "    l, a, b = cv2.split(lab)\n",
    "    \n",
    "    l_hist_LAB = cv2.calcHist([l], [0], None, [26], [0, 256])\n",
    "    l_hist_LAB = l_hist_LAB.ravel()\n",
    "    l_hist_LAB = l_hist_LAB.astype('float')\n",
    "    l_hist_LAB /= l_hist_LAB.sum()\n",
    "    \n",
    "    a_hist_LAB = cv2.calcHist([a], [0], None, [26], [0, 256])\n",
    "    a_hist_LAB = a_hist_LAB.ravel()\n",
    "    a_hist_LAB = a_hist_LAB.astype('float')\n",
    "    a_hist_LAB /= a_hist_LAB.sum()\n",
    "    \n",
    "    b_hist_LAB = cv2.calcHist([b], [0], None, [26], [0, 256])\n",
    "    b_hist_LAB = b_hist_LAB.ravel()\n",
    "    b_hist_LAB = b_hist_LAB.astype('float')\n",
    "    b_hist_LAB /= b_hist_LAB.sum()\n",
    "    \n",
    "    \n",
    "    y, cr, cb = cv2.split(ycrcb)\n",
    "    \n",
    "    y *= 255\n",
    "    cr *= 255\n",
    "    cb *= 255\n",
    "    \n",
    "    y_hist = cv2.calcHist([y], [0], None, [26], [0, 256])\n",
    "    y_hist = y_hist.ravel()\n",
    "    y_hist = y_hist.astype('float')\n",
    "    y_hist /= y_hist.sum()\n",
    "    \n",
    "    cr_hist = cv2.calcHist([cr], [0], None, [26], [0, 256])\n",
    "    cr_hist = cr_hist.ravel()\n",
    "    cr_hist = cr_hist.astype('float')\n",
    "    cr_hist /= cr_hist.sum()\n",
    "    \n",
    "    cb_hist = cv2.calcHist([cb], [0], None, [26], [0, 256])\n",
    "    cb_hist = cb_hist.ravel()\n",
    "    cb_hist = cb_hist.astype('float')\n",
    "    cb_hist /= cb_hist.sum()\n",
    "    \n",
    "    face_index = str(face_index)\n",
    "    \n",
    "    return r_hist, g_hist, b_hist, h_hist_HSV, s_hist_HSV, v_hist_HSV, h_hist_HSL, s_hist_HSL, l_hist_HSL, l_hist_LAB, a_hist_LAB, b_hist_LAB, y_hist, cr_hist, cb_hist\n",
    "\n",
    "# From https://medium.com/mlearning-ai/color-shape-and-texture-feature-extraction-using-opencv-cb1feb2dbd73\n",
    "# Extracts Local Binary Pattern (Texture) of an image\n",
    "def extract_lbp(path, face_index, image, version):\n",
    "    # reads the input image as a grayscale image\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    desc = LocalBinaryPatterns(24, 8)\n",
    "    lbp_hist, lbp_img = desc.describe(gray)\n",
    "    \n",
    "    return lbp_hist\n",
    "    \n",
    "# From https://docs.opencv.org/4.x/d2/d2c/tutorial_sobel_derivatives.html and https://gist.github.com/rahit/c078cabc0a48f2570028bff397a9e154\n",
    "def extract_gradients(path, face_index, image, version):\n",
    "    # Uses the Sobel Filter to extract the gradients of an image\n",
    "    # reads the input image, then converts BGR color space to RGB\n",
    "    # img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # compute the 1st order Sobel derivative in X-direction\n",
    "    sobelx = cv2.Sobel(image, cv2.CV_64F, 1, 0, ksize=5)\n",
    "\n",
    "    # compute the 1st order Sobel derivative in Y-direction\n",
    "    sobely = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize=5)\n",
    "    \n",
    "    # combine sobelx and sobely to form sobel\n",
    "    sobel = sobelx + sobely\n",
    "    \n",
    "    sobelx_hist = cv2.calcHist([np.float32(sobelx)], [0], None, [26], [0, 256])\n",
    "    sobelx_hist = sobelx_hist.ravel()\n",
    "    sobelx_hist = sobelx_hist.astype('float')\n",
    "    sobelx_hist /= sobelx_hist.sum()\n",
    "    \n",
    "    sobely_hist = cv2.calcHist([np.float32(sobely)], [0], None, [26], [0, 256])\n",
    "    sobely_hist = sobely_hist.ravel()\n",
    "    sobely_hist = sobely_hist.astype('float')\n",
    "    sobely_hist /= sobely_hist.sum()\n",
    "    \n",
    "    sobel_hist = cv2.calcHist([np.float32(sobel)], [0], None, [26], [0, 256])\n",
    "    sobel_hist = sobel_hist.ravel()\n",
    "    sobel_hist = sobel_hist.astype('float')\n",
    "    sobel_hist /= sobel_hist.sum()\n",
    "    \n",
    "    return sobelx_hist, sobely_hist, sobel_hist\n",
    "\n",
    "def extract_image_attributes(row, path, face_index, image, version):\n",
    "    r_hist, g_hist, b_hist, h_hist_HSV, s_hist_HSV, v_hist_HSV, h_hist_HSL, s_hist_HSL, l_hist_HSL, l_hist_LAB, a_hist_LAB, b_hist_LAB, y_hist, cr_hist, cb_hist = extract_color_channel(path, face_index, image, version)\n",
    "    lbp_hist = extract_lbp(path, face_index, image, version)\n",
    "    sobelx_hist, sobely_hist, sobel_hist = extract_gradients(path, face_index, image, version)\n",
    "    \n",
    "    RGB_size = SV_HSV_size = SL_HSL_size = LAB_size = YCRCB_size = 26\n",
    "    \n",
    "    for i in range(0, RGB_size):\n",
    "        row['R_BIN_' + version + '_' + str(i)] = r_hist[i]\n",
    "        \n",
    "    for i in range(0, RGB_size):\n",
    "        row['G_BIN_' + version + '_' + str(i)] = g_hist[i]\n",
    "        \n",
    "    for i in range(0, RGB_size):\n",
    "        row['B_BIN_' + version + '_' + str(i)] = b_hist[i]\n",
    "        \n",
    "    for i in range(0, h_hist_HSV.size):\n",
    "        row['H_HSV_BIN_' + version + '_' + str(i)] = h_hist_HSV[i]\n",
    "        \n",
    "    for i in range(0, SV_HSV_size):\n",
    "        row['S_HSV_BIN_' + version + '_' + str(i)] = s_hist_HSV[i]\n",
    "        \n",
    "    for i in range(0, SV_HSV_size):\n",
    "        row['V_HSV_BIN_' + version + '_' + str(i)] = v_hist_HSV[i]\n",
    "        \n",
    "    for i in range(0, h_hist_HSL.size):\n",
    "        row['H_HSL_BIN_' + version + '_' + str(i)] = h_hist_HSL[i]\n",
    "        \n",
    "    for i in range(0, SL_HSL_size):\n",
    "        row['S_HSL_BIN_' + version + '_' + str(i)] = s_hist_HSL[i]\n",
    "        \n",
    "    for i in range(0, SL_HSL_size):\n",
    "        row['L_HSL_BIN_' + version + '_' + str(i)] = l_hist_HSL[i]\n",
    "        \n",
    "    for i in range(0, LAB_size):\n",
    "        row['L_LAB_BIN_' + version + '_' + str(i)] = l_hist_LAB[i]\n",
    "    \n",
    "    for i in range(0, LAB_size):\n",
    "        row['A_LAB_BIN_' + version + '_' + str(i)] = a_hist_LAB[i]\n",
    "        \n",
    "    for i in range(0, LAB_size):\n",
    "        row['B_LAB_BIN_' + version + '_' + str(i)] = b_hist_LAB[i]\n",
    "        \n",
    "    for i in range(0, YCRCB_size):\n",
    "        row['Y_BIN_' + version + '_' + str(i)] = y_hist[i]\n",
    "        \n",
    "    for i in range(0, YCRCB_size):\n",
    "        row['CR_BIN_' + version + '_' + str(i)] = cr_hist[i]\n",
    "        \n",
    "    for i in range(0, YCRCB_size):\n",
    "        row['CB_BIN_' + version + '_' + str(i)] = cb_hist[i]\n",
    "        \n",
    "    for i in range(0, lbp_hist.size):\n",
    "        row[\"LBP_BIN_\" + version + '_' + str(i)] = lbp_hist[i]\n",
    "        \n",
    "    for i in range(0, sobelx_hist.size):\n",
    "        row[\"SOBELX_BIN_\" + version + '_' + str(i)] = sobelx_hist[i]\n",
    "        \n",
    "    for i in range(0, sobely_hist.size):\n",
    "        row[\"SOBELY_BIN_\" + version + '_' + str(i)] = sobely_hist[i]\n",
    "        \n",
    "    for i in range(0, sobel_hist.size):\n",
    "        row[\"SOBEL_BIN_\" + version + '_' + str(i)] = sobel_hist[i]\n",
    "    \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43b4538d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detach_cpu(image):\n",
    "    return image.detach().cpu()\n",
    "\n",
    "# convert 1x3x416x416 to 416x416x3\n",
    "def reshape_image(image):\n",
    "    return np.transpose(np.squeeze(image), (1 ,2, 0))\n",
    "\n",
    "# convert 1x3x416x416 tensor to 416x416x3 numpy image\n",
    "def tensor_to_image(image):\n",
    "    return np.transpose(image.detach().cpu().squeeze().numpy(), (1, 2, 0))\n",
    "\n",
    "def save_tensor_as_image(image, path):\n",
    "    save_img = cv2.cvtColor(np.moveaxis((image.detach().numpy() * 255).squeeze(), 0, -1).astype('uint8'), cv2.COLOR_RGB2BGR)\n",
    "    cv2.imwrite(path, save_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5f0a347d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsm_attack(image, e, data_grad, bbox):\n",
    "    # Collect the element-wise sign of the data gradient\n",
    "    sign_data_grad = data_grad.sign().numpy()\n",
    "    # Create the perturbed image by adjusting each pixel of the input image\n",
    "    image = image.clone().detach()\n",
    "    perturbed_image = image\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    perturbed_image[..., y1:y2, x1:x2] = perturbed_image[..., y1:y2, x1:x2] + e * sign_data_grad[..., y1:y2, x1:x2]\n",
    "    # Adding clipping to maintain [0,1] range\n",
    "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "    # Return the perturbed image\n",
    "    return perturbed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3ebb8121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(model, device, path, eps_model):\n",
    "    \n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    row = {} #the information/columns for a single row in the dataset is stored here\n",
    "    \n",
    "    df = pd.DataFrame() # dataframe storing the dataset\n",
    "    row['path'] = path\n",
    "    file_basename = os.path.basename(path)\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    model.gradient_mode = False\n",
    "    for yolo_layer in model.yolo_layers:\n",
    "        yolo_layer.gradient_mode = False\n",
    "\n",
    "    # read and transform the image from the path\n",
    "    data = cv2.imread(path)  # read the image\n",
    "    data = cv2.cvtColor(data, cv2.COLOR_BGR2RGB) #change to rgb\n",
    "    data = transforms.Compose([DEFAULT_TRANSFORMS,Resize(416)])((data, np.zeros((1, 5))))[0].unsqueeze(0) # transform the image\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Forward pass the data through the model and call non max suppression\n",
    "        nms, nms_output = non_max_suppression(model(data), 0.5, 0.5) #conf_thres and iou_thres = 0.5\n",
    "\n",
    "    face_list = []\n",
    "    if type(nms_output[0]) is not int:\n",
    "        face_list = nms_output[0]\n",
    "\n",
    "    data = data.to(device)\n",
    "    # Set requires_grad attribute of tensor. Important for Attack\n",
    "    data.requires_grad = True\n",
    "\n",
    "    model.gradient_mode = True\n",
    "    for yolo_layer in model.yolo_layers:\n",
    "        yolo_layer.gradient_mode = True\n",
    "\n",
    "    output = model(data)\n",
    "\n",
    "    # loop through each of the faces in the image\n",
    "    for face_index, face_row in enumerate(face_list): #nms_output[0] because the model is designed to take in several images at a time from the dataloader but we are only loading the image one at a time\n",
    "\n",
    "        row['face_index'] = face_index\n",
    "        print(\"Face\", face_index)\n",
    "\n",
    "        x, y, w, h = face_row[0], face_row[1], face_row[2], face_row[3]\n",
    "        row['x'], row['y'], row['w'], row['h'] = x, y, w, h\n",
    "        \n",
    "        factor_x, factor_y, factor_w, factor_h = random.uniform(1, 2), random.uniform(1, 2), random.uniform(1, 2), random.uniform(1, 2)\n",
    "        normal_x, normal_y, normal_w, normal_h = x / 416, y / 416, w / 416, h / 416\n",
    "\n",
    "        new_x = normal_x * factor_x if random.choice([True, False]) else normal_x / factor_x\n",
    "        new_y = normal_y * factor_y if random.choice([True, False]) else normal_y / factor_y\n",
    "        new_w = normal_w * factor_w if random.choice([True, False]) else normal_w / factor_w\n",
    "        new_h = normal_h * factor_h if random.choice([True, False]) else normal_h / factor_h\n",
    "\n",
    "        new_x, new_y, new_w, new_h = max(min(1, new_x), 0), max(min(1, new_y), 0), max(min(1, new_w), 0), max(min(1, new_h), 0)\n",
    "\n",
    "        target = torch.tensor([[0.0, 0, new_x, new_y, new_w, new_h]])\n",
    "        target = target.to(device)\n",
    "\n",
    "        loss, loss_components = compute_loss(output, target, model)\n",
    "\n",
    "        # cropped image with bounding box\n",
    "        # getting (x1, y1) upper left, (x2, y2) lower right\n",
    "        x1 = max(int(np.floor((x - w / 2).detach().cpu().numpy())), 0)\n",
    "        y1 = max(int(np.floor((y - h / 2).detach().cpu().numpy())), 0)\n",
    "        x2 = min(int(np.ceil((x + w / 2).detach().cpu().numpy())), 415)\n",
    "        y2 = min(int(np.ceil((y + h / 2).detach().cpu().numpy())), 415)\n",
    "\n",
    "        cropped_image = detach_cpu(data)[:, :, y1:y2, x1:x2] #get the first dimension, the channels, and crop it\n",
    "        cropped_image = tensor_to_image(cropped_image) #reshape the image to (w/h, h/w, channel)\n",
    "\n",
    "        # Zero all existing gradients\n",
    "        model.zero_grad()\n",
    "        data.grad = None\n",
    "\n",
    "        # Calculate gradients of model in backward pass\n",
    "        loss.backward(retain_graph=True) #TODO: Amos - check if this is correct\n",
    "\n",
    "        # Collect datagrad\n",
    "        data_grad = data.grad.data\n",
    "\n",
    "        row = extract_image_attributes(row, path, face_index, cropped_image, \"bbox\")\n",
    "        \n",
    "        \n",
    "        row['x'], row['y'], row['w'], row['h'] = row['x'] / 416, row['y'] / 416, row['w'] / 416, row['h'] / 416\n",
    "        \n",
    "        \n",
    "        df = df.append(row, ignore_index=True) #append the attributes of one face to the dataframe\n",
    "        \n",
    "        predict_features = get_features(CHOSEN_COLOR_SPACE, CHOSEN_REGION)\n",
    "        X_features = df.loc[:,  predict_features]\n",
    "        \n",
    "        min_eps = eps_model.predict(X_features)\n",
    "        \n",
    "        return fgsm_attack(data, min_eps, data_grad, (x1, y1, x2, y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "94ca1ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(color_space, region):\n",
    "    features = [\"w\", \"h\", \"x\", \"y\"]\n",
    "    for color_channel in color_channels[color_space]: \n",
    "        features += [color_channel + region + \"_\" + str(i) for i in range(26)]\n",
    "    features += [\"LBP_BIN_\" + region + \"_\" + str(i) for i in range(26)]\n",
    "    features += [\"SOBELX_BIN_\" + region + \"_\" + str(i) for i in range(20)]\n",
    "    features += [\"SOBELY_BIN_\" + region + \"_\" + str(i) for i in range(20)]\n",
    "    features += [\"SOBEL_BIN_\" + region + \"_\" + str(i) for i in range(20)]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "48b59783",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_regions = [\"bbox\", \"mask\"]\n",
    "\n",
    "color_channels = {\n",
    "    \"RGB\": (\"R_BIN_\", \"G_BIN_\", \"B_BIN_\"),\n",
    "    \"HSV\": (\"H_HSV_BIN_\", \"S_HSV_BIN_\", \"V_HSV_BIN_\"),\n",
    "    \"HSL\": (\"H_HSL_BIN_\", \"S_HSL_BIN_\", \"L_HSL_BIN_\"),\n",
    "    \"LAB\": (\"L_LAB_BIN_\", \"A_LAB_BIN_\", \"B_LAB_BIN_\"),\n",
    "    \"YCBCR\": (\"Y_BIN_\", \"CR_BIN_\", \"CB_BIN_\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c9b043e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"E:\\\\Documents\\\\GitHub\\\\THS-ST1\\\\images\\\\img_celeba_139\\\\139005.jpg\"\n",
    "CHOSEN_COLOR_SPACE = \"LAB\" #<-- pick a colorspace\n",
    "CHOSEN_REGION = \"bbox\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "578d470c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Program Files\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:289: RuntimeWarning: invalid value encountered in true_divide\n",
      "  correlation_coefficient /= X_norms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('selector',\n",
       "                 SelectKBest(k=109,\n",
       "                             score_func=<function f_regression at 0x00000294D9BB5820>)),\n",
       "                ('mpr',\n",
       "                 MLPRegressor(alpha=0.0006027865352538995,\n",
       "                              beta_1=0.798511248023137, beta_2=0.85,\n",
       "                              early_stopping=True, epsilon=1e-07,\n",
       "                              hidden_layer_sizes=(113, 75, 50),\n",
       "                              learning_rate_init=0.005, max_fun=11112,\n",
       "                              max_iter=1000, momentum=0.8014392629174594,\n",
       "                              n_iter_no_change=15, nesterovs_momentum=False,\n",
       "                              random_state=100, tol=0.1,\n",
       "                              validation_fraction=0.14942278940909776))])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO: load model here (edit this whole cell as necessary, idk paano magproperly load from joblib)\n",
    "random_state = 100\n",
    "\n",
    "import joblib\n",
    "from sklearn.base import clone as clone_model\n",
    "\n",
    "CSV_FILENAME = \"final_eda_features.csv\" #<-- update csv name\n",
    "df_features = pd.read_csv(CSV_FILENAME)\n",
    "\n",
    "labels = {\"mask\": \"e_bbox_yf\", \"bbox\": \"e_face_yf\"} #<-- inverted\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "categorical_columns = df_features.select_dtypes(include=[bool, object]).columns\n",
    "encoded_columns = df_features[categorical_columns].apply(encoder.fit_transform)\n",
    "\n",
    "df_encoded_features = df_features.copy()\n",
    "df_encoded_features[categorical_columns] = encoded_columns\n",
    "\n",
    "def get_features_and_label(color_space, region):\n",
    "    features = [\"w\", \"h\", \"x\", \"y\"]\n",
    "    for color_channel in color_channels[color_space]: \n",
    "        features += [color_channel + region + \"_\" + str(i) for i in range(26)]\n",
    "    features += [\"LBP_BIN_\" + region + \"_\" + str(i) for i in range(26)]\n",
    "    features += [\"SOBELX_BIN_\" + region + \"_\" + str(i) for i in range(20)]\n",
    "    features += [\"SOBELY_BIN_\" + region + \"_\" + str(i) for i in range(20)]\n",
    "    features += [\"SOBEL_BIN_\" + region + \"_\" + str(i) for i in range(20)]\n",
    "    return features, labels[region]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "features, label = get_features_and_label(CHOSEN_COLOR_SPACE, CHOSEN_REGION)\n",
    "\n",
    "X_features =  df_encoded_features.loc[:,  features]\n",
    "y_features = df_encoded_features.loc[:, label].values #<-- pick label\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_features, y_features, test_size = 0.2, random_state=random_state)\n",
    "\n",
    "eps_model = joblib.load(\"model_dumps/mpr_tunedfs_LAB_bbox.pkl\")\n",
    "eps_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6679df28",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m image_features \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m(model, device, img_path, eps_model)\n\u001b[0;32m      2\u001b[0m predict_features \u001b[38;5;241m=\u001b[39m get_features(CHOSEN_COLOR_SPACE, CHOSEN_REGION)\n\u001b[0;32m      3\u001b[0m X_features \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mloc[:,  predict_features]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "image_features = pipeline(model, device, img_path, eps_model)\n",
    "predict_features = get_features(CHOSEN_COLOR_SPACE, CHOSEN_REGION)\n",
    "X_features = df.loc[:,  predict_features]\n",
    "min_eps = eps_model.predict(X_features)\n",
    "fgsm_attack(data, min_eps, data_grad, (x1, y1, x2, y2))\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9be7b2e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c970ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db578273",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
