{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ba50a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523ab874",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_results = pd.read_csv(os.path.join(os.getcwd(), 'e_from_nms_conf.csv'))\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072e3925",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nms_group = df_results[df_results[\"nms\"] >= 0.4].groupby(\"nms\")\n",
    "plt.plot(nms_group.groups.keys(), nms_group.mean(numeric_only = True).iloc[:, -3:], label=[\"YuNet\", \"MediaPipe\", \"Yolo-face\"])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "conf_group = df_results[df_results[\"conf\"] >= 0.4].groupby(\"conf\")\n",
    "plt.plot(conf_group.groups.keys(), conf_group.mean(numeric_only = True).iloc[:, -3:], label=[\"YuNet\", \"MediaPipe\", \"Yolo-face\"])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0790403",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df_results[\"nms\"], df_results.iloc[:, -6], label=\"Yolo-face\")\n",
    "plt.scatter(df_results[\"nms\"], df_results.iloc[:, -5], label=\"MediaPipe\")\n",
    "plt.scatter(df_results[\"nms\"], df_results.iloc[:, -4], label=\"Yolo-face\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(df_results[\"conf\"], df_results.iloc[:, -6], label=\"Yolo-face\")\n",
    "plt.scatter(df_results[\"conf\"], df_results.iloc[:, -5], label=\"MediaPipe\")\n",
    "plt.scatter(df_results[\"conf\"], df_results.iloc[:, -4], label=\"Yolo-face\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a27d6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(nms_group.groups.keys(), nms_group.apply((lambda x: (x.iloc[:, -3:] >= 3).sum())), label=[\"YuNet\", \"MediaPipe\", \"Yolo-face\"])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(conf_group.groups.keys(), conf_group.apply((lambda x: (x.iloc[:, -3:] >= 3).sum())), label=[\"YuNet\", \"MediaPipe\", \"Yolo-face\"])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7ce3fd",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb4d744",
   "metadata": {},
   "outputs": [],
   "source": [
    "import minepsilon as minE\n",
    "\n",
    "import itertools\n",
    "import random\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "from torch import autograd\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from pytorchyolo.models import load_model\n",
    "from pytorchyolo.utils.transforms import Resize, DEFAULT_TRANSFORMS\n",
    "from pytorchyolo.utils.utils import non_max_suppression\n",
    "from pytorchyolo.utils.loss import compute_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c62cf3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "REF_SET = pd.read_csv(os.path.join(os.getcwd(), 'reference_dataset.csv'), index_col=0)\n",
    "REF_SET.reset_index()\n",
    "REF_SET = REF_SET[REF_SET[\"path\"].str.contains(\"amosc\")]              #<-- only sample fr ur local files (smth only u hv in path)\n",
    "celeba_rows = REF_SET[\"path\"].str.contains(\"celeba\")\n",
    "REF_SET_celeba = REF_SET[celeba_rows].sample(16)                      #<-- Change face sample count for celeba\n",
    "REF_SET_wider = REF_SET[celeba_rows == False].sample(16)              #<-- Change face sample count for widerface\n",
    "sample_set = pd.concat([REF_SET_celeba, REF_SET_wider])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0aca01c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Patterned after FGSM tutorial (https://pytorch.org/tutorials/beginner/fgsm_tutorial.html)\n",
    "# Define what device we are using\n",
    "print(\"CUDA Available: \", torch.cuda.is_available())\n",
    "device, model = load_model('./weights/yolo_face_sthanhng.cfg', \"./weights/yolo_face_sthanhng.weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300e1b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detach_cpu(image):\n",
    "    return image.detach().cpu()\n",
    "\n",
    "# convert 1x3x416x416 to 416x416x3\n",
    "def reshape_image(image):\n",
    "    return np.transpose(np.squeeze(image), (1 ,2, 0))\n",
    "\n",
    "# convert 1x3x416x416 tensor to 416x416x3 numpy image\n",
    "def tensor_to_image(image):\n",
    "    return np.transpose(image.detach().cpu().squeeze().numpy(), (1, 2, 0))\n",
    "\n",
    "def save_tensor_as_image(image, path):\n",
    "    save_img = cv2.cvtColor(np.moveaxis((image.detach().numpy() * 255).squeeze(), 0, -1).astype('uint8'), cv2.COLOR_RGB2BGR)\n",
    "    cv2.imwrite(path, save_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f3f549",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nms_scores = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "conf_scores = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "df = pd.DataFrame()\n",
    "row = {}\n",
    "\n",
    "for path in sample_set[\"path\"]:\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    row['path'] = path\n",
    "    REF_SUBSET = sample_set[sample_set['path'] == path]\n",
    "\n",
    "    model.eval()\n",
    "    model.gradient_mode = False\n",
    "    for yolo_layer in model.yolo_layers:\n",
    "        yolo_layer.gradient_mode = False\n",
    "\n",
    "    # read and transform the image from the path\n",
    "    data = cv2.imread(path)  # read the image\n",
    "    data = cv2.cvtColor(data, cv2.COLOR_BGR2RGB) #change to rgb\n",
    "    data = transforms.Compose([DEFAULT_TRANSFORMS,Resize(416)])((data, np.zeros((1, 5))))[0].unsqueeze(0) # transform the image\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Forward pass the data through the model and call non max suppression\n",
    "        nms, nms_output = non_max_suppression(model(data), 0.5, 0.5) #conf_thres and iou_thres = 0.5\n",
    "\n",
    "    face_list = []\n",
    "    if type(nms_output[0]) is not int:\n",
    "        face_list = nms_output[0]\n",
    "\n",
    "    data = data.to(device)\n",
    "    # Set requires_grad attribute of tensor. Important for Attack\n",
    "    data.requires_grad = True\n",
    "\n",
    "    model.gradient_mode = True\n",
    "    for yolo_layer in model.yolo_layers:\n",
    "        yolo_layer.gradient_mode = True\n",
    "\n",
    "    output = model(data)\n",
    "\n",
    "    # loop through each of the faces in the image\n",
    "    for face_index, face_row in enumerate(face_list):\n",
    "        row['face_index'] = face_index\n",
    "\n",
    "        if face_index in set(REF_SUBSET['face_index']):\n",
    "            x, y, w, h = face_row[0], face_row[1], face_row[2], face_row[3]\n",
    "\n",
    "            factor_x, factor_y, factor_w, factor_h = random.uniform(1, 2), random.uniform(1, 2), random.uniform(1, 2), random.uniform(1, 2)\n",
    "            normal_x, normal_y, normal_w, normal_h = x / 416, y / 416, w / 416, h / 416\n",
    "\n",
    "            new_x = normal_x * factor_x if random.choice([True, False]) else normal_x / factor_x\n",
    "            new_y = normal_y * factor_y if random.choice([True, False]) else normal_y / factor_y\n",
    "            new_w = normal_w * factor_w if random.choice([True, False]) else normal_w / factor_w\n",
    "            new_h = normal_h * factor_h if random.choice([True, False]) else normal_h / factor_h\n",
    "\n",
    "            new_x, new_y, new_w, new_h = max(min(1, new_x), 0), max(min(1, new_y), 0), max(min(1, new_w), 0), max(min(1, new_h), 0)\n",
    "\n",
    "            target = torch.tensor([[0.0, 0, new_x, new_y, new_w, new_h]])\n",
    "            target = target.to(device)\n",
    "\n",
    "            loss, loss_components = compute_loss(output, target, model)\n",
    "\n",
    "            # cropped image with bounding box\n",
    "            # getting (x1, y1) upper left, (x2, y2) lower right\n",
    "            x1 = max(int(np.floor((x - w / 2).detach().cpu().numpy())), 0)\n",
    "            y1 = max(int(np.floor((y - h / 2).detach().cpu().numpy())), 0)\n",
    "            x2 = min(int(np.ceil((x + w / 2).detach().cpu().numpy())), 415)\n",
    "            y2 = min(int(np.ceil((y + h / 2).detach().cpu().numpy())), 415)\n",
    "\n",
    "            cropped_image = detach_cpu(data)[:, :, y1:y2, x1:x2] #get the first dimension, the channels, and crop it\n",
    "            cropped_image = tensor_to_image(cropped_image) #reshape the image to (w/h, h/w, channel)\n",
    "\n",
    "            # Zero all existing gradients\n",
    "            model.zero_grad()\n",
    "            data.grad = None\n",
    "\n",
    "            # Calculate gradients of model in backward pass\n",
    "            loss.backward(retain_graph=True)\n",
    "\n",
    "            # Collect datagrad\n",
    "            data_grad = data.grad.data\n",
    "\n",
    "            bbox = (x1, y1, x2, y2)\n",
    "\n",
    "            bbox_mask = np.zeros(data.shape)\n",
    "            bbox_mask[..., y1:y2, x1:x2] = 1\n",
    "            for nms, conf in itertools.product(nms_scores, conf_scores):\n",
    "                row[\"nms\"] = nms\n",
    "                row[\"conf\"] = conf\n",
    "                minE.nms_thres = nms\n",
    "                minE.conf_thres = conf\n",
    "\n",
    "                yn_min_e_bbox = minE.min_model_eps(data.clone().detach(), data_grad.clone().detach(), minE.yn_det_fn, bbox_mask, bbox)\n",
    "                mp_min_e_bbox = minE.min_model_eps(data.clone().detach(), data_grad.clone().detach(), minE.mp_det_fn, bbox_mask, bbox)\n",
    "                yf_min_e_bbox = minE.min_model_eps(data.clone().detach(), data_grad.clone().detach(), minE.yf_det_fn, bbox_mask, bbox)\n",
    "\n",
    "                print(\"yn min bbox:\", yn_min_e_bbox, \"mp min bbox:\", mp_min_e_bbox, \"yf min bbox:\", yf_min_e_bbox)\n",
    "                row['e_bbox_yn'], row['e_bbox_mp'], row['e_bbox_yf'] = yn_min_e_bbox, mp_min_e_bbox, yf_min_e_bbox\n",
    "\n",
    "                df = df.append(row, ignore_index=True)\n",
    "df.to_csv('e_from_nms_conf.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419177e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4489dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"yn_unperturbable\"] = (df[\"e_bbox_yn\"] >= 3).count()\n",
    "df[\"mp_unperturbable\"] = (df[\"e_bbox_mp\"] >= 3).count()\n",
    "df[\"yf_unperturbable\"] = (df[\"e_bbox_yf\"] >= 3).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c4db0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception(\"STOP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262d6c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "evil = sample_set.iloc[0, :][\"path\"]\n",
    "evil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e04c43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "last1 = None\n",
    "last2 = None\n",
    "row = {}\n",
    "df = pd.DataFrame()\n",
    "for path in [evil, evil, evil]:\n",
    "    print(path)\n",
    "    row['path'] = path\n",
    "    REF_SUBSET = sample_set[sample_set['path'] == path]\n",
    "\n",
    "    model.eval()\n",
    "    model.gradient_mode = False\n",
    "    for yolo_layer in model.yolo_layers:\n",
    "        yolo_layer.gradient_mode = False\n",
    "\n",
    "    # read and transform the image from the path\n",
    "    data = cv2.imread(path)  # read the image\n",
    "    data = cv2.cvtColor(data, cv2.COLOR_BGR2RGB) #change to rgb\n",
    "    data = transforms.Compose([DEFAULT_TRANSFORMS,Resize(416)])((data, np.zeros((1, 5))))[0].unsqueeze(0) # transform the image\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Forward pass the data through the model and call non max suppression\n",
    "        nms, nms_output = non_max_suppression(model(data), 0.5, 0.5) #conf_thres and iou_thres = 0.5\n",
    "\n",
    "    face_list = []\n",
    "    if type(nms_output[0]) is not int:\n",
    "        face_list = nms_output[0]\n",
    "\n",
    "    data = data.to(device)\n",
    "    # Set requires_grad attribute of tensor. Important for Attack\n",
    "    data.requires_grad = True\n",
    "\n",
    "    model.gradient_mode = True\n",
    "    for yolo_layer in model.yolo_layers:\n",
    "        yolo_layer.gradient_mode = True\n",
    "\n",
    "    output = model(data)\n",
    "\n",
    "    # loop through each of the faces in the image\n",
    "    for face_index, face_row in enumerate(face_list):\n",
    "        row['face_index'] = face_index\n",
    "\n",
    "        if face_index in set(REF_SUBSET['face_index']):\n",
    "            x, y, w, h = face_row[0], face_row[1], face_row[2], face_row[3]\n",
    "\n",
    "            factor_x, factor_y, factor_w, factor_h = random.uniform(1, 2), random.uniform(1, 2), random.uniform(1, 2), random.uniform(1, 2)\n",
    "            normal_x, normal_y, normal_w, normal_h = x / 416, y / 416, w / 416, h / 416\n",
    "\n",
    "            new_x = normal_x * factor_x if random.choice([True, False]) else normal_x / factor_x\n",
    "            new_y = normal_y * factor_y if random.choice([True, False]) else normal_y / factor_y\n",
    "            new_w = normal_w * factor_w if random.choice([True, False]) else normal_w / factor_w\n",
    "            new_h = normal_h * factor_h if random.choice([True, False]) else normal_h / factor_h\n",
    "\n",
    "            new_x, new_y, new_w, new_h = max(min(1, new_x), 0), max(min(1, new_y), 0), max(min(1, new_w), 0), max(min(1, new_h), 0)\n",
    "\n",
    "            target = torch.tensor([[0.0, 0, new_x, new_y, new_w, new_h]])\n",
    "            target = target.to(device)\n",
    "\n",
    "            loss, loss_components = compute_loss(output, target, model)\n",
    "\n",
    "            # cropped image with bounding box\n",
    "            # getting (x1, y1) upper left, (x2, y2) lower right\n",
    "            x1 = max(int(np.floor((x - w / 2).detach().cpu().numpy())), 0)\n",
    "            y1 = max(int(np.floor((y - h / 2).detach().cpu().numpy())), 0)\n",
    "            x2 = min(int(np.ceil((x + w / 2).detach().cpu().numpy())), 415)\n",
    "            y2 = min(int(np.ceil((y + h / 2).detach().cpu().numpy())), 415)\n",
    "\n",
    "            cropped_image = detach_cpu(data)[:, :, y1:y2, x1:x2] #get the first dimension, the channels, and crop it\n",
    "            cropped_image = tensor_to_image(cropped_image) #reshape the image to (w/h, h/w, channel)\n",
    "\n",
    "            # Zero all existing gradients\n",
    "            model.zero_grad()\n",
    "            data.grad = None\n",
    "\n",
    "            # Calculate gradients of model in backward pass\n",
    "            #loss.backward(retain_graph=True)\n",
    "            data_grad = autograd.grad(loss, data, torch.ones_like(loss), retain_graph=True)[0]\n",
    "            #print(data_grad)\n",
    "            # Collect datagrad\n",
    "            #data_grad = data.grad.data\n",
    "\n",
    "            bbox = (x1, y1, x2, y2)\n",
    "\n",
    "            bbox_mask = np.zeros(data.shape)\n",
    "            bbox_mask[..., y1:y2, x1:x2] = 1\n",
    "            \n",
    "            if (last1 is not None):\n",
    "                print(torch.any(last1 == data))\n",
    "                print(torch.any(last2 == data_grad))\n",
    "            last1 = data\n",
    "            last2 = data_grad\n",
    "            \n",
    "            yn_min_e_bbox = minE.min_model_eps(data.clone().detach(), data_grad.clone().detach(), minE.yn_det_fn, bbox_mask, bbox)\n",
    "            mp_min_e_bbox = minE.min_model_eps(data.clone().detach(), data_grad.clone().detach(), minE.mp_det_fn, bbox_mask, bbox)\n",
    "            yf_min_e_bbox = minE.min_model_eps(data.clone().detach(), data_grad.clone().detach(), minE.yf_det_fn, bbox_mask, bbox)\n",
    "\n",
    "            #print(\"yn min bbox:\", yn_min_e_bbox, \"mp min bbox:\", mp_min_e_bbox, \"yf min bbox:\", yf_min_e_bbox)\n",
    "            row['e_bbox_yn'], row['e_bbox_mp'], row['e_bbox_yf'] = yn_min_e_bbox, mp_min_e_bbox, yf_min_e_bbox\n",
    "\n",
    "            df = df.append(row, ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d563a451",
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in [evil, evil, evil, evil, evil, evil]:\n",
    "    data = cv2.imread(path)  # read the image\n",
    "    data = cv2.cvtColor(data, cv2.COLOR_BGR2RGB) #change to rgb\n",
    "    data = transforms.Compose([DEFAULT_TRANSFORMS,Resize(416)])((data, np.zeros((1, 5))))[0].unsqueeze(0) # transform the image\n",
    "    print(minE.min_model_eps(data.clone().detach(), data_grad.clone().detach(), minE.yf_det_fn, bbox_mask, bbox))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d9a334",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, yf_face_detector = minE.models.load_model('./weights/yolo_face_sthanhng.cfg', \"./weights/yolo_face_sthanhng.weights\")\n",
    "yf_face_detector.eval()\n",
    "for i in range(10):\n",
    "    for path in [evil, evil, evil, evil, evil, evil]:\n",
    "        image = cv2.imread(path)  # read the image\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) #change to rgb\n",
    "        #bboxes = minE.detect.detect_image(yf_face_detector, image, conf_thres=0.5, nms_thres=0.5)\n",
    "        image = transforms.Compose([DEFAULT_TRANSFORMS,Resize(416)])((image, np.zeros((1, 5))))[0].unsqueeze(0) # transform the image\n",
    "        print(yf_face_detector(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7738a68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
