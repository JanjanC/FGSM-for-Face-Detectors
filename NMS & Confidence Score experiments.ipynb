{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d554b245",
   "metadata": {},
   "outputs": [],
   "source": [
    "import minepsilon as minE\n",
    "\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from pytorchyolo.models import load_model\n",
    "from pytorchyolo.utils.transforms import Resize, DEFAULT_TRANSFORMS\n",
    "from pytorchyolo.utils.utils import non_max_suppression\n",
    "from pytorchyolo.utils.loss import compute_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d7f0f7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = pd.read_csv(\"compiled_features_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b30f41b3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#my_celeba = [\"img_celeba_\" + str(i) for i in range(51, 102)]\n",
    "REF_SET = pd.read_csv(os.path.join(os.getcwd(), 'reference_dataset.csv'), index_col=0)\n",
    "REF_SET.reset_index()\n",
    "REF_SET = REF_SET[REF_SET[\"path\"].str.contains(\"amosc\")]\n",
    "celeba_rows = REF_SET[\"path\"].str.contains(\"celeba\")\n",
    "REF_SET_celeba = REF_SET[celeba_rows].sample(10)                      #<-- Change face sample count for celeba\n",
    "REF_SET_wider = REF_SET[celeba_rows == False].sample(10)              #<-- Change face sample count for wi\n",
    "sample_set = pd.concat([REF_SET_celeba, REF_SET_wider])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "148ddca1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available:  False\n"
     ]
    }
   ],
   "source": [
    "# Patterned after FGSM tutorial (https://pytorch.org/tutorials/beginner/fgsm_tutorial.html)\n",
    "# Define what device we are using\n",
    "print(\"CUDA Available: \", torch.cuda.is_available())\n",
    "device, model = load_model('./weights/yolo_face_sthanhng.cfg', \"./weights/yolo_face_sthanhng.weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ccdd16f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detach_cpu(image):\n",
    "    return image.detach().cpu()\n",
    "\n",
    "# convert 1x3x416x416 to 416x416x3\n",
    "def reshape_image(image):\n",
    "    return np.transpose(np.squeeze(image), (1 ,2, 0))\n",
    "\n",
    "# convert 1x3x416x416 tensor to 416x416x3 numpy image\n",
    "def tensor_to_image(image):\n",
    "    return np.transpose(image.detach().cpu().squeeze().numpy(), (1, 2, 0))\n",
    "\n",
    "def save_tensor_as_image(image, path):\n",
    "    save_img = cv2.cvtColor(np.moveaxis((image.detach().numpy() * 255).squeeze(), 0, -1).astype('uint8'), cv2.COLOR_RGB2BGR)\n",
    "    cv2.imwrite(path, save_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d3300b3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yn min bbox: 0.15 mp min bbox: 0.24499999999999997 yf min bbox: 0.145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amosc\\AppData\\Local\\Temp\\ipykernel_5032\\2217610674.py:100: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yn min bbox: 0.15499999999999997 mp min bbox: 0 yf min bbox: 0.10999999999999999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amosc\\AppData\\Local\\Temp\\ipykernel_5032\\2217610674.py:100: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yn min bbox: 3.0999999999999974 mp min bbox: 0.16499999999999998 yf min bbox: 0.15499999999999997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amosc\\AppData\\Local\\Temp\\ipykernel_5032\\2217610674.py:100: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yn min bbox: 0.18499999999999997 mp min bbox: 0.285 yf min bbox: 0.21999999999999997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amosc\\AppData\\Local\\Temp\\ipykernel_5032\\2217610674.py:100: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yn min bbox: 0.16499999999999998 mp min bbox: 0.175 yf min bbox: 0.19499999999999998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amosc\\AppData\\Local\\Temp\\ipykernel_5032\\2217610674.py:100: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yn min bbox: 0.18999999999999997 mp min bbox: 0.19999999999999998 yf min bbox: 0.285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amosc\\AppData\\Local\\Temp\\ipykernel_5032\\2217610674.py:100: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yn min bbox: 0.7150000000000003 mp min bbox: 0.21999999999999997 yf min bbox: 0.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amosc\\AppData\\Local\\Temp\\ipykernel_5032\\2217610674.py:100: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yn min bbox: 0.16999999999999998 mp min bbox: 0.20999999999999996 yf min bbox: 0.25999999999999995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amosc\\AppData\\Local\\Temp\\ipykernel_5032\\2217610674.py:100: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yn min bbox: 0.10499999999999998 mp min bbox: 0.18499999999999997 yf min bbox: 0.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amosc\\AppData\\Local\\Temp\\ipykernel_5032\\2217610674.py:100: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yn min bbox: 0.12 mp min bbox: 0.3950000000000001 yf min bbox: 0.125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amosc\\AppData\\Local\\Temp\\ipykernel_5032\\2217610674.py:100: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yn min bbox: 0.15 mp min bbox: 0.23999999999999996 yf min bbox: 0.24999999999999997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amosc\\AppData\\Local\\Temp\\ipykernel_5032\\2217610674.py:100: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yn min bbox: 0.15999999999999998 mp min bbox: 0.25499999999999995 yf min bbox: 0.10999999999999999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amosc\\AppData\\Local\\Temp\\ipykernel_5032\\2217610674.py:100: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yn min bbox: 0.15999999999999998 mp min bbox: 0 yf min bbox: 0.05500000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amosc\\AppData\\Local\\Temp\\ipykernel_5032\\2217610674.py:100: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(row, ignore_index=True)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 95\u001b[0m\n\u001b[0;32m     93\u001b[0m yn_min_e_bbox \u001b[38;5;241m=\u001b[39m minE\u001b[38;5;241m.\u001b[39mmin_model_eps(data\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach(), data_grad\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach(), minE\u001b[38;5;241m.\u001b[39myn_det_fn, bbox_mask, bbox)\n\u001b[0;32m     94\u001b[0m mp_min_e_bbox \u001b[38;5;241m=\u001b[39m minE\u001b[38;5;241m.\u001b[39mmin_model_eps(data\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach(), data_grad\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach(), minE\u001b[38;5;241m.\u001b[39mmp_det_fn, bbox_mask, bbox)\n\u001b[1;32m---> 95\u001b[0m yf_min_e_bbox \u001b[38;5;241m=\u001b[39m \u001b[43mminE\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin_model_eps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_grad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mminE\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43myf_det_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myn min bbox:\u001b[39m\u001b[38;5;124m\"\u001b[39m, yn_min_e_bbox, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmp min bbox:\u001b[39m\u001b[38;5;124m\"\u001b[39m, mp_min_e_bbox, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myf min bbox:\u001b[39m\u001b[38;5;124m\"\u001b[39m, yf_min_e_bbox)\n\u001b[0;32m     98\u001b[0m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124me_bbox_yn\u001b[39m\u001b[38;5;124m'\u001b[39m], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124me_bbox_mp\u001b[39m\u001b[38;5;124m'\u001b[39m], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124me_bbox_yf\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m yn_min_e_bbox, mp_min_e_bbox, yf_min_e_bbox\n",
      "File \u001b[1;32mD:\\Users\\amosc\\Documents\\Coding\\Thesis\\THS-ST2\\THS-ST1\\minepsilon.py:132\u001b[0m, in \u001b[0;36mmin_model_eps\u001b[1;34m(image, data_grad, det_fn, mask, bbox, start, end, step, background)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;124;03m\"\"\" # Save img sample\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;124;03msave_img = cv2.cvtColor(np.moveaxis((perturbed_img.detach().numpy() * 255).squeeze(), 0, -1).astype('uint8'), cv2.COLOR_RGB2BGR)\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;124;03mcv2.imwrite('_2cantdetect.png', save_img)\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03mprint(\"\\te undetectable | closest bbox:\", closest_bbox(det_fn(perturbed_img), bbox), \"eps:\", eps)\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;66;03m#print(\"IOU2:\", closest_bbox(det_fn(perturbed_img), bbox)[1])\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \n\u001b[0;32m    131\u001b[0m \u001b[38;5;66;03m# Decrease the epsilon value by 0.01 until it can be detected by the detection function or until the start\u001b[39;00m\n\u001b[1;32m--> 132\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m closest_bbox(\u001b[43mdet_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mperturbed_img\u001b[49m\u001b[43m)\u001b[49m, bbox)[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m eps \u001b[38;5;241m>\u001b[39m start:\n\u001b[0;32m    133\u001b[0m     step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.005\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m eps \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.01\u001b[39m\n\u001b[0;32m    134\u001b[0m     eps \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m step\n",
      "File \u001b[1;32mD:\\Users\\amosc\\Documents\\Coding\\Thesis\\THS-ST2\\THS-ST1\\minepsilon.py:223\u001b[0m, in \u001b[0;36myf_det_fn\u001b[1;34m(image, return_boxes, conf_thres, nms_thres)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21myf_det_fn\u001b[39m(image, return_boxes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, conf_thres\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, nms_thres\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m):\n\u001b[0;32m    222\u001b[0m     image \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmoveaxis((image\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze(), \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muint8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 223\u001b[0m     bboxes \u001b[38;5;241m=\u001b[39m \u001b[43mdetect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43myf_face_detector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf_thres\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf_thres\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnms_thres\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnms_thres\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_boxes:\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m bboxes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Users\\amosc\\Documents\\Coding\\Thesis\\THS-ST2\\THS-ST1\\pytorchyolo\\detect.py:94\u001b[0m, in \u001b[0;36mdetect_image\u001b[1;34m(model, image, img_size, conf_thres, nms_thres)\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;66;03m# Get detections\u001b[39;00m\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 94\u001b[0m         detections \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_img\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m         detections, _ \u001b[38;5;241m=\u001b[39m non_max_suppression(detections, conf_thres, nms_thres)\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m#         detections = rescale_boxes(detections[0], img_size, image.shape[:2])\u001b[39;00m\n",
      "File \u001b[1;32mD:\\ProgramData\\conda\\envs\\thesis-pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\Users\\amosc\\Documents\\Coding\\Thesis\\THS-ST2\\THS-ST1\\pytorchyolo\\models.py:186\u001b[0m, in \u001b[0;36mDarknet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (module_def, module) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_defs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_list)):\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module_def[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconvolutional\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupsample\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaxpool\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m--> 186\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m module_def[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroute\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    188\u001b[0m         combined_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([layer_outputs[\u001b[38;5;28mint\u001b[39m(layer_i)] \u001b[38;5;28;01mfor\u001b[39;00m layer_i \u001b[38;5;129;01min\u001b[39;00m module_def[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayers\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m)], \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mD:\\ProgramData\\conda\\envs\\thesis-pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\ProgramData\\conda\\envs\\thesis-pytorch\\lib\\site-packages\\torch\\nn\\modules\\container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mD:\\ProgramData\\conda\\envs\\thesis-pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\ProgramData\\conda\\envs\\thesis-pytorch\\lib\\site-packages\\torch\\nn\\modules\\conv.py:457\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\ProgramData\\conda\\envs\\thesis-pytorch\\lib\\site-packages\\torch\\nn\\modules\\conv.py:453\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    451\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    452\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nms_scores = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "conf_scores = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "df = pd.DataFrame()\n",
    "row = {}\n",
    "\n",
    "for nms, conf in itertools.product(nms_scores, conf_scores):\n",
    "    row[\"nms\"] = nms\n",
    "    row[\"conf\"] = conf\n",
    "    minE.nms_thres = nms\n",
    "    minE.conf_thres = conf\n",
    "    \n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    \n",
    "    for path in sample_set[\"path\"]:\n",
    "        row['path'] = path\n",
    "        REF_SUBSET = sample_set[sample_set['path'] == path]\n",
    "        \n",
    "        model.eval()\n",
    "        model.gradient_mode = False\n",
    "        for yolo_layer in model.yolo_layers:\n",
    "            yolo_layer.gradient_mode = False\n",
    "\n",
    "        # read and transform the image from the path\n",
    "        data = cv2.imread(path)  # read the image\n",
    "        data = cv2.cvtColor(data, cv2.COLOR_BGR2RGB) #change to rgb\n",
    "        data = transforms.Compose([DEFAULT_TRANSFORMS,Resize(416)])((data, np.zeros((1, 5))))[0].unsqueeze(0) # transform the image\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Forward pass the data through the model and call non max suppression\n",
    "            nms, nms_output = non_max_suppression(model(data), 0.5, 0.5) #conf_thres and iou_thres = 0.5\n",
    "\n",
    "        face_list = []\n",
    "        if type(nms_output[0]) is not int:\n",
    "            face_list = nms_output[0]\n",
    "\n",
    "        data = data.to(device)\n",
    "        # Set requires_grad attribute of tensor. Important for Attack\n",
    "        data.requires_grad = True\n",
    "\n",
    "        model.gradient_mode = True\n",
    "        for yolo_layer in model.yolo_layers:\n",
    "            yolo_layer.gradient_mode = True\n",
    "\n",
    "        output = model(data)\n",
    "\n",
    "        # loop through each of the faces in the image\n",
    "        for face_index, face_row in enumerate(face_list):\n",
    "            row['face_index'] = face_index\n",
    "            \n",
    "            if face_index in set(REF_SUBSET['face_index']):\n",
    "                x, y, w, h = face_row[0], face_row[1], face_row[2], face_row[3]\n",
    "\n",
    "                factor_x, factor_y, factor_w, factor_h = random.uniform(1, 2), random.uniform(1, 2), random.uniform(1, 2), random.uniform(1, 2)\n",
    "                normal_x, normal_y, normal_w, normal_h = x / 416, y / 416, w / 416, h / 416\n",
    "\n",
    "                new_x = normal_x * factor_x if random.choice([True, False]) else normal_x / factor_x\n",
    "                new_y = normal_y * factor_y if random.choice([True, False]) else normal_y / factor_y\n",
    "                new_w = normal_w * factor_w if random.choice([True, False]) else normal_w / factor_w\n",
    "                new_h = normal_h * factor_h if random.choice([True, False]) else normal_h / factor_h\n",
    "\n",
    "                new_x, new_y, new_w, new_h = max(min(1, new_x), 0), max(min(1, new_y), 0), max(min(1, new_w), 0), max(min(1, new_h), 0)\n",
    "\n",
    "                target = torch.tensor([[0.0, 0, new_x, new_y, new_w, new_h]])\n",
    "                target = target.to(device)\n",
    "\n",
    "                loss, loss_components = compute_loss(output, target, model)\n",
    "\n",
    "                # cropped image with bounding box\n",
    "                # getting (x1, y1) upper left, (x2, y2) lower right\n",
    "                x1 = max(int(np.floor((x - w / 2).detach().cpu().numpy())), 0)\n",
    "                y1 = max(int(np.floor((y - h / 2).detach().cpu().numpy())), 0)\n",
    "                x2 = min(int(np.ceil((x + w / 2).detach().cpu().numpy())), 415)\n",
    "                y2 = min(int(np.ceil((y + h / 2).detach().cpu().numpy())), 415)\n",
    "\n",
    "                cropped_image = detach_cpu(data)[:, :, y1:y2, x1:x2] #get the first dimension, the channels, and crop it\n",
    "                cropped_image = tensor_to_image(cropped_image) #reshape the image to (w/h, h/w, channel)\n",
    "\n",
    "                # Zero all existing gradients\n",
    "                model.zero_grad()\n",
    "                data.grad = None\n",
    "\n",
    "                # Calculate gradients of model in backward pass\n",
    "                loss.backward(retain_graph=True)\n",
    "\n",
    "                # Collect datagrad\n",
    "                data_grad = data.grad.data\n",
    "\n",
    "                bbox = (x1, y1, x2, y2)\n",
    "\n",
    "                bbox_mask = np.zeros(data.shape)\n",
    "                bbox_mask[..., y1:y2, x1:x2] = 1\n",
    "\n",
    "                yn_min_e_bbox = minE.min_model_eps(data.clone().detach(), data_grad.clone().detach(), minE.yn_det_fn, bbox_mask, bbox)\n",
    "                mp_min_e_bbox = minE.min_model_eps(data.clone().detach(), data_grad.clone().detach(), minE.mp_det_fn, bbox_mask, bbox)\n",
    "                yf_min_e_bbox = minE.min_model_eps(data.clone().detach(), data_grad.clone().detach(), minE.yf_det_fn, bbox_mask, bbox)\n",
    "                \n",
    "                #print(\"yn min bbox:\", yn_min_e_bbox, \"mp min bbox:\", mp_min_e_bbox, \"yf min bbox:\", yf_min_e_bbox)\n",
    "                row['e_bbox_yn'], row['e_bbox_mp'], row['e_bbox_yf'] = yn_min_e_bbox, mp_min_e_bbox, yf_min_e_bbox\n",
    "\n",
    "                df = df.append(row, ignore_index=True)\n",
    "df.to_csv('e_from_nms_conf.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0b20e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
