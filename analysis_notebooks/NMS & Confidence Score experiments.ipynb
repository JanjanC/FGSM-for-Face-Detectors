{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e80a5ff1",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7185eb45",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "df_results = pd.read_csv(os.path.join(os.getcwd(), 'e_from_nms_conf.csv'))\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2243d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nms_group = df_results[df_results[\"nms\"] >= 0.4].groupby(\"nms\")\n",
    "plt.plot(nms_group.groups.keys(), nms_group.mean(numeric_only = True).iloc[:, -3:], label=[\"YuNet\", \"MediaPipe\", \"Yolo-face\"])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "conf_group = df_results[df_results[\"conf\"] >= 0.4].groupby(\"conf\")\n",
    "plt.plot(conf_group.groups.keys(), conf_group.mean(numeric_only = True).iloc[:, -3:], label=[\"YuNet\", \"MediaPipe\", \"Yolo-face\"])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9027a14a",
   "metadata": {},
   "source": [
    "plt.scatter(df_results[\"nms\"], df_results.iloc[:, -6], label=\"Yolo-face\")\n",
    "plt.scatter(df_results[\"nms\"], df_results.iloc[:, -5], label=\"MediaPipe\")\n",
    "plt.scatter(df_results[\"nms\"], df_results.iloc[:, -4], label=\"Yolo-face\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(df_results[\"conf\"], df_results.iloc[:, -6], label=\"Yolo-face\")\n",
    "plt.scatter(df_results[\"conf\"], df_results.iloc[:, -5], label=\"MediaPipe\")\n",
    "plt.scatter(df_results[\"conf\"], df_results.iloc[:, -4], label=\"Yolo-face\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fa1383",
   "metadata": {},
   "source": [
    "plt.plot(nms_group.groups.keys(), nms_group.apply((lambda x: (x.iloc[:, -3:] >= 3).sum())), label=[\"YuNet\", \"MediaPipe\", \"Yolo-face\"])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(conf_group.groups.keys(), conf_group.apply((lambda x: (x.iloc[:, -3:] >= 3).sum())), label=[\"YuNet\", \"MediaPipe\", \"Yolo-face\"])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7ce3fd",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553b644a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edb4d744",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytorchyolo'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtransforms\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m#libraries for yolo\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorchyolo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_model\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorchyolo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Resize, DEFAULT_TRANSFORMS\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorchyolo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m non_max_suppression\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pytorchyolo'"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import itertools\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import cv2\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "#libraries for yolo\n",
    "from pytorchyolo.models import load_model\n",
    "from pytorchyolo.utils.transforms import Resize, DEFAULT_TRANSFORMS\n",
    "from pytorchyolo.utils.utils import non_max_suppression\n",
    "from pytorchyolo.utils.loss import compute_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9c62cf3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m REF_SET_celeba \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../images/img_celeba_1/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwalk\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../images/img_celeba_1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m2\u001b[39m]])\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;241m16\u001b[39m)\n\u001b[0;32m      2\u001b[0m REF_SET_wider \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../images/1--Handshaking/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(os\u001b[38;5;241m.\u001b[39mwalk(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../images/1--Handshaking\u001b[39m\u001b[38;5;124m'\u001b[39m))[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m2\u001b[39m]])\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;241m16\u001b[39m)\n\u001b[0;32m      3\u001b[0m sample_set \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([REF_SET_celeba, REF_SET_wider])\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "REF_SET_celeba = pd.Series([\"../images/img_celeba_100/\" + i for i in list(os.walk('../images/img_celeba_100'))[0][2]]).sample(16)\n",
    "REF_SET_wider = pd.Series(['../images/1--Handshaking/' + i for i in list(os.walk('../images/1--Handshaking'))[0][2]]).sample(16)\n",
    "sample_set = pd.concat([REF_SET_celeba, REF_SET_wider]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2951f34a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                   ../images/img_celeba_100/099318.jpg\n",
       "1                   ../images/img_celeba_100/099888.jpg\n",
       "2                   ../images/img_celeba_100/099122.jpg\n",
       "3                   ../images/img_celeba_100/099288.jpg\n",
       "4                   ../images/img_celeba_100/099809.jpg\n",
       "5                   ../images/img_celeba_100/099731.jpg\n",
       "6                   ../images/img_celeba_100/099159.jpg\n",
       "7                   ../images/img_celeba_100/099356.jpg\n",
       "8                   ../images/img_celeba_100/099518.jpg\n",
       "9                   ../images/img_celeba_100/099392.jpg\n",
       "10                  ../images/img_celeba_100/099166.jpg\n",
       "11                  ../images/img_celeba_100/099496.jpg\n",
       "12                  ../images/img_celeba_100/099327.jpg\n",
       "13                  ../images/img_celeba_100/099764.jpg\n",
       "14                  ../images/img_celeba_100/099044.jpg\n",
       "15                  ../images/img_celeba_100/099326.jpg\n",
       "16    ../images/1--Handshaking/1_Handshaking_Handsha...\n",
       "17    ../images/1--Handshaking/1_Handshaking_Handsha...\n",
       "18    ../images/1--Handshaking/1_Handshaking_Handsha...\n",
       "19    ../images/1--Handshaking/1_Handshaking_Handsha...\n",
       "20    ../images/1--Handshaking/1_Handshaking_Handsha...\n",
       "21    ../images/1--Handshaking/1_Handshaking_Handsha...\n",
       "22    ../images/1--Handshaking/1_Handshaking_Handsha...\n",
       "23    ../images/1--Handshaking/1_Handshaking_Handsha...\n",
       "24    ../images/1--Handshaking/1_Handshaking_Handsha...\n",
       "25    ../images/1--Handshaking/1_Handshaking_Handsha...\n",
       "26    ../images/1--Handshaking/1_Handshaking_Handsha...\n",
       "27    ../images/1--Handshaking/1_Handshaking_Handsha...\n",
       "28    ../images/1--Handshaking/1_Handshaking_Handsha...\n",
       "29    ../images/1--Handshaking/1_Handshaking_Handsha...\n",
       "30    ../images/1--Handshaking/1_Handshaking_Handsha...\n",
       "31    ../images/1--Handshaking/1_Handshaking_Handsha...\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300e1b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.utils import detach_cpu, tensor_to_np_img, save_tensor_img, clone_detach, open_img_as_tensor, display_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a70bf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CUDA Available: \", torch.cuda.is_available())\n",
    "main_yf = YoloFace()\n",
    "device, model = main_yf.device, main_yf.yf_face_detector\n",
    "\n",
    "\n",
    "# Set the model in evaluation mode. In this case this is for the Dropout layers\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1caf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mask(filename, face_num, target_bbox):\n",
    "    # Get the mask name and open it\n",
    "    filename = \"restored_mask_\" + os.path.splitext(filename)[0] + \"_\" + str(face_num) + \"_image_final.png\"\n",
    "    mask = cv2.imread(os.path.join('../', RESTORED_MASK_PATH, filename), 0)\n",
    "    \n",
    "    # Find the corresponding row of the mask in the FACES_DF to get the padding\n",
    "    face_row = FACES_DF.loc[FACES_DF['filename'] == filename]\n",
    "    padded_dim = (int(face_row[\"x2_pad\"] - face_row[\"x1_pad\"]), int(face_row[\"y2_pad\"] - face_row[\"y1_pad\"]))\n",
    "    \n",
    "    # Get the target dimensions based on the target bounding boxes\n",
    "    target_dim = (int(target_bbox[2] - target_bbox[0]), int(target_bbox[3] - target_bbox[1]))\n",
    "    \n",
    "    # Get the number of white pixels in the mask\n",
    "    num_white = dict(zip(*np.unique(mask, return_counts = True)))[255]\n",
    "    # Return an clear if the number of white pixels is less than 10% of the target dimensions\n",
    "    if num_white < int(target_dim[0] * target_dim[1] * 0.1):\n",
    "        return torch.ones((1, 3, target_dim[1], target_dim[0])), False\n",
    "    \n",
    "    # Appply morphological transformation to the mask\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (int(mask.shape[0] * 0.5), int(mask.shape[1] * 0.5)))\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "    mask = cv2.cvtColor(mask, cv2.COLOR_GRAY2RGB)\n",
    "    mask = transforms.Compose([DEFAULT_TRANSFORMS])((mask, np.zeros((1, 5))))[0].unsqueeze(0)\n",
    "    \n",
    "    # Masks are always square and should be smaller than the padded dimensions and target dimensions\n",
    "    current_dim = max(mask.shape)\n",
    "    diff_x, diff_y = abs(padded_dim[0] - current_dim) / 2, abs(padded_dim[1] - current_dim) / 2\n",
    "    \n",
    "    # Adjust the mask to be the same as the padded dimensions\n",
    "    if diff_y != 0:\n",
    "        mask = mask[..., int(np.floor(diff_y)):-int(np.ceil(diff_y)), :]\n",
    "    if diff_x != 0:\n",
    "        mask = mask[..., int(np.floor(diff_x)):-int(np.ceil(diff_x))]\n",
    "        \n",
    "    # Calculate the actual size without the padding\n",
    "    padding = [\n",
    "        int(abs(face_row[\"x1\"] - face_row[\"x1_pad\"])),\n",
    "        int(abs(face_row[\"y1\"] - face_row[\"y1_pad\"])),\n",
    "        int(abs(face_row[\"x2\"] - face_row[\"x2_pad\"])),\n",
    "        int(abs(face_row[\"y2\"] - face_row[\"y2_pad\"]))\n",
    "    ]\n",
    "    \n",
    "    # Adjust the padding so that it will match the target bounding box\n",
    "    new_dim = padded_dim[0] - padding[0] - padding[2], padded_dim[1] - padding[1] - padding[3]\n",
    "    diff_x, diff_y = (target_dim[0] - new_dim[0]) / 2, (target_dim[1] - new_dim[1]) / 2\n",
    "    \n",
    "    padding[0] -= int(np.floor(diff_x))\n",
    "    padding[1] -= int(np.floor(diff_y))\n",
    "    padding[2] -= int(np.ceil(diff_x))\n",
    "    padding[3] -= int(np.ceil(diff_y))\n",
    "    \n",
    "    mask = F.pad(input=mask, pad=(-padding[0], -padding[2], -padding[1], -padding[3]), mode='constant', value=0)\n",
    "    \n",
    "    return mask, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12203f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(path):\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    \n",
    "    df = pd.DataFrame() # dataframe storing the dataset\n",
    "    row = {} #the information/columns for a single row in the dataset is stored here\n",
    "    grads = []\n",
    "    bboxes = []\n",
    "    \n",
    "    file_basename = os.path.basename(path)\n",
    "    print(file_basename, end=\" \")\n",
    "    print(\"<- working on\")\n",
    "\n",
    "    row['path'] = path\n",
    "\n",
    "    model.eval()\n",
    "    model.gradient_mode = False\n",
    "\n",
    "    for yolo_layer in model.yolo_layers:\n",
    "        yolo_layer.gradient_mode = False\n",
    "\n",
    "    # Read and transform the image from the path\n",
    "    data = cv2.imread(path)\n",
    "    row['source_w'], row['source_h'], _ = data.shape\n",
    "    data = cv2.cvtColor(data, cv2.COLOR_BGR2RGB)\n",
    "    data = transforms.Compose([DEFAULT_TRANSFORMS,Resize(416)])((data, np.zeros((1, 5))))[0].unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Forward pass the data through the model and call non max suppression\n",
    "        nms, nms_output = non_max_suppression(model(data), 0.5, 0.5) #conf_thres and iou_thres = 0.5\n",
    "\n",
    "    face_list = []\n",
    "    if type(nms_output[0]) is not int:\n",
    "        face_list = nms_output[0]\n",
    "\n",
    "    data = data.to(device)\n",
    "\n",
    "    # Set requires_grad attribute of tensor. Important for attack\n",
    "    data.requires_grad = True\n",
    "\n",
    "    model.gradient_mode = True\n",
    "    for yolo_layer in model.yolo_layers:\n",
    "        yolo_layer.gradient_mode = True\n",
    "\n",
    "    output = model(data)\n",
    "\n",
    "    # loop through each of the faces in the image\n",
    "    for face_index, face_row in enumerate(face_list): #nms_output[0] because the model is designed to take in several images at a time from the dataloader but we are only loading the image one at a time\n",
    "\n",
    "        row['face_index'] = face_index\n",
    "        print(\"Face\", face_index)\n",
    "\n",
    "        row['obj_score'] = face_row[4].item()\n",
    "        row['class_score'] = face_row[5].item()\n",
    "        x, y, w, h = face_row[0], face_row[1], face_row[2], face_row[3]\n",
    "\n",
    "        normal_x, normal_y, normal_w, normal_h = x / 415, y / 415, w / 415, h / 415\n",
    "\n",
    "        target = torch.tensor([[0.0, 0, normal_x, normal_y, normal_w, normal_h]])\n",
    "\n",
    "        target = target.to(device)\n",
    "        loss, loss_components = compute_loss(output, target, model)\n",
    "\n",
    "        # cropped image with bounding box\n",
    "        # getting (x1, y1) upper left, (x2, y2) lower right\n",
    "        x1 = max(int(np.floor((x - w / 2).detach().cpu().numpy())), 0)\n",
    "        y1 = max(int(np.floor((y - h / 2).detach().cpu().numpy())), 0)\n",
    "        x2 = min(int(np.ceil((x + w / 2).detach().cpu().numpy())), 415)\n",
    "        y2 = min(int(np.ceil((y + h / 2).detach().cpu().numpy())), 415)\n",
    "\n",
    "        row['x1'], row['y1'], row['x2'], row['y2'] = x1, y1, x2, y2\n",
    "        row['x'], row['y'], row['w'], row['h'] = x, y, w, h\n",
    "        \n",
    "        cropped_image = detach_cpu(data)[:, :, y1:y2, x1:x2] #get the first dimension, the channels, and crop it\n",
    "        cropped_image = tensor_to_np_img(cropped_image) #reshape the image to (w/h, h/w, channel)\n",
    "\n",
    "        # Zero all existing gradients\n",
    "        model.zero_grad()\n",
    "        data.grad = None\n",
    "\n",
    "        # Calculate gradients of model in backward pass\n",
    "        loss.backward(retain_graph=True) #TODO: Amos - check if this is correct\n",
    "        \n",
    "        # Collect datagrad\n",
    "        data_grad = data.grad.data\n",
    "        grads.append(clone_detach(data_grad))\n",
    "        \n",
    "        bbox = (x1, y1, x2, y2)\n",
    "        bboxes.append(bbox)\n",
    "        \n",
    "        df = pd.concat([df, pd.DataFrame([row])], axis=0, ignore_index=True)\n",
    "    \n",
    "    return df, grads, bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cbe4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "nms_scores = np.linspace(0.5, 1, 10)\n",
    "conf_scores = np.linspace(0.5, 1, 10)\n",
    "df = pd.DataFrame()\n",
    "row = {}\n",
    "\n",
    "for path in sample_set:\n",
    "    data = open_img_as_tensor(path)\n",
    "    feats, grads, bboxes = get_features(path)\n",
    "    \n",
    "    #RESTORED_MASK_PATH = os.path.join(\"../images\", path.split(\"/\")[-2] + '_restored_mask')\n",
    "    #FACES_DF = pd.read_csv(os.path.join(\"../images\", os.path.join(path, path.split(\"/\")[-2] + '_CSV'), file))\n",
    "    \n",
    "    for face_index, (data_grad, bbox) in enumerate(zip(grads, bboxes)):\n",
    "        for nms, conf in itertools.product(nms_scores, conf_scores):\n",
    "            whole_mask = np.zeros(data.shape)\n",
    "            whole_mask[..., bbox[1]:bbox[3], bbox[0]:bbox[2]] = 1\n",
    "            \n",
    "            print(\"nms:\", nms, \"conf:\", conf)\n",
    "            row[\"nms\"] = nms\n",
    "            row[\"conf\"] = conf\n",
    "\n",
    "            yn = YoloFace(nms=nms, conf=conf)\n",
    "            mp = MediaPipe(conf=conf)\n",
    "            yf = YoloFace(nms=nms, conf=conf)\n",
    "\n",
    "            yn_min_e_face = fgsm.binary_search(clone_detach(data), clone_detach(data_grad), yn, whole_mask, bbox)\n",
    "            mp_min_e_face = fgsm.binary_search(clone_detach(data), clone_detach(data_grad), mp, whole_mask, bbox)\n",
    "            yf_min_e_face = fgsm.binary_search(clone_detach(data), clone_detach(data_grad), yf, whole_mask, bbox)\n",
    "\n",
    "            print(\"yn min face:\", yn_min_e_face, \"mp min face:\", mp_min_e_face, \"yf min face:\", yf_min_e_face)\n",
    "            row['e_face_yn'], row['e_face_mp'], row['e_face_yf'] = yn_min_e_face, mp_min_e_face, yf_min_e_face\n",
    "            \n",
    "            df = pd.concat([df, pd.DataFrame([row])], axis=0, ignore_index=True)\n",
    "df.to_csv(\"nmsconf.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419177e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758c24db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as plt\n",
    "\n",
    "nms_group = df[df_results[\"nms\"] >= 0.4].groupby(\"nms\")\n",
    "plt.plot(nms_group.groups.keys(), nms_group.mean(numeric_only = True).iloc[:, -3:], label=[\"YuNet\", \"MediaPipe\", \"Yolo-face\"])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "conf_group = df[df_results[\"conf\"] >= 0.4].groupby(\"conf\")\n",
    "plt.plot(conf_group.groups.keys(), conf_group.mean(numeric_only = True).iloc[:, -3:], label=[\"YuNet\", \"MediaPipe\", \"Yolo-face\"])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c4db0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception(\"STOP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262d6c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "evil = sample_set.iloc[0, :][\"path\"]\n",
    "evil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e04c43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "last1 = None\n",
    "last2 = None\n",
    "row = {}\n",
    "df = pd.DataFrame()\n",
    "for path in [evil, evil, evil]:\n",
    "    print(path)\n",
    "    row['path'] = path\n",
    "    REF_SUBSET = sample_set[sample_set['path'] == path]\n",
    "\n",
    "    model.eval()\n",
    "    model.gradient_mode = False\n",
    "    for yolo_layer in model.yolo_layers:\n",
    "        yolo_layer.gradient_mode = False\n",
    "\n",
    "    # read and transform the image from the path\n",
    "    data = cv2.imread(path)  # read the image\n",
    "    data = cv2.cvtColor(data, cv2.COLOR_BGR2RGB) #change to rgb\n",
    "    data = transforms.Compose([DEFAULT_TRANSFORMS,Resize(416)])((data, np.zeros((1, 5))))[0].unsqueeze(0) # transform the image\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Forward pass the data through the model and call non max suppression\n",
    "        nms, nms_output = non_max_suppression(model(data), 0.5, 0.5) #conf_thres and iou_thres = 0.5\n",
    "\n",
    "    face_list = []\n",
    "    if type(nms_output[0]) is not int:\n",
    "        face_list = nms_output[0]\n",
    "\n",
    "    data = data.to(device)\n",
    "    # Set requires_grad attribute of tensor. Important for Attack\n",
    "    data.requires_grad = True\n",
    "\n",
    "    model.gradient_mode = True\n",
    "    for yolo_layer in model.yolo_layers:\n",
    "        yolo_layer.gradient_mode = True\n",
    "\n",
    "    output = model(data)\n",
    "\n",
    "    # loop through each of the faces in the image\n",
    "    for face_index, face_row in enumerate(face_list):\n",
    "        row['face_index'] = face_index\n",
    "\n",
    "        if face_index in set(REF_SUBSET['face_index']):\n",
    "            x, y, w, h = face_row[0], face_row[1], face_row[2], face_row[3]\n",
    "\n",
    "            factor_x, factor_y, factor_w, factor_h = random.uniform(1, 2), random.uniform(1, 2), random.uniform(1, 2), random.uniform(1, 2)\n",
    "            normal_x, normal_y, normal_w, normal_h = x / 416, y / 416, w / 416, h / 416\n",
    "\n",
    "            new_x = normal_x * factor_x if random.choice([True, False]) else normal_x / factor_x\n",
    "            new_y = normal_y * factor_y if random.choice([True, False]) else normal_y / factor_y\n",
    "            new_w = normal_w * factor_w if random.choice([True, False]) else normal_w / factor_w\n",
    "            new_h = normal_h * factor_h if random.choice([True, False]) else normal_h / factor_h\n",
    "\n",
    "            new_x, new_y, new_w, new_h = max(min(1, new_x), 0), max(min(1, new_y), 0), max(min(1, new_w), 0), max(min(1, new_h), 0)\n",
    "\n",
    "            target = torch.tensor([[0.0, 0, new_x, new_y, new_w, new_h]])\n",
    "            target = target.to(device)\n",
    "\n",
    "            loss, loss_components = compute_loss(output, target, model)\n",
    "\n",
    "            # cropped image with bounding box\n",
    "            # getting (x1, y1) upper left, (x2, y2) lower right\n",
    "            x1 = max(int(np.floor((x - w / 2).detach().cpu().numpy())), 0)\n",
    "            y1 = max(int(np.floor((y - h / 2).detach().cpu().numpy())), 0)\n",
    "            x2 = min(int(np.ceil((x + w / 2).detach().cpu().numpy())), 415)\n",
    "            y2 = min(int(np.ceil((y + h / 2).detach().cpu().numpy())), 415)\n",
    "\n",
    "            cropped_image = detach_cpu(data)[:, :, y1:y2, x1:x2] #get the first dimension, the channels, and crop it\n",
    "            cropped_image = tensor_to_image(cropped_image) #reshape the image to (w/h, h/w, channel)\n",
    "\n",
    "            # Zero all existing gradients\n",
    "            model.zero_grad()\n",
    "            data.grad = None\n",
    "\n",
    "            # Calculate gradients of model in backward pass\n",
    "            #loss.backward(retain_graph=True)\n",
    "            data_grad = autograd.grad(loss, data, torch.ones_like(loss), retain_graph=True)[0]\n",
    "            #print(data_grad)\n",
    "            # Collect datagrad\n",
    "            #data_grad = data.grad.data\n",
    "\n",
    "            bbox = (x1, y1, x2, y2)\n",
    "\n",
    "            bbox_mask = np.zeros(data.shape)\n",
    "            bbox_mask[..., y1:y2, x1:x2] = 1\n",
    "            \n",
    "            if (last1 is not None):\n",
    "                print(torch.any(last1 == data))\n",
    "                print(torch.any(last2 == data_grad))\n",
    "            last1 = data\n",
    "            last2 = data_grad\n",
    "            \n",
    "            yn_min_e_bbox = minE.min_model_eps(data.clone().detach(), data_grad.clone().detach(), minE.yn_det_fn, bbox_mask, bbox)\n",
    "            mp_min_e_bbox = minE.min_model_eps(data.clone().detach(), data_grad.clone().detach(), minE.mp_det_fn, bbox_mask, bbox)\n",
    "            yf_min_e_bbox = minE.min_model_eps(data.clone().detach(), data_grad.clone().detach(), minE.yf_det_fn, bbox_mask, bbox)\n",
    "\n",
    "            #print(\"yn min bbox:\", yn_min_e_bbox, \"mp min bbox:\", mp_min_e_bbox, \"yf min bbox:\", yf_min_e_bbox)\n",
    "            row['e_bbox_yn'], row['e_bbox_mp'], row['e_bbox_yf'] = yn_min_e_bbox, mp_min_e_bbox, yf_min_e_bbox\n",
    "\n",
    "            df = df.append(row, ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d563a451",
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in [evil, evil, evil, evil, evil, evil]:\n",
    "    data = cv2.imread(path)  # read the image\n",
    "    data = cv2.cvtColor(data, cv2.COLOR_BGR2RGB) #change to rgb\n",
    "    data = transforms.Compose([DEFAULT_TRANSFORMS,Resize(416)])((data, np.zeros((1, 5))))[0].unsqueeze(0) # transform the image\n",
    "    print(minE.min_model_eps(data.clone().detach(), data_grad.clone().detach(), minE.yf_det_fn, bbox_mask, bbox))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d9a334",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, yf_face_detector = minE.models.load_model('./weights/yolo_face_sthanhng.cfg', \"./weights/yolo_face_sthanhng.weights\")\n",
    "yf_face_detector.eval()\n",
    "for i in range(10):\n",
    "    for path in [evil, evil, evil, evil, evil, evil]:\n",
    "        image = cv2.imread(path)  # read the image\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) #change to rgb\n",
    "        #bboxes = minE.detect.detect_image(yf_face_detector, image, conf_thres=0.5, nms_thres=0.5)\n",
    "        image = transforms.Compose([DEFAULT_TRANSFORMS,Resize(416)])((image, np.zeros((1, 5))))[0].unsqueeze(0) # transform the image\n",
    "        print(yf_face_detector(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7738a68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
