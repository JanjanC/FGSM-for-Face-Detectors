{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cac237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import vgg16, VGG16_Weights\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from pytorchyolo.models import load_model\n",
    "from pytorchyolo.utils.transforms import Resize, DEFAULT_TRANSFORMS\n",
    "from pytorchyolo.utils.utils import non_max_suppression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991df75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_model = vgg16(weights=VGG16_Weights.IMAGENET1K_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e8301d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54578a61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vgg_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed80fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove classifier\n",
    "#reference: https://discuss.pytorch.org/t/how-to-delete-layer-in-pretrained-model/17648\n",
    "vgg_model.classifier = Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd990127",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vgg_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e07877",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  224x224\n",
    "vgg_16_transform = transforms.Compose([\n",
    "    transforms.Resize(224, interpolation=transforms.InterpolationMode.NEAREST),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe31b26e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_img = Image.open('152018.jpg')\n",
    "sample_img = vgg_16_transform(sample_img)\n",
    "plt.imshow(transforms.ToPILImage()(sample_img), interpolation=\"bicubic\")\n",
    "\n",
    "sample_output = vgg_model(sample_img)\n",
    "sample_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcfbe4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = ['2--Demonstration', '6--Funeral', '10--People_Marching', '14--Traffic', '18--Concerts', '22--Picnic', '26--Soldier_Drilling', '30--Surgeons', '34--Baseball', '38--Tennis', '42--Car_Racing', '46--Jockey', '50--Celebration_Or_Party', '54--Rescue', '58--Hockey', 'img_celeba_102', 'img_celeba_103', 'img_celeba_104', 'img_celeba_105', 'img_celeba_106', 'img_celeba_107', 'img_celeba_108', 'img_celeba_109', 'img_celeba_110', 'img_celeba_111', 'img_celeba_112', 'img_celeba_113', 'img_celeba_114', 'img_celeba_115', 'img_celeba_116', 'img_celeba_117', 'img_celeba_118', 'img_celeba_119', 'img_celeba_120', 'img_celeba_121', 'img_celeba_122', 'img_celeba_123', 'img_celeba_124', 'img_celeba_125', 'img_celeba_126', 'img_celeba_127', 'img_celeba_128', 'img_celeba_129', 'img_celeba_130', 'img_celeba_131', 'img_celeba_132', 'img_celeba_133', 'img_celeba_134', 'img_celeba_135', 'img_celeba_136', 'img_celeba_137', 'img_celeba_138', 'img_celeba_139', 'img_celeba_140', 'img_celeba_141', 'img_celeba_142', 'img_celeba_143', 'img_celeba_144', 'img_celeba_145', 'img_celeba_146', 'img_celeba_147', 'img_celeba_148', 'img_celeba_149', 'img_celeba_150', 'img_celeba_151', 'img_celeba_152']\n",
    "\n",
    "_, model = load_model('./weights/yolo_face_sthanhng.cfg', \"./weights/yolo_face_sthanhng.weights\")\n",
    "\n",
    "ref_df = pd.read_csv(os.path.join(os.getcwd(), 'reference_dataset.csv'), index_col=0)\n",
    "ref_df = ref_df.reset_index(drop=True)\n",
    "\n",
    "OUTPUT_FOLDER = os.path.join(os.getcwd(), \"WIDER\")\n",
    "if not os.path.exists(OUTPUT_FOLDER):\n",
    "        os.mkdir(OUTPUT_FOLDER)\n",
    "        \n",
    "for FOLDER_NAME in folders:\n",
    "    \n",
    "    features_df = pd.DataFrame(columns=range(25088))\n",
    "    \n",
    "    INPUT_PATH = os.path.join(os.getcwd(), 'images', FOLDER_NAME)\n",
    "    \n",
    "    FOLDER_PATH = os.path.join(OUTPUT_FOLDER, FOLDER_NAME)\n",
    "    if not os.path.exists(FOLDER_PATH):\n",
    "        os.mkdir(FOLDER_PATH)\n",
    "    \n",
    "    CSV_PATH = os.path.join(FOLDER_PATH, FOLDER_NAME + '_CSV')\n",
    "    if not os.path.exists(CSV_PATH):\n",
    "        os.mkdir(CSV_PATH)\n",
    "    \n",
    "    for path in glob.glob(os.path.join(INPUT_PATH, '*.jpg')):\n",
    "        print(filename)\n",
    "        filename = os.path.basename(path)\n",
    "        filename_df = ref_df[ref_df['source_file'] == filename]\n",
    "        \n",
    "        if len(filename_df.index) > 0:\n",
    "            model.eval()\n",
    "            \n",
    "            data = cv2.imread(path)  # read the image\n",
    "            data = cv2.cvtColor(data, cv2.COLOR_BGR2RGB) #change to rgb\n",
    "            data = transforms.Compose([DEFAULT_TRANSFORMS,Resize(416)])((data, np.zeros((1, 5))))[0].unsqueeze(0) # transform the image\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # Forward pass the data through the model and call non max suppression\n",
    "                nms, nms_output = non_max_suppression(model(data), 0.5, 0.5) #conf_thres and iou_thres = 0.5\n",
    "\n",
    "            face_list = []\n",
    "            if type(nms_output[0]) is not int:\n",
    "                face_list = nms_output[0]\n",
    "            \n",
    "            for face_index, face_row in enumerate(face_list): #nms_output[0] because the model is designed to take in several images at a time from the dataloader but we are only loading the image one at a time\n",
    "                if face_index in set(filename_df['face_index']):\n",
    "                    print(face_index)\n",
    "                    x, y, w, h = face_row[0], face_row[1], face_row[2], face_row[3]\n",
    "                    \n",
    "                    x1 = max(int(np.floor((x - w / 2).detach().cpu().numpy())), 0)\n",
    "                    y1 = max(int(np.floor((y - h / 2).detach().cpu().numpy())), 0)\n",
    "                    x2 = min(int(np.ceil((x + w / 2).detach().cpu().numpy())), 415)\n",
    "                    y2 = min(int(np.ceil((y + h / 2).detach().cpu().numpy())), 415)\n",
    "                    \n",
    "                    cropped_image = data.detach().cpu()[:, :, y1:y2, x1:x2]\n",
    "                    cropped_image = np.transpose(cropped_image.detach().cpu().squeeze().numpy(), (1, 2, 0))\n",
    "                    cropped_image = (cropped_image * 255).astype(np.uint8)\n",
    "                    cropped_image = Image.fromarray(cropped_image)\n",
    "                    \n",
    "#                     plt.imshow(cropped_image, interpolation='nearest')\n",
    "#                     plt.show()\n",
    "                    \n",
    "                    cropped_image = vgg_16_transform(cropped_image)\n",
    "                    with torch.no_grad():\n",
    "                        vgg_output = vgg_model(cropped_image)\n",
    "\n",
    "                    vgg_features = torch.flatten(vgg_output).tolist()\n",
    "#                     print(type(vgg_features))\n",
    "                    features_df.loc[len(features_df)] = vgg_features\n",
    "    \n",
    "    features_df.to_csv(os.path.join(CSV_PATH, FOLDER_NAME + '_VGG16_features' + str(int(time.time())) + '.csv'), index=False)  #save to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031322d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_df.to_csv(os.path.join(CSV_PATH, FOLDER_NAME + '_VGG16_features' + str(int(time.time())) + '.csv'), index=False)  #save to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bd1f9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
