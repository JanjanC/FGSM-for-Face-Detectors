{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cac237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import vgg16, VGG16_Weights\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from pytorchyolo.models import load_model\n",
    "from pytorchyolo.utils.transforms import Resize, DEFAULT_TRANSFORMS\n",
    "from pytorchyolo.utils.utils import non_max_suppression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991df75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_model = vgg16(weights=VGG16_Weights.IMAGENET1K_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e8301d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54578a61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vgg_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed80fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove classifier\n",
    "#reference: https://discuss.pytorch.org/t/how-to-delete-layer-in-pretrained-model/17648\n",
    "vgg_model.classifier = Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd990127",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vgg_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e07877",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  224x224\n",
    "vgg_16_transform = transforms.Compose([\n",
    "    transforms.Resize(224, interpolation=transforms.InterpolationMode.NEAREST),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c77c7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_img = Image.open('152018.jpg')\n",
    "sample_img = vgg_16_transform(sample_img)\n",
    "plt.imshow(transforms.ToPILImage()(sample_img), interpolation=\"bicubic\")\n",
    "\n",
    "sample_output = vgg_model(sample_img)\n",
    "sample_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad9fd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = ['2--Demonstration']\n",
    "\n",
    "_, model = load_model('./weights/yolo_face_sthanhng.cfg', \"./weights/yolo_face_sthanhng.weights\")\n",
    "\n",
    "ref_df = pd.read_csv(os.path.join(os.getcwd(), 'reference_dataset.csv'), index_col=0)\n",
    "ref_df = ref_df.reset_index(drop=True)\n",
    "\n",
    "OUTPUT_FOLDER = os.path.join(os.getcwd(), \"WIDER\")\n",
    "if not os.path.exists(OUTPUT_FOLDER):\n",
    "        os.mkdir(OUTPUT_FOLDER)\n",
    "        \n",
    "for FOLDER_NAME in folders:\n",
    "    \n",
    "    features_df = pd.DataFrame(columns=range(25088))\n",
    "    \n",
    "    INPUT_PATH = os.path.join(os.getcwd(), 'images', FOLDER_NAME)\n",
    "    \n",
    "    FOLDER_PATH = os.path.join(OUTPUT_FOLDER, FOLDER_NAME)\n",
    "    if not os.path.exists(FOLDER_PATH):\n",
    "        os.mkdir(FOLDER_PATH)\n",
    "    \n",
    "    CSV_PATH = os.path.join(FOLDER_PATH, FOLDER_NAME + '_CSV')\n",
    "    if not os.path.exists(CSV_PATH):\n",
    "        os.mkdir(CSV_PATH)\n",
    "    \n",
    "    for path in glob.glob(os.path.join(INPUT_PATH, '*.jpg')):\n",
    "        filename = os.path.basename(path)\n",
    "        \n",
    "        filename_df = ref_df[ref_df['source_file'] == filename]\n",
    "        \n",
    "        if len(filename_df.index) > 0:\n",
    "            model.eval()\n",
    "            \n",
    "            data = cv2.imread(path)  # read the image\n",
    "            data = cv2.cvtColor(data, cv2.COLOR_BGR2RGB) #change to rgb\n",
    "            data = transforms.Compose([DEFAULT_TRANSFORMS,Resize(416)])((data, np.zeros((1, 5))))[0].unsqueeze(0) # transform the image\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # Forward pass the data through the model and call non max suppression\n",
    "                nms, nms_output = non_max_suppression(model(data), 0.5, 0.5) #conf_thres and iou_thres = 0.5\n",
    "\n",
    "            face_list = []\n",
    "            if type(nms_output[0]) is not int:\n",
    "                face_list = nms_output[0]\n",
    "            \n",
    "            for face_index, face_row in enumerate(face_list): #nms_output[0] because the model is designed to take in several images at a time from the dataloader but we are only loading the image one at a time\n",
    "                if face_index in set(filename_df['face_index']):\n",
    "                    \n",
    "                    x, y, w, h = face_row[0], face_row[1], face_row[2], face_row[3]\n",
    "                    \n",
    "                    x1 = max(int(np.floor((x - w / 2).detach().cpu().numpy())), 0)\n",
    "                    y1 = max(int(np.floor((y - h / 2).detach().cpu().numpy())), 0)\n",
    "                    x2 = min(int(np.ceil((x + w / 2).detach().cpu().numpy())), 415)\n",
    "                    y2 = min(int(np.ceil((y + h / 2).detach().cpu().numpy())), 415)\n",
    "                    \n",
    "                    cropped_image = data.detach().cpu()[:, :, y1:y2, x1:x2]\n",
    "                    cropped_image = np.transpose(cropped_image.detach().cpu().squeeze().numpy(), (1, 2, 0))\n",
    "                    cropped_image = (cropped_image * 255).astype(np.uint8)\n",
    "                    cropped_image = Image.fromarray(cropped_image)\n",
    "                    \n",
    "                    plt.imshow(cropped_image, interpolation='nearest')\n",
    "                    plt.show()\n",
    "                    \n",
    "                    cropped_image = vgg_16_transform(cropped_image)\n",
    "#                     with torch.no_grad():\n",
    "#                         vgg_output = vgg_model(cropped_image)\n",
    "\n",
    "                    vgg_features = torch.flatten(vgg_output).tolist()\n",
    "#                     print(type(vgg_features))\n",
    "                    features_df.loc[len(features_df)] = vgg_features\n",
    "    \n",
    "    features_df.to_csv(os.path.join(CSV_PATH, FOLDER_NAME + '_VGG16_features' + str(int(time.time())) + '.csv'), index=False)  #save to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777e8ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_df.to_csv(os.path.join(CSV_PATH, FOLDER_NAME + '_VGG16_features' + str(int(time.time())) + '.csv'), index=False)  #save to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5de0f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
