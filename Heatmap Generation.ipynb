{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c8fd36",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "#libraries for yolo\n",
    "from pytorchyolo.utils.transforms import Resize, DEFAULT_TRANSFORMS\n",
    "from pytorchyolo.utils.utils import non_max_suppression\n",
    "from pytorchyolo.utils.loss import compute_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e89d98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.utils import detach_cpu, tensor_to_np_img, save_tensor_img, clone_detach, open_img_as_tensor, remap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7376ded",
   "metadata": {},
   "source": [
    "## Face Detector Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4aa6149",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scripts.face_detectors import YoloFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87537d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "yf = YoloFace()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fd4752",
   "metadata": {},
   "source": [
    "## YOLOFace with FGSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a22160e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patterned after FGSM tutorial (https://pytorch.org/tutorials/beginner/fgsm_tutorial.html)\n",
    "# Define what device we are using\n",
    "print(\"CUDA Available: \", torch.cuda.is_available())\n",
    "main_yf = YoloFace()\n",
    "device, model = main_yf.device, main_yf.yf_face_detector\n",
    "\n",
    "\n",
    "# Set the model in evaluation mode. In this case this is for the Dropout layers\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cd60e3",
   "metadata": {},
   "source": [
    "## FGSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b600713",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import scripts.fgsm as fgsm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39c4a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grads(data, force=None):\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    \n",
    "    model.eval()\n",
    "    model.gradient_mode = False\n",
    "\n",
    "    for yolo_layer in model.yolo_layers:\n",
    "        yolo_layer.gradient_mode = False\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Forward pass the data through the model and call non max suppression\n",
    "        nms, nms_output = non_max_suppression(model(data), 0.5, 0.5) #conf_thres and iou_thres = 0.5\n",
    "\n",
    "    face_list = []\n",
    "    if type(nms_output[0]) is not int:\n",
    "        face_list = nms_output[0]\n",
    "\n",
    "    data = data.to(device)\n",
    "\n",
    "    # Set requires_grad attribute of tensor. Important for attack\n",
    "    data.requires_grad = True\n",
    "\n",
    "    model.gradient_mode = True\n",
    "    for yolo_layer in model.yolo_layers:\n",
    "        yolo_layer.gradient_mode = True\n",
    "\n",
    "    output = model(data)\n",
    "    \n",
    "    grads = []\n",
    "    bboxes = []\n",
    "    targets = []\n",
    "    \n",
    "    if force is not None:\n",
    "        force = clone_detach(force)\n",
    "        target = force.to(device)\n",
    "        loss, loss_components = compute_loss(output, target, model)\n",
    "        # Zero all existing gradients\n",
    "        model.zero_grad()\n",
    "        data.grad = None\n",
    "\n",
    "        # Calculate gradients of model in backward pass\n",
    "        loss.backward(retain_graph=True)\n",
    "\n",
    "        # Collect datagrad\n",
    "        data_grad = data.grad.data\n",
    "        return data_grad\n",
    "    \n",
    "    # loop through each of the faces in the image\n",
    "    for face_index, face_row in enumerate(face_list): #nms_output[0] because the model is designed to take in several images at a time from the dataloader but we are only loading the image one at a time\n",
    "            \n",
    "        x, y, w, h = face_row[0], face_row[1], face_row[2], face_row[3]\n",
    "\n",
    "        normal_x, normal_y, normal_w, normal_h = x / 415, y / 415, w / 415, h / 415\n",
    "\n",
    "        if fgsm_loss_target == \"bbox\":\n",
    "            target = torch.tensor([[face_row[4].item(), face_row[5].item(), 0, 0, 0, 0]])\n",
    "        elif fgsm_loss_target == \"conf\":\n",
    "            target = torch.tensor([[0.0, 0, normal_x, normal_y, normal_w, normal_h]])\n",
    "\n",
    "        target = target.to(device)\n",
    "        loss, loss_components = compute_loss(output, target, model)\n",
    "        \n",
    "        targets.append(target)\n",
    "\n",
    "        # cropped image with bounding box\n",
    "        # getting (x1, y1) upper left, (x2, y2) lower right\n",
    "        x1 = max(int(np.floor((x - w / 2).detach().cpu().numpy())), 0)\n",
    "        y1 = max(int(np.floor((y - h / 2).detach().cpu().numpy())), 0)\n",
    "        x2 = min(int(np.ceil((x + w / 2).detach().cpu().numpy())), 415)\n",
    "        y2 = min(int(np.ceil((y + h / 2).detach().cpu().numpy())), 415)\n",
    "\n",
    "        cropped_image = detach_cpu(data)[:, :, y1:y2, x1:x2] #get the first dimension, the channels, and crop it\n",
    "        cropped_image = tensor_to_np_img(cropped_image) #reshape the image to (w/h, h/w, channel)\n",
    "\n",
    "        # Zero all existing gradients\n",
    "        model.zero_grad()\n",
    "        data.grad = None\n",
    "\n",
    "        # Calculate gradients of model in backward pass\n",
    "        loss.backward(retain_graph=True)\n",
    "\n",
    "        # Collect datagrad\n",
    "        data_grad = data.grad.data\n",
    "        grads.append(data_grad)\n",
    "        \n",
    "        bbox = (x1, y1, x2, y2)\n",
    "        bboxes.append(bbox)\n",
    "        \n",
    "    return grads, bboxes, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7c3f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_heatmap = torch.zeros(1, 3, 416, 416)\n",
    "\n",
    "def pipeline(model, device):\n",
    "    global combined_heatmap\n",
    "    \n",
    "    # Loop over all examples in input path\n",
    "    for path in glob.glob(os.path.join(INPUT_PATH, '*.jpg')):\n",
    "        file_basename = os.path.basename(path)\n",
    "        print(file_basename, end=\" \")\n",
    "        \n",
    "        if use_refset:\n",
    "            # Get indices of faces in the image\n",
    "            face_indices = set(REF_SET.loc[REF_SET['source_file'] == file_basename, \"face_index\"])\n",
    "            # If there are no face indices then it means the image is not in the ref set\n",
    "            if not face_indices:\n",
    "                print(\"(skipped)\")\n",
    "                continue\n",
    "        print(\"<- working on\")\n",
    "        \n",
    "        # Read and transform the image from the path\n",
    "        data = cv2.imread(path)\n",
    "        data = cv2.cvtColor(data, cv2.COLOR_BGR2RGB)\n",
    "        data = transforms.Compose([DEFAULT_TRANSFORMS,Resize(416)])((data, np.zeros((1, 5))))[0].unsqueeze(0)\n",
    "        \n",
    "        print(\"Getting grads...\")\n",
    "        grads, bboxes, targets = get_grads(data)\n",
    "        \n",
    "        for i, (data_grad, bbox, target) in enumerate(zip(grads, bboxes, targets)):\n",
    "            heatmap_filename = os.path.join(HEATMAP_PATH, str(i) + \"_\" + file_basename)\n",
    "            no_mask = np.ones(data.shape)\n",
    "            \n",
    "            #\"\"\"\n",
    "            print(\"Calculating min epsilon...\")\n",
    "            yf_min_e = fgsm.find_min_e(clone_detach(data), clone_detach(data_grad), yf, no_mask, bbox)\n",
    "            print(\"face\", i, \"yf_min_e: \", yf_min_e)\n",
    "            \"\"\"\n",
    "            perturbed = fgsm.fgsm_attack(clone_detach(data), yf_min_e, clone_detach(data_grad), no_mask).to(torch.float32)\n",
    "            perturbed_grad = get_grads(perturbed, force=target)\n",
    "            #\"\"\"\n",
    "            \n",
    "            heatmap = remap(clone_detach(data_grad).sign(), -1, 1, 0, 1) * yf_min_e\n",
    "            #heatmap = clone_detach(data_grad)\n",
    "            #heatmap = data_grad - perturbed_grad\n",
    "            \n",
    "            if save_noise:\n",
    "                save_tensor_img(heatmap, heatmap_filename)\n",
    "            \n",
    "            combined_heatmap += heatmap\n",
    "            \n",
    "    combined_heatmap = remap(combined_heatmap, torch.amin(combined_heatmap), torch.amax(combined_heatmap), 0, 1)\n",
    "    save_tensor_img(combined_heatmap, os.path.join(HEATMAP_PATH, \"_combined_map.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5af1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "fgsm_loss_target = \"conf\" # or \"bbox\"\n",
    "output_csv_tag = \"no_random\"\n",
    "save_noise = False\n",
    "use_existing_noise = True\n",
    "use_refset = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3efbbeb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "folders = [ 'Test']\n",
    "fails = []\n",
    "OUTPUT_FOLDER = os.path.join(os.getcwd(), \"outputs\")\n",
    "\n",
    "REF_SET = pd.read_csv(os.path.join(os.getcwd(), \"csv\", \"reference_dataset.csv\"), index_col=0)\n",
    "REF_SET.reset_index()\n",
    "\n",
    "for FOLDER_NAME in folders:\n",
    "    INPUT_PATH = os.path.join(os.getcwd(), 'images', FOLDER_NAME)\n",
    "    FOLDER_PATH = os.path.join(OUTPUT_FOLDER, FOLDER_NAME)\n",
    "    HEATMAP_PATH = os.path.join(FOLDER_PATH, FOLDER_NAME + '_HEATMAP')\n",
    "    \n",
    "    os.makedirs(HEATMAP_PATH, exist_ok=True)\n",
    "    \n",
    "    print(\"Working on\", FOLDER_NAME, \"folder\")\n",
    "    pipeline(model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830ff1cf",
   "metadata": {},
   "source": [
    "##### ___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
